{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# systems\n",
    "import os, sys\n",
    "\n",
    "from dateutil import parser\n",
    "\n",
    "# scraping websites\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# language processing\n",
    "from sutime import SUTime\n",
    "\n",
    "#\n",
    "sys.path.insert(0, '../src/')\n",
    "from listing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../listings/mitml'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_id = \"mitml\"\n",
    "base_url = \"http://mailman.mit.edu/mailman/private/\"\n",
    "\n",
    "local_dir = '../listings/'\n",
    "local_list_dir = os.path.join(local_dir, list_id)\n",
    "local_list_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import URLs and indices\n",
    "with open(os.path.join(local_list_dir, 'urls.txt'), 'r') as f: \n",
    "    urls = [line.rstrip() for line in f.readlines()]\n",
    "    indices = [os.path.basename(url) for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://mailman.mit.edu/mailman/private/mitml/2018-August/000480.html',\n",
       " 'http://mailman.mit.edu/mailman/private/mitml/2018-August/000479.html',\n",
       " 'http://mailman.mit.edu/mailman/private/mitml/2018-July/000478.html',\n",
       " 'http://mailman.mit.edu/mailman/private/mitml/2018-June/000477.html',\n",
       " 'http://mailman.mit.edu/mailman/private/mitml/2018-June/000476.html']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing = Listing(list_id, '000480.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Mitml] Call for Papers: NIPS \u001b[1;31m2018\u001b[0m Workshop \"All of Bayesian Nonparametrics (Especially the Useful Bits)\"\n"
     ]
    }
   ],
   "source": [
    "listing.highlight_text(listing.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time/date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://mailman.mit.edu/mailman/private/mitml/2018-August/000479.html\n",
      "{'summary': 'Statistics Special Seminar - Prateek Jain (Microsoft Research) - Tuesday, Aug 21st@2:00pm, E18-304', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-August/000479.html', 'location': 'E18-304', 'start': {'dateTime': '2018-08-21T14:00:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-08-21T15:00:00', 'timeZone': 'America/New_York'}}\n",
      "\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000477.html\n",
      "{'summary': 'Fwd: TALK: Tuesday 06-05-2018 Noisy Natural Gradient as Variational Inference', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-June/000477.html', 'location': '32-G882', 'start': {'dateTime': '2018-06-05T14:00:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-06-05T15:00:00', 'timeZone': 'America/New_York'}}\n",
      "\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000476.html\n",
      "{'summary': 'TALK: Tuesday 06-05-2018 Noisy Natural Gradient as Variational Inference', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-June/000476.html', 'location': '32-G882', 'start': {'dateTime': '2018-06-05T14:00:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-06-05T15:00:00', 'timeZone': 'America/New_York'}}\n",
      "\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000473.html\n",
      "{'summary': 'TALK: Tuesday 06-05-2018 Noisy Natural Gradient as Variational Inference', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-May/000473.html', 'location': '32-G882', 'start': {'dateTime': '2018-06-05T14:00:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-06-05T15:00:00', 'timeZone': 'America/New_York'}}\n",
      "\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000469.html\n",
      "{'summary': 'TALK: Wednesday 05-09-2018 Reproducibility in Deep Reinforcement Learning and Beyond', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-May/000469.html', 'location': '32-141', 'start': {'dateTime': '2018-05-09T16:30:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-05-09T17:30:00', 'timeZone': 'America/New_York'}}\n",
      "\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000468.html\n",
      "{'summary': 'TALK: Monday 05-07-2018 Responsibility Sensitive Safety of Self-Driving Cars', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-May/000468.html', 'location': '32-G449', 'start': {'dateTime': '2018-05-07T15:00:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-05-07T16:00:00', 'timeZone': 'America/New_York'}}\n",
      "\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000465.html\n",
      "{'summary': 'Joelle Pineau: ML seminar & meetings Wedn May 9 \"Reproducibility in Deep Reinforcement Learning and Beyond\"', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-May/000465.html', 'location': '32-141', 'start': {'dateTime': '2018-05-09T16:30:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-05-09T17:30:00', 'timeZone': 'America/New_York'}}\n",
      "\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000464.html\n",
      "{'summary': 'TALK: Wednesday 05-09-2018 Reproducibility in Deep Reinforcement Learning and Beyond', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-May/000464.html', 'location': '32-141', 'start': {'dateTime': '2018-05-09T16:30:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-05-09T17:30:00', 'timeZone': 'America/New_York'}}\n",
      "\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000463.html\n",
      "{'summary': 'TALK: Wednesday 05-02-2018 Kernel Methods for Representing Probabilities', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-May/000463.html', 'location': '32-G882', 'start': {'dateTime': '2018-05-02T16:00:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-05-02T17:00:00', 'timeZone': 'America/New_York'}}\n",
      "\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000462.html\n",
      "{'summary': 'Seminar - Inventing Algorithms via Deep Learning - Wednesday May 2, 2018 3:00PM - RM 26-168 - Pramod Viswanath', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-April/000462.html', 'location': '26-168', 'start': {'dateTime': '2018-05-02T15:00:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-05-02T16:00:00', 'timeZone': 'America/New_York'}}\n",
      "\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000461.html\n",
      "{'summary': 'TALK: Monday 05-07-2018 Responsibility Sensitive Safety of Self-Driving Cars', 'description': 'http://mailman.mit.edu/mailman/private/mitml/2018-April/000461.html', 'location': '32-G449', 'start': {'dateTime': '2018-05-07T15:00:00', 'timeZone': 'America/New_York'}, 'end': {'dateTime': '2018-05-07T16:00:00', 'timeZone': 'America/New_York'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in indices[:20]: \n",
    "    listing = Listing(list_id, index)\n",
    "\n",
    "    if listing.is_talk: \n",
    "        print(listing.url)\n",
    "        print(listing.get_parsed_metadata())\n",
    "        \n",
    "        print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "from itertools import combinations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = lambda a,b: SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_listings = []\n",
    "\n",
    "for index in indices[:50]: \n",
    "    l = Listing(list_id, index)\n",
    "    if l.is_talk: \n",
    "        recent_listings.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02384937238493724\n",
      "0.023299161230195712\n",
      "0.023299161230195712\n",
      "0.01383185649448887\n",
      "0.01236603462489695\n",
      "0.03976311336717428\n",
      "0.01383185649448887\n",
      "0.03465445130452101\n",
      "0.019664732430689877\n",
      "0.01236603462489695\n",
      "0.01236603462489695\n",
      "0.03465445130452101\n",
      "0.021906693711967545\n",
      "0.016666666666666666\n",
      "0.033748801534036435\n",
      "0.0344686089454247\n",
      "0.01715622563036132\n",
      "0.01899050474762619\n",
      "0.016666666666666666\n",
      "0.025552108048914036\n",
      "0.016666666666666666\n",
      "0.01899050474762619\n",
      "0.02379841343910406\n",
      "0.03433327553320617\n",
      "0.04148747786491273\n",
      "0.011069182389937107\n",
      "0.02213666987487969\n",
      "0.037012557832121616\n",
      "0.012258796821793417\n",
      "0.01226158038147139\n",
      "0.7731958762886598\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000477.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000476.html\n",
      "0.7731958762886598\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000477.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000473.html\n",
      "0.16260162601626016\n",
      "0.1401831939466348\n",
      "0.01347488770926909\n",
      "0.16260162601626016\n",
      "0.15064534771720284\n",
      "0.021022905553812362\n",
      "0.1401831939466348\n",
      "0.1401831939466348\n",
      "0.15064534771720284\n",
      "0.06784313725490196\n",
      "0.07463917525773196\n",
      "0.0627669452181987\n",
      "0.020222045995241873\n",
      "0.03186457555389594\n",
      "0.1332694151486098\n",
      "0.07463917525773196\n",
      "0.04779607010090282\n",
      "0.07463917525773196\n",
      "0.1332694151486098\n",
      "0.07495511669658887\n",
      "0.011116725618999495\n",
      "0.010186757215619695\n",
      "0.11966224366706876\n",
      "0.04900601017105871\n",
      "0.10915268634529623\n",
      "0.015737704918032787\n",
      "0.015741145605596852\n",
      "1.0\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000476.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000473.html\n",
      "0.21072174518449757\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000476.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000469.html\n",
      "0.18306131451257168\n",
      "0.015873015873015872\n",
      "0.21072174518449757\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000476.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000464.html\n",
      "0.19561981713799703\n",
      "0.0309208290859667\n",
      "0.18306131451257168\n",
      "0.18306131451257168\n",
      "0.19561981713799703\n",
      "0.11405030355594102\n",
      "0.11233379183860615\n",
      "0.04737594445578926\n",
      "0.014047410008779631\n",
      "0.035137432700481724\n",
      "0.22095548317046687\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000476.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000452.html\n",
      "0.11233379183860615\n",
      "0.07517922883162177\n",
      "0.11233379183860615\n",
      "0.22095548317046687\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000476.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000448.html\n",
      "0.07006048387096774\n",
      "0.01688383189576069\n",
      "0.017606602475928473\n",
      "0.20617992890347278\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-June/000476.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000444.html\n",
      "0.053673788431474724\n",
      "0.10992655768775171\n",
      "0.016148764374847076\n",
      "0.016152716593245228\n",
      "0.21072174518449757\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000473.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000469.html\n",
      "0.18306131451257168\n",
      "0.015873015873015872\n",
      "0.21072174518449757\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000473.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000464.html\n",
      "0.19561981713799703\n",
      "0.0309208290859667\n",
      "0.18306131451257168\n",
      "0.18306131451257168\n",
      "0.19561981713799703\n",
      "0.11405030355594102\n",
      "0.11233379183860615\n",
      "0.04737594445578926\n",
      "0.014047410008779631\n",
      "0.035137432700481724\n",
      "0.22095548317046687\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000473.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000452.html\n",
      "0.11233379183860615\n",
      "0.07517922883162177\n",
      "0.11233379183860615\n",
      "0.22095548317046687\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000473.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000448.html\n",
      "0.07006048387096774\n",
      "0.01688383189576069\n",
      "0.017606602475928473\n",
      "0.20617992890347278\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000473.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000444.html\n",
      "0.053673788431474724\n",
      "0.10992655768775171\n",
      "0.016148764374847076\n",
      "0.016152716593245228\n",
      "0.16635859519408502\n",
      "0.09863013698630137\n",
      "1.0\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000469.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000464.html\n",
      "0.16196903533148074\n",
      "0.03986497347693297\n",
      "0.16635859519408502\n",
      "0.16635859519408502\n",
      "0.16196903533148074\n",
      "0.09824135839902971\n",
      "0.1860762188631041\n",
      "0.029434250764525993\n",
      "0.013085258638315273\n",
      "0.024327122153209108\n",
      "0.19308285643194825\n",
      "0.1860762188631041\n",
      "0.0735080058224163\n",
      "0.1860762188631041\n",
      "0.19308285643194825\n",
      "0.043225656518707876\n",
      "0.030428769017980636\n",
      "0.03727959697732997\n",
      "0.19939879759519039\n",
      "0.04696860771627127\n",
      "0.02633889376646181\n",
      "0.0203527815468114\n",
      "0.020357385206966748\n",
      "0.015694164989939637\n",
      "0.1922365988909427\n",
      "0.16492494774843244\n",
      "0.031647533354017994\n",
      "1.0\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000468.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000461.html\n",
      "1.0\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000468.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000460.html\n",
      "0.16492494774843244\n",
      "0.10672853828306264\n",
      "0.15156440471353108\n",
      "0.7410665200659703\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000468.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000456.html\n",
      "0.013682564503518374\n",
      "0.025923208608461725\n",
      "0.18190386427898209\n",
      "0.15156440471353108\n",
      "0.0590805803181262\n",
      "0.15156440471353108\n",
      "0.18190386427898209\n",
      "0.05918727915194346\n",
      "0.016308870028290897\n",
      "0.022407628128724672\n",
      "0.17785155323689827\n",
      "0.03319690768531151\n",
      "0.09537753608031793\n",
      "0.017645792984721326\n",
      "0.017649591046061126\n",
      "0.7780821917808219\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000465.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000464.html\n",
      "0.017902315625608095\n",
      "0.014552356849098386\n",
      "0.03742454728370221\n",
      "0.03742454728370221\n",
      "0.017902315625608095\n",
      "0.02456418383518225\n",
      "0.03626511046269279\n",
      "0.025876617288580536\n",
      "0.04927884615384615\n",
      "0.015636822194199245\n",
      "0.015048543689320388\n",
      "0.03626511046269279\n",
      "0.021082722887261034\n",
      "0.03626511046269279\n",
      "0.015048543689320388\n",
      "0.01952770208900999\n",
      "0.052336448598130844\n",
      "0.06681405060181773\n",
      "0.023454678719765452\n",
      "0.02526906878802059\n",
      "0.03865149237706678\n",
      "0.023877957108114082\n",
      "0.023883237505528527\n",
      "0.16196903533148074\n",
      "0.03986497347693297\n",
      "0.16635859519408502\n",
      "0.16635859519408502\n",
      "0.16196903533148074\n",
      "0.09824135839902971\n",
      "0.1860762188631041\n",
      "0.029434250764525993\n",
      "0.013085258638315273\n",
      "0.024327122153209108\n",
      "0.19308285643194825\n",
      "0.1860762188631041\n",
      "0.0735080058224163\n",
      "0.1860762188631041\n",
      "0.19308285643194825\n",
      "0.043225656518707876\n",
      "0.030428769017980636\n",
      "0.03727959697732997\n",
      "0.19939879759519039\n",
      "0.04696860771627127\n",
      "0.02633889376646181\n",
      "0.0203527815468114\n",
      "0.020357385206966748\n",
      "0.03325774754346183\n",
      "0.15960478814364432\n",
      "0.15960478814364432\n",
      "1.0\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-May/000463.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000459.html\n",
      "0.09398988953379517\n",
      "0.10056963268513062\n",
      "0.03519374333451831\n",
      "0.0174077578051088\n",
      "0.028651949271958667\n",
      "0.17221844550192614\n",
      "0.10056963268513062\n",
      "0.06112054329371817\n",
      "0.10056963268513062\n",
      "0.17221844550192614\n",
      "0.05109644453906749\n",
      "0.01877630301068307\n",
      "0.018790100824931256\n",
      "0.17099863201094392\n",
      "0.026713378585504707\n",
      "0.09333333333333334\n",
      "0.018272425249169437\n",
      "0.018276220145379024\n",
      "0.032578343158547934\n",
      "0.032578343158547934\n",
      "0.02418745275888133\n",
      "0.02851011649294911\n",
      "0.035065349059611096\n",
      "0.039359671023645175\n",
      "0.026901669758812616\n",
      "0.026098143723580224\n",
      "0.027519656897784132\n",
      "0.035065349059611096\n",
      "0.012441679626749611\n",
      "0.035065349059611096\n",
      "0.027519656897784132\n",
      "0.017346938775510204\n",
      "0.020105963863605487\n",
      "0.029565530917613124\n",
      "0.012928712515711977\n",
      "0.032\n",
      "0.01565302462090331\n",
      "0.021003500583430573\n",
      "0.02100700233411137\n",
      "1.0\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000461.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000460.html\n",
      "0.16492494774843244\n",
      "0.10672853828306264\n",
      "0.15156440471353108\n",
      "0.7410665200659703\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000461.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000456.html\n",
      "0.013682564503518374\n",
      "0.025923208608461725\n",
      "0.18190386427898209\n",
      "0.15156440471353108\n",
      "0.0590805803181262\n",
      "0.15156440471353108\n",
      "0.18190386427898209\n",
      "0.05918727915194346\n",
      "0.016308870028290897\n",
      "0.022407628128724672\n",
      "0.17785155323689827\n",
      "0.03319690768531151\n",
      "0.09537753608031793\n",
      "0.017645792984721326\n",
      "0.017649591046061126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16492494774843244\n",
      "0.10672853828306264\n",
      "0.15156440471353108\n",
      "0.7410665200659703\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000460.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000456.html\n",
      "0.013682564503518374\n",
      "0.025923208608461725\n",
      "0.18190386427898209\n",
      "0.15156440471353108\n",
      "0.0590805803181262\n",
      "0.15156440471353108\n",
      "0.18190386427898209\n",
      "0.05918727915194346\n",
      "0.016308870028290897\n",
      "0.022407628128724672\n",
      "0.17785155323689827\n",
      "0.03319690768531151\n",
      "0.09537753608031793\n",
      "0.017645792984721326\n",
      "0.017649591046061126\n",
      "0.09398988953379517\n",
      "0.10056963268513062\n",
      "0.03519374333451831\n",
      "0.0174077578051088\n",
      "0.028651949271958667\n",
      "0.17221844550192614\n",
      "0.10056963268513062\n",
      "0.06112054329371817\n",
      "0.10056963268513062\n",
      "0.17221844550192614\n",
      "0.05109644453906749\n",
      "0.01877630301068307\n",
      "0.018790100824931256\n",
      "0.17099863201094392\n",
      "0.026713378585504707\n",
      "0.09333333333333334\n",
      "0.018272425249169437\n",
      "0.018276220145379024\n",
      "0.092\n",
      "0.016982836495031618\n",
      "0.026954177897574125\n",
      "0.027357811375089993\n",
      "0.0962517353077279\n",
      "0.092\n",
      "0.08553198827384031\n",
      "0.092\n",
      "0.0962517353077279\n",
      "0.029960920538428137\n",
      "0.023985542960407426\n",
      "0.023870816756377252\n",
      "0.08707799767171129\n",
      "0.029490616621983913\n",
      "0.014406256431364478\n",
      "0.017777777777777778\n",
      "0.017781541066892465\n",
      "0.026490066225165563\n",
      "0.7637540453074434\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000457.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000454.html\n",
      "0.01940260403369926\n",
      "0.1900785854616896\n",
      "1.0\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000457.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000451.html\n",
      "0.08397909533249234\n",
      "1.0\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000457.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000449.html\n",
      "0.1900785854616896\n",
      "0.05463728191000918\n",
      "0.0342641768031523\n",
      "0.035794183445190156\n",
      "0.20024721878862795\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000457.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000444.html\n",
      "0.019876952200662566\n",
      "0.03601648947711\n",
      "0.01564245810055866\n",
      "0.015645954403218598\n",
      "0.0186165358642088\n",
      "0.02021563342318059\n",
      "0.06338181028869112\n",
      "0.052980132450331126\n",
      "0.020052596975673898\n",
      "0.052980132450331126\n",
      "0.06338181028869112\n",
      "0.19300756491515028\n",
      "0.03044569993722536\n",
      "0.023255813953488372\n",
      "0.05458515283842795\n",
      "0.026044948540222643\n",
      "0.03654743390357698\n",
      "0.0187624750499002\n",
      "0.0187662208025554\n",
      "0.018486986134760398\n",
      "0.02297233942803563\n",
      "0.7665857605177994\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000454.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000451.html\n",
      "0.02368100296012537\n",
      "0.7665857605177994\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000454.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000449.html\n",
      "0.02297233942803563\n",
      "0.03208791208791209\n",
      "0.05007461449179241\n",
      "0.11524780649751008\n",
      "0.01509790044821892\n",
      "0.021266968325791856\n",
      "0.0341453258380179\n",
      "0.022702934247162132\n",
      "0.022707797772065125\n",
      "0.7730781105279407\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000453.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000452.html\n",
      "0.016339034975746746\n",
      "0.019508057675996608\n",
      "0.016339034975746746\n",
      "0.7730781105279407\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000453.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000448.html\n",
      "0.6897530513766676\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000453.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000447.html\n",
      "0.5911270983213429\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000453.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000446.html\n",
      "0.8300940438871474\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000453.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000445.html\n",
      "0.773972602739726\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000453.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000444.html\n",
      "0.022399056881815503\n",
      "0.012711864406779662\n",
      "0.010982976386600769\n",
      "0.010985992859104642\n",
      "0.12033398821218075\n",
      "0.06692670909464175\n",
      "0.12033398821218075\n",
      "1.0\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000452.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000448.html\n",
      "0.0701468189233279\n",
      "0.5039736383019965\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000452.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000446.html\n",
      "0.7563527653213752\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000452.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000445.html\n",
      "0.9872289872289872\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000452.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000444.html\n",
      "0.04114994363021421\n",
      "0.11396591198168406\n",
      "0.023176191730313406\n",
      "0.023182297154899896\n",
      "0.08397909533249234\n",
      "1.0\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000451.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000449.html\n",
      "0.1900785854616896\n",
      "0.05463728191000918\n",
      "0.0342641768031523\n",
      "0.035794183445190156\n",
      "0.20024721878862795\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000451.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000444.html\n",
      "0.019876952200662566\n",
      "0.03601648947711\n",
      "0.01564245810055866\n",
      "0.015645954403218598\n",
      "0.0742476121823752\n",
      "0.06323136932867994\n",
      "0.021726479146459747\n",
      "0.02471368294153104\n",
      "0.026545002073828285\n",
      "0.05532617671345995\n",
      "0.0354228855721393\n",
      "0.02514792899408284\n",
      "0.017444065225635193\n",
      "0.017447373411720084\n",
      "0.1900785854616896\n",
      "0.05463728191000918\n",
      "0.0342641768031523\n",
      "0.035794183445190156\n",
      "0.20024721878862795\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000449.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000444.html\n",
      "0.019876952200662566\n",
      "0.03601648947711\n",
      "0.01564245810055866\n",
      "0.015645954403218598\n",
      "0.0701468189233279\n",
      "0.5039736383019965\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000448.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000446.html\n",
      "0.7563527653213752\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000448.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000445.html\n",
      "0.9872289872289872\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000448.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000444.html\n",
      "0.04114994363021421\n",
      "0.11396591198168406\n",
      "0.023176191730313406\n",
      "0.023182297154899896\n",
      "0.45085430828587175\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000447.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000446.html\n",
      "0.662441443923946\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000447.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000445.html\n",
      "0.05313612708846891\n",
      "0.014091858037578288\n",
      "0.044602609727164885\n",
      "0.022053418279833373\n",
      "0.022058823529411766\n",
      "0.37573385518590996\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000446.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000445.html\n",
      "0.28409976617303195\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000446.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000444.html\n",
      "0.021080368906455864\n",
      "0.024929775280898875\n",
      "0.01438331535418914\n",
      "0.014385901816220105\n",
      "0.5376732971669681\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000445.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-April/000444.html\n",
      "0.02572163475278651\n",
      "0.02575991756826378\n",
      "0.017075773745997867\n",
      "0.017080330931411796\n",
      "0.021016756603237718\n",
      "0.11936475409836066\n",
      "0.02440318302387268\n",
      "0.024409657734146988\n",
      "0.054834761321909425\n",
      "0.025816249050873197\n",
      "0.025822784810126582\n",
      "0.01707429626211352\n",
      "0.01707823678744519\n",
      "0.9997618480590617\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-February/000434.html\n",
      "http://mailman.mit.edu/mailman/private/mitml/2018-February/000435.html\n"
     ]
    }
   ],
   "source": [
    "for a,b in combinations(recent_listings, 2): \n",
    "    print(similar(a.message, b.message))\n",
    "    if similar(a.message, b.message) > 0.2: \n",
    "        print(a.url)\n",
    "        print(b.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Listing(list_id, '000446.html')\n",
    "l2 = Listing(list_id, '000445.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.37573385518590996"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar(l1.message, l2.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary': 'TALK: Thursday 09-17-2015 Extreme Classification: A New Paradigm for Ranking & Recommendation',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-September/000000.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2015-09-17T14:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-09-17T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Just a reminder about this MIT-ML Seminar talk. Notice: this week time is 2:00PM to 3:00PM.\\n\\nThanks,\\nSuvrit.\\n–\\n\\nBegin forwarded message:\\n\\n> From: calendar at csail.mit.edu\\n> Subject: TALK: Thursday 09-17-2015 Extreme Classification: A New Paradigm for Ranking & Recommendation\\n> Date: September 16, 2015 at 0:01:22 EDT\\n> To: seminars at csail.mit.edu\\n> \\n> Extreme Classification: A New Paradigm for Ranking & Recommendation\\n> \\n> Speaker: Manik Varma\\n> Speaker Affiliation: MSR Bangalore\\n> Host: Suvrit Sra, Stefanie Jegelka\\n> Host Affiliation: CSAIL/LIDS\\n> \\n> Date: Thursday, September 17, 2015\\n> Time:  2:00 PM to 3:00 PM\\n> \\n> Location: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar Room D507</a>\\n> \\n> Abstract:\\n> ----------\\n> The objective in extreme multi-label classification is to learn a classifier that can automatically tag a data point with the most relevant subset of labels from a large label set. Extreme multi-label classification is an important research problem since not only does it enable the tackling of applications with many labels but it also allows the reformulation of ranking and recommendation problems with certain advantages over existing formulations.\\n> Our objective, in this talk, is to develop an extreme multi-label classifier that is faster to train and more accurate at prediction than the state-of-the-art Multi-label Random Forest (MLRF) algorithm [Agrawal et al. WWW 13] and the Label Partitioning for Sub-linear Ranking (LPSR) algorithm [Weston et al. ICML 13]. MLRF and LPSR learn a hierarchy to deal with the large number of labels but optimize task independent measures, such as the Gini index or clustering error, in order to learn the hierarchy. Our proposed FastXML algorithm achieves significantly higher accuracies by directly optimizing an nDCG based ranking loss function. We also develop an alternating minimization algorithm for efficiently optimizing the proposed formulation. Experiments reveal that FastXML can be trained on problems with more than a million labels on a standard desktop in eight hours using a single core and in an hour using multiple cores.\\n> \\n> BIO\\n> ——\\n> Manik Varma is a researcher at Microsoft Research India. Manik received a bachelor\\'s degree in Physics from St. Stephen\\'s College, University of Delhi in 1997 and another one in Computation from the University of Oxford in 2000 on a Rhodes Scholarship. He then stayed on at Oxford on a University Scholarship and obtained a DPhil in Engineering in 2004. Before joining Microsoft Research, he was a Post-Doctoral Fellow at the Mathematical Sciences Research Institute Berkeley. He has been an Adjunct Professor at the Indian Institute of Technology (IIT) Delhi in the Computer Science and Engineering Department since 2009 and jointly in the School of Information Technology since 2011. His research interests lie in the areas of machine learning, computational advertising and computer vision. He has served as an Area Chair for machine learning and computer vision conferences such as CVPR, ICCV, ICML and NIPS. He has been awarded the Microsoft Gold Star award and has won the PASCAL VOC Object Detection Challenge. Classifiers that he has developed are running live on millions of machines around the world protecting them from viruses and malware.\\n> \\n> Relevant URL: \\n> For more information please contact: Stefanie Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n> \\n\\n> _______________________________________________\\n> Seminars mailing list\\n> Seminars at lists.csail.mit.edu\\n> https://lists.csail.mit.edu/mailman/listinfo/seminars\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-09-16',\n",
       "  'id': 1},\n",
       " {'summary': 'Fwd: TALK: Thursday 10-22-2015 Brian Kulis: Small-Variance Asymptotics for Large-Scale Learning',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-October/000014.html',\n",
       "  'start': {'dateTime': '2015-10-22T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-10-22T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Begin forwarded message:\\n\\n> From: calendar at csail.mit.edu\\n> Subject: TALK: Thursday 10-22-2015 Brian Kulis: Small-Variance Asymptotics for Large-Scale Learning\\n> Date: October 17, 2015 at 0:01:12 EDT\\n> To: seminars at csail.mit.edu\\n> \\n> Small-Variance Asymptotics for Large-Scale Learning\\n> \\n> Speaker: Brian Kulis\\n> Speaker Affiliation: BU\\n> Host: Jonas Mueller, Suvrit Sra\\n> Host Affiliation: MIT\\n> \\n> Date: Thursday, October 22, 2015\\n> Time:  4:00 PM to 5:00 PM\\n> \\n> Location: Seminar Room D507\\n> \\n> Abstract:\\n> This talk will focus on designing scalable learning algorithms via the technique of small-variance asymptotics.  We will take as a starting point the widely known relationship between the Gaussian mixture model and k-means, for clustering data: as the covariances of the clusters shrink, the EM algorithm approaches the k-means algorithm and the negative log-likelihood approaches the k-means objective.  Similar asymptotic connections exist for other machine learning models, including dimensionality reduction (probabilistic PCA becomes PCA), multiview learning (probabilistic CCA becomes CCA), and classification (a restricted Bayes optimal classifier becomes the SVM).  The asymptotic non-probabilistic counterparts to the probabilistic models are almost always more scalable, and are typically easier to analyze, making them useful alternatives to the probabilistic models in many situations.  We will explore how to extend such asymptotics to a richer class of probabilistic models, with a focus on large-scale graphical models, Bayesian nonparametric models, and time-series data.  We will develop the necessary mathematical tools needed for these extensions and will describe a framework for designing scalable optimization problems derived from the rich probabilistic models.  Applications are diverse, and include topic modeling, network evolution, and deep feature learning for large-scale data.\\n> \\n> Bio:\\n> Brian Kulis is the Peter J. Levine Career Development Assistant Professor in the Department of Electrical and Computer Engineering and the Department of Computer Science at Boston University.   His research focuses on machine learning, statistics, computer vision, data mining, and large-scale optimization.  Previously, he was an assistant professor in computer science and in statistics at Ohio State University, and prior to that was a postdoctoral fellow at UC Berkeley EECS.  He obtained his PhD in computer science from the University of Texas in 2008, and his BA degree from Cornell University in computer science and mathematics in 2003.  For his research, he has won three best paper awards at top-tier conferences---two at the International Conference on Machine Learning (in 2005 and 2007) and one at the IEEE Conference on Computer Vision and Pattern Recognition (in 2008).  He is also the recipient of an NSF CAREER Award in 2015, an MCD graduate fellowship from the University of Texas (2003-2007), and an Award of Excellence from the College of Natural Sciences at the University of Texas.\\n> \\n> Relevant URL: http://suvrit.de/mit/ml/\\n> For more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n> \\n\\n> _______________________________________________\\n> Seminars mailing list\\n> Seminars at lists.csail.mit.edu\\n> https://lists.csail.mit.edu/mailman/listinfo/seminars\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-10-21',\n",
       "  'id': 2},\n",
       " {'summary': 'TALK: Thursday 11-12-2015 COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-November/000022.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2015-11-12T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-11-12T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution\\n\\nSpeaker: Manuel Gomez Rodriguez\\nSpeaker Affiliation: Max Planck Institute for Software Systems\\nHost: Suvrit Sra, Stefanie Jegelka\\n\\n \\nDate: Thursday, November 12, 2015\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar Room D507</a>\\n\\nCOEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution\\n\\nInformation diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. In this talk, we introduce a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives. \\n\\n————\\n\\nBio: Manuel Gomez Rodriguez is a tenure-track faculty at Max Planck Institute for Software Systems. Manuel develops machine learning and large-scale data mining methods for the analysis, modeling and control of large real-world networks and processes that take place over them. He is particularly interested in problems arising in the Web and social media and has received several recognitions for his research, including an Outstanding Paper Award at NIPS\\'13 and a Best Research Paper Honorable Mention at KDD\\'10. Manuel holds a PhD in Electrical Engineering from Stanford University and a BS in Electrical Engineering from Carlos III University in Madrid (Spain). You can find more about him at http://www.mpi-sws.org/~manuelgr/.\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-11-05',\n",
       "  'id': 3},\n",
       " {'summary': 'Tuesday Nov. 10 - Seminar: Leonardo Badino - Speech Production Features for Deep Neural Network Acoustic Modeling',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-November/000025.html',\n",
       "  'location': '32-G449',\n",
       "  'start': {'dateTime': '2015-11-10T13:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-11-10T14:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Title: Speech Production Features for Deep Neural Network Acoustic Modeling\\n\\nSpeaker: Leonardo Badino, Istituto Italiano di Technologia (IIT)\\nDate: Tuesday, November 10, 2015\\nTime: 1:00-2:00 PM\\nLocation: MIT 32-G449 (Stata Center, Patil/Kiva Seminar Room)\\nHosts: Lorenzo Rosasco (Laboratory for Computational and Statistical\\nLearning, MIT-IIT)\\n\\nAbstract: In the last few years DNNs have become the dominant technique for\\nacoustic modeling in automatic speech recognition (ASR). The diverse set of\\napproaches proposed to further improve ASR performance includes DNN- based\\nacoustic modeling that uses speech production knowledge (SPK), i.e.,\\ninformation about how the vocal tract produces speech sounds. While\\nstandard acoustic modeling already relies on some phonological SPK binary\\nfeatures (e.g., fricative) to model phonetic context and define the DNN\\ntargets, more explicit uses of SPK for DNN acoustic model training can be\\nexplored. In this talk I will be presenting two SPK-based approaches. The\\nfirst approach relies on measurements of vocal tract movements to extract\\nnew acoustic features that are appended to the DNN input vector. The second\\napproach extracts continuous valued SPK features from binary phonological\\nfeatures which are then used to build a structured output for the DNN. The\\ntwo approaches, tested on mngu0 and TIMIT datasets, show a consistent phone\\nrecognition error reduction over a baseline that does not use SPK.\\n\\nRelevant URL: http://lcsl.mit.edu/#/events/seminars\\n\\nThis talk is part of the LCSL \"Machine Learning Seminar Series\" organized\\nby the Laboratory for Computational and Statistical Learning (LCSL) (a\\njoint lab between MIT and the Italian Institute of Technology), and\\ncoordinated with the Center for Brains, Minds and Machines (CBMM).\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-11-09',\n",
       "  'id': 4},\n",
       " {'summary': 'Risi Kondor: MIT-ML Seminar (Tuesday, November 17,\\t11am-noon)',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-November/000026.html',\n",
       "  'start': {'dateTime': '2015-11-17T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-11-17T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Dear all,\\nProf. Risi Kondor from University of Chicago will be visiting us next Tuesday, and giving a talk on his super-interesting work on large scale multilevel matrix factorizations for unraveling the structure in graphs, and connections to harmonic analysis.  \\n\\nThe talk will be held in G-451 from 11am-12pm. Please find the abstract of the talk below.\\n\\nBest,\\nVikas\\n\\n\\n\\nAbstract: \\nThe sheer size of today's datasets dictates that learning algorithms compress or reduce their input data and/or make use of parallelism. In this work we make a connection between such computational strategies and some classical themes in Applied Mathematics, namely Multiresolution Analysis and Multigrid Methods. In particular, we make the point that similarity matrices appearing in data often have multiresolution structure, which can be\\nexploited both to facilitate computation, and to inform the statistical analysis. \\n\\nMultiresolution Matrix Factorization (MMF) is a specific fast algorithm to extract this structure. Applications of MMF to matrix compression and discovering structure in large graphs/networks are of particular interest. We demonstrate MMF with experiments on a variety of datasets. This work is joint with Nedelina Teneva, Pramod Mudrakarta and Vikas Garg. \\n\\n\\nBio:\\nRisi Kondor is Assistant Professor at The University of Chicago in the Department of Computer Science and \\nthe Department of Statistics. His research focuses on Machine Learning and Computational Harmonic \\nAnalysis. He obtained his PhD from Columbia University and was a postdoc at the Gatsby Unit at UCL, \\nand at Caltech. http://people.cs.uchicago.edu/~risi/ <http://people.cs.uchicago.edu/~risi/>\\n\\n\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-11-10',\n",
       "  'id': 5},\n",
       " {'summary': 'TALK: Thursday 11-12-2015 COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-November/000027.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2015-11-12T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-11-12T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution\\n\\nSpeaker: Manuel Gomez Rodriguez\\nSpeaker Affiliation: Max Planck Institute for Software Systems\\nHost: Suvrit Sra, Stefanie Jegelka\\n\\n \\nDate: Thursday, November 12, 2015\\nTime:  3:00 PM to 4:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar Room D507</a>\\n\\nMachine Learning Seminar\\n\\nCOEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution\\n\\nInformation diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. In this talk, we introduce a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives. \\n\\n————\\n\\nBio: Manuel Gomez Rodriguez is a tenure-track faculty at Max Planck Institute for Software Systems. Manuel develops machine learning and large-scale data mining methods for the analysis, modeling and control of large real-world networks and processes that take place over them. He is particularly interested in problems arising in the Web and social media and has received several recognitions for his research, including an Outstanding Paper Award at NIPS\\'13 and a Best Research Paper Honorable Mention at KDD\\'10. Manuel holds a PhD in Electrical Engineering from Stanford University and a BS in Electrical Engineering from Carlos III University in Madrid (Spain). You can find more about him at http://www.mpi-sws.org/~manuelgr/.\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-11-12',\n",
       "  'id': 3},\n",
       " {'summary': 'Fwd: TALK: Thursday 11-12-2015 COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-November/000029.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2015-11-12T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-11-12T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Today! this talk will be of interest to those who care about Networks (and related topics!)\\n\\nSuvrit.\\n–\\n\\n\\nBegin forwarded message:\\n\\n> From: calendar at csail.mit.edu\\n> Subject: TALK: Thursday 11-12-2015 COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution\\n> Date: November 12, 2015 at 0:01:11 EST\\n> To: seminars at csail.mit.edu, mitml at mit.edu\\n> \\n> ML seminar: COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution\\n> \\n> Speaker: Manuel Gomez Rodriguez\\n> Speaker Affiliation: Max Planck Institute for Software Systems\\n> Host: Suvrit Sra, Stefanie Jegelka\\n> \\n> \\n> Date: Thursday, November 12, 2015\\n> Time:  3:00 PM to 4:00 PM\\n> \\n> Location: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar Room D507</a>\\n> \\n> Machine Learning Seminar\\n> \\n> COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution\\n> \\n> Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. In this talk, we introduce a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives. \\n> \\n> ————\\n> \\n> Bio: Manuel Gomez Rodriguez is a tenure-track faculty at Max Planck Institute for Software Systems. Manuel develops machine learning and large-scale data mining methods for the analysis, modeling and control of large real-world networks and processes that take place over them. He is particularly interested in problems arising in the Web and social media and has received several recognitions for his research, including an Outstanding Paper Award at NIPS\\'13 and a Best Research Paper Honorable Mention at KDD\\'10. Manuel holds a PhD in Electrical Engineering from Stanford University and a BS in Electrical Engineering from Carlos III University in Madrid (Spain). You can find more about him at http://www.mpi-sws.org/~manuelgr/.\\n> \\n> Relevant URL: \\n> For more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n> \\n\\n> _______________________________________________\\n> Seminars mailing list\\n> Seminars at lists.csail.mit.edu\\n> https://lists.csail.mit.edu/mailman/listinfo/seminars\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-11-12',\n",
       "  'id': 3},\n",
       " {'summary': 'Rebecca Steorts Thursday 4pm at MIT-MSR ML seminar',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-November/000031.html',\n",
       "  'location': '32-D463',\n",
       "  'start': {'dateTime': '2015-11-19T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-11-19T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Thursday at the MIT-MSR ML seminar, Rebecca Steorts will speak at 4pm, \\nthis time in MIT Stata 32-D463\\n\\n#######################################################################\\n\\nTITLE: *Methods for Quantifying Conflict Casualties in Syria*\\n\\nSPEAKER: Rebecca Steorts <https://stat.duke.edu/%7Ercs46/bio.html>\\n\\nAFFILIATION:     Microsoft Research\\n\\nWHEN:                  4:00PM Thu, Nov 19\\n\\nWHERE:                MIT Stata 32-D463 (see directions below)\\n\\n#######################################################################\\n\\nAbstract\\n\\nInformation about social entities is often spread across multiple large \\ndatabases, each degraded by noise, and without unique identifiers shared \\nacross databases. Record linkage—reconstructing the actual entities and \\ntheir attributes—is essential to using big data and is challenging not \\nonly for inference but also for computation.  In this talk, I motivate \\nrecord linkage by the current conflict in Syria. It has been \\ntremendously well documented, however, we still do not know how many \\npeople have been killed from conflict-related violence. We describe a \\nnovel approach towards estimating death counts in Syria and challenges \\nthat are unique to this database. We first introduce a novel approach to \\nrecord linkage by discovering a bipartite graph, which links manifest \\nrecords to a common set of latent entities. Our model quantifies the \\nuncertainty in the inference and propagates this uncertainty into \\nsubsequent analyses. We then introduce computational speed-ups to avoid \\nall-to-all record comparisons based upon locality-sensitive hashing from \\nthe computer science literature. Finally, we speak to the success and \\nchallenges of solving a problem that is at the forefront of national \\nheadlines and news.\\n\\nBiography\\n\\nRebecca C. Steorts is currently an Assistant Professor at Duke \\nUniversity in the Department of Statistical Science with affiations in \\nthe Social Science Research Institute and the information iniative at \\nDuke. She was a Visiting Assistant Professor in the Statistics \\nDepartment at Carnegie Mellon University from 2012--2015. She received \\nher B.S. in Mathematics in 2005 from Davidson College, her MS in \\nMathematical Sciences in 2007 from Clemson University, and her PhD in \\n2012 from the Department of Statistics at the University of Florida \\nunder the supervision of Malay Ghosh. Rebecca is a recipient of the \\nGraduate Alumni Fellowship Award (2007-2010) from the University of \\nFlorida and the U.S. Census Bureau Dissertation Fellowship Award \\n(2010-2011). In 2011, she was awarded the UF Innovation through \\nInstitutional Integration Program (I-Cubed) and NSF for development of \\nan introductory Bayesian course for undergraduates. She has also been \\nawarded Finalist for the 2012 Leonard J. Savage Thesis Award in Applied \\nMethology. She is interested in scalable computational methods for \\nsocial science applications. Her current works focuses on recovering \\nhigh dimensional objects from degraded data and determining how to \\nrecover the underlying structure. Methods used for this are entity \\nresolution, small area estimation, locality sensitive hashing, and \\nprivacy-preserving record linkage as applied to medical studies, fmri \\nstudies, human rights violations, and estimation of poverty rates in \\nhard to reach domains. Her research was on record linkage and sparse \\nclustering was recently funded by the John Templeton Foundation, \\nMetaKnowledge Network Grants Awarded, November 2014. She was recently \\nnamed to MIT Technology Review's 35 Innovators Under 35 for 2015 as a \\nhumantarian in the field of software. Her work will be profiled in the \\nSeptmember/October issue of MIT Technology Review and she will be \\nrecognized at a special ceremony along with an invited talk at EmTech in \\nNovember 2015.\\n\\n\\nSubscribing:\\n\\nSend a blank message to:\\n\\nMMM-subscribe-request at LISTS.RESEARCH.MICROSOFT.COM\\n\\n\\n\\n\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-11-15',\n",
       "  'id': 6},\n",
       " {'summary': 'Risi Kondor: MIT-ML Seminar (Tuesday, November 17,\\t11am-noon)',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-November/000032.html',\n",
       "  'start': {'dateTime': '2015-11-17T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-11-17T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Reminder - this talk will be happening tomorrow (Tuesday) at 11am.\\n\\n> On Nov 10, 2015, at 8:15 PM, Vikas Garg <vgarg at csail.mit.edu <mailto:vgarg at csail.mit.edu>> wrote:\\n> \\n> Dear all,\\n> Prof. Risi Kondor from University of Chicago will be visiting us next Tuesday, and giving a talk on his super-interesting work on large scale multilevel matrix factorizations for unraveling the structure in graphs, and connections to harmonic analysis.  \\n> \\n> The talk will be held in G-451 from 11am-12pm. Please find the abstract of the talk below.\\n> \\n> Best,\\n> Vikas\\n> \\n> \\n> \\n> Abstract: \\n> The sheer size of today's datasets dictates that learning algorithms compress or reduce their input data and/or make use of parallelism. In this work we make a connection between such computational strategies and some classical themes in Applied Mathematics, namely Multiresolution Analysis and Multigrid Methods. In particular, we make the point that similarity matrices appearing in data often have multiresolution structure, which can be\\n> exploited both to facilitate computation, and to inform the statistical analysis. \\n> \\n> Multiresolution Matrix Factorization (MMF) is a specific fast algorithm to extract this structure. Applications of MMF to matrix compression and discovering structure in large graphs/networks are of particular interest. We demonstrate MMF with experiments on a variety of datasets. This work is joint with Nedelina Teneva, Pramod Mudrakarta and Vikas Garg. \\n> \\n> \\n> Bio:\\n> Risi Kondor is Assistant Professor at The University of Chicago in the Department of Computer Science and \\n> the Department of Statistics. His research focuses on Machine Learning and Computational Harmonic \\n> Analysis. He obtained his PhD from Columbia University and was a postdoc at the Gatsby Unit at UCL, \\n> and at Caltech. http://people.cs.uchicago.edu/~risi/ <http://people.cs.uchicago.edu/~risi/>\\n> \\n> _______________________________________________\\n> Mitml mailing list\\n> Mitml at mit.edu <mailto:Mitml at mit.edu>\\n> http://mailman.mit.edu/mailman/listinfo/mitml\\n\\n\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-11-16',\n",
       "  'id': 5},\n",
       " {'summary': 'Risi Kondor: MIT-ML Seminar (Tuesday, November 17,\\t11am-noon)',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-November/000035.html',\n",
       "  'start': {'dateTime': '2015-11-17T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-11-17T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"For those who are interested, Risi will be presenting today at 11am in G-451. \\n\\n> On Nov 16, 2015, at 7:58 AM, Vikas Garg <vgarg at csail.mit.edu> wrote:\\n> \\n> Reminder - this talk will be happening tomorrow (Tuesday) at 11am.\\n> \\n>> On Nov 10, 2015, at 8:15 PM, Vikas Garg <vgarg at csail.mit.edu <mailto:vgarg at csail.mit.edu>> wrote:\\n>> \\n>> Dear all,\\n>> Prof. Risi Kondor from University of Chicago will be visiting us next Tuesday, and giving a talk on his super-interesting work on large scale multilevel matrix factorizations for unraveling the structure in graphs, and connections to harmonic analysis.  \\n>> \\n>> The talk will be held in G-451 from 11am-12pm. Please find the abstract of the talk below.\\n>> \\n>> Best,\\n>> Vikas\\n>> \\n>> \\n>> \\n>> Abstract: \\n>> The sheer size of today's datasets dictates that learning algorithms compress or reduce their input data and/or make use of parallelism. In this work we make a connection between such computational strategies and some classical themes in Applied Mathematics, namely Multiresolution Analysis and Multigrid Methods. In particular, we make the point that similarity matrices appearing in data often have multiresolution structure, which can be\\n>> exploited both to facilitate computation, and to inform the statistical analysis. \\n>> \\n>> Multiresolution Matrix Factorization (MMF) is a specific fast algorithm to extract this structure. Applications of MMF to matrix compression and discovering structure in large graphs/networks are of particular interest. We demonstrate MMF with experiments on a variety of datasets. This work is joint with Nedelina Teneva, Pramod Mudrakarta and Vikas Garg. \\n>> \\n>> \\n>> Bio:\\n>> Risi Kondor is Assistant Professor at The University of Chicago in the Department of Computer Science and \\n>> the Department of Statistics. His research focuses on Machine Learning and Computational Harmonic \\n>> Analysis. He obtained his PhD from Columbia University and was a postdoc at the Gatsby Unit at UCL, \\n>> and at Caltech. http://people.cs.uchicago.edu/~risi/ <http://people.cs.uchicago.edu/~risi/>\\n>> \\n>> _______________________________________________\\n>> Mitml mailing list\\n>> Mitml at mit.edu <mailto:Mitml at mit.edu>\\n>> http://mailman.mit.edu/mailman/listinfo/mitml <http://mailman.mit.edu/mailman/listinfo/mitml>\\n> \\n\\n\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-11-17',\n",
       "  'id': 5},\n",
       " {'summary': 'Fwd: TALK: Thursday 11-19-2015 Methods for Quantifying Conflict Casualties in Syria (MIT-MSR ML seminar)',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-November/000036.html',\n",
       "  'location': '9-201',\n",
       "  'start': {'dateTime': '2015-11-19T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-11-19T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Begin forwarded message:\\n\\n> From: calendar at csail.mit.edu\\n> Subject: TALK: Thursday 11-19-2015 Methods for Quantifying Conflict Casualties in Syria (MIT-MSR ML seminar)\\n> Date: November 19, 2015 at 0:01:11 EST\\n> To: seminars at csail.mit.edu\\n> \\n> Methods for Quantifying Conflict Casualties in Syria\\n> \\n> Speaker: Rebecca C. Steorts\\n> Speaker Affiliation: Carnegie Mellon University\\n> Host: Stefanie Jegelka, Adam Kalai\\n> Host Affiliation: MIT\\n> \\n> Date: Thursday, November 19, 2015\\n> Time:  4:00 PM to 5:00 PM\\n> \\n> Location: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D463\">Seminar Room D463 (Star)</a>\\n> \\n> MIT-MSR Machine Learning Seminar\\n> \\n> \\n> Title: Methods for Quantifying Conflict Casualties in Syria\\n> \\n> Information about social entities is often spread across multiple large databases, each degraded by noise, and without unique identifiers shared across databases. Record linkage—reconstructing the actual entities and their attributes—is essential to using big data and is challenging not only for inference but also for computation.  In this talk, I motivate record linkage by the current conflict in Syria. It has been tremendously well documented, however, we still do not know how many people have been killed from conflict-related violence. We describe a novel approach towards estimating death counts in Syria and challenges that are unique to this database. We first introduce a novel approach to record linkage by discovering a bipartite graph, which links manifest records to a common set of latent entities. Our model quantifies the uncertainty in the inference and propagates this uncertainty into subsequent analyses. We then introduce computational speed-ups to avoid all-to-all record comparisons based upon locality-sensitive hashing from the computer science literature. Finally, we speak to the success and challenges of solving a problem that is at the forefront of national headlines and news.\\n> \\n> Relevant URL: \\n> For more information please contact: Stefanie S. Jegelka, 2064580678, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n> \\n\\n> _______________________________________________\\n> Seminars mailing list\\n> Seminars at lists.csail.mit.edu\\n> https://lists.csail.mit.edu/mailman/listinfo/seminars\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-11-19',\n",
       "  'id': 7},\n",
       " {'summary': 'Talk: Hamed Hassani: Sequential Information Gathering: From Theory to Designs (Wed, Dec 2)',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-December/000042.html',\n",
       "  'start': {'dateTime': '2015-12-02T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-12-02T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Title:  Sequential Information Gathering: From Theory to Designs\\nSpeaker: Hamed Hassani Seyed  (ETH)\\nWed, Dec 2, 4pm\\nD507\\n\\nAbstract: Optimal information gathering using uncertain observations is \\na central challenge in many disciplines of machine learning such as \\nBayesian experimental design, automated diagnosis, active learning, and \\ndecision making. A widely used method is to perform sequential \\nobservation selection, where the choice of the next observation depends \\non the history seen thus far. Despite the importance and widespread use \\nin applications, little is known about the theoretical properties of \\nsequential observation selection policies in the presence of noise. In \\nparticular, a long-open direction has been to analyse the \\npersistent-noise setting that is arguably more relevant in practical \\napplications. In this talk, we will present a new framework to capture \\nthe role of noise, and explain how it leads to the first rigorous \\nanalysis of the famous information-gain policy. We will then consider \\nmore general information gathering settings and provide new efficient \\nalgorithms, with provable guarantees, to deal with noisy observations.\\n\\n\\nBio: Hamed Hassani is currently a post-doctoral scholar in the Institute \\nof Machine Learning at ETHZ. He received a Ph.D. degree in Computer and \\nCommunication Sciences from EPFL, Lausanne. Prior to that, he received \\nB.Sc. degrees in Electrical Engineering as well as Mathematics from \\nSharif University of Technology, Tehran. Hamed's fields of interest \\ninclude machine learning, theory and application of graphical models, as \\nwell coding and information theory. He is the recipient of the 2014 IEEE \\nInformation Theory Society Thomas M. Cover Dissertation Award.\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-12-01',\n",
       "  'id': 8},\n",
       " {'summary': 'Fwd: [Lids-seminars] Friday, Dec 11, 11am, 32-141- Peter Bartlett - Efficient Optimal Strategies for Universal Prediction',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2015-December/000045.html',\n",
       "  'location': '32-141',\n",
       "  'start': {'dateTime': '2015-12-11T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2015-12-11T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"---------- Forwarded message ----------\\nFrom: Elizabeth Raymer <eraymer at mit.edu>\\nDate: Fri, Dec 4, 2015 at 3:19 PM\\nSubject: [Lids-seminars] Friday, Dec 11, 11am, 32-141- Peter Bartlett -\\nEfficient Optimal Strategies for Universal Prediction\\nTo: stoch-stat <stoch-stat at mit.edu>, stoch-stat-org <stoch-stat-org at mit.edu>\\n\\n\\nHello all,\\n\\nOn behalf of the seminar organizers, I write to invite you to the next\\nStochastics and Statistics Seminar.\\n\\nBest,\\nLizzie\\n\\n*________________________*\\n\\n*STOCHASTICS AND STATISTICS SEMINAR*\\nhttp://idss.mit.edu/news-events/events/\\n\\n*DATE: Friday, December 11*\\n*TIME: 11am-12pm*\\n*VENUE: **32-141*\\n\\nSpeaker: *Peter Bartlett, **UC Berkeley *\\nhttp://www.stat.berkeley.edu/~bartlett/\\n\\n\\n*TITLE: *Efficient Optimal Strategies for Universal Prediction\\n\\n*ABSTRACT:  *In game-theoretic formulations of prediction problems, a\\nstrategy makes a decision, observes an outcome and pays a loss.  The aim is\\nto minimize the regret, which is the amount by which the total loss\\nincurred exceeds the total loss of the best decision in hindsight. This\\ntalk will focus on the minimax optimal strategy, which minimizes the\\nregret, in three settings: prediction with log loss (a formulation of\\nsequential probability density estimation that is closely related to\\nsequential compression, coding, gambling and investment problems),\\nsequential least squares (where decisions and outcomes lie in a subset of a\\nHilbert space, and loss is squared distance), and linear regression (where\\nthe aim is to predict real-valued labels as well as the best linear\\nfunction).  We obtain the minimax optimal strategy for these problems, and\\nshow that it can be efficiently computed.\\n\\n*BIO: *Peter Bartlett is a professor in the Division of Computer Science\\nand the Department of Statistics. He is the co-author of the book *Learning\\nin Neural Networks: Theoretical Foundations*. He has served as associate\\neditor of the journals *Machine Learning*, *Mathematics of Control Signals\\nand Systems*, the *Journal of Machine Learning Research*, the *Journal of\\nArtificial Intelligence Research*, and the *IEEE Transactions on\\nInformation Theory*. He was awarded the Malcolm McIntosh Prize for Physical\\nScientist of the Year in Australia for his work in statistical learning\\ntheory. He was a Miller Institute Visiting Research Professor in Statistics\\nand Computer Science at U.C. Berkeley in Fall 2001, and a fellow, senior\\nfellow and professor in the Research School of Information Sciences and\\nEngineering at the Australian National University's Institute for Advanced\\nStudies (1993-2003). He is also an honorary professor in the Department of\\nComputer Science and Electrical Engineering at the University of Queensland.\\n\\n*_____________________________*\\n\\n*Event Organizers: *\\nVictor Chernozhukov <http://www.mit.edu/%7Evchern/>\\nDavid Gamarnik <http://www.mit.edu/%7Egamarnik/home.html>\\nPhilippe Rigollet <http://www-math.mit.edu/%7Erigollet/>\\nDevavrat Shah <http://www.mit.edu/%7Edevavrat/>\\n\\n\\n_______________________________________________\\nIdss-community mailing list\\nIdss-community at mit.edu\\nhttp://mailman.mit.edu/mailman/listinfo/idss-community\\n\\n_______________________________________________\\nLids-seminars mailing list\\nLids-seminars at mit.edu\\nhttp://mailman.mit.edu/mailman/listinfo/lids-seminars\\n\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2015-12-07',\n",
       "  'id': 9},\n",
       " {'summary': 'Machine Learning Tea seminar : Looking for new organizers to take charge next year',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-January/000050.html',\n",
       "  'message': 'Hi all,\\n\\nWe have been running the Machine Learning Tea seminar for the past 2 years\\nand are looking for 2-3 volunteers (PhD students or postdocs) who would\\nlike to take charge of the seminar next year.  Ideally, you would start\\nworking with us as an organizer this Spring semester to get a feel for how\\nthings run.\\n\\nThis is a low time commitment opportunity to shape the ML community at MIT,\\nmeet researchers across all areas of the field both in/out-side of MIT, and\\nwork with local ML organizations like Google (from which we receive\\nfunding).\\n\\nLet either of us know if you are interested or would like more details!\\nJonas and Tejas\\n(jonasmueller at csail.mit.edu, tejask at mit.edu)\\n\\nP.S. We are also currently seeking speakers for Spring 2016, so please get\\nin touch if you would like to give a talk about some recent work sometime\\nthis semester.\\n\\n\\n*About MLtea:*\\n\\nDuring the academic year, ML tea generally takes place every Monday (except\\nInstitute holidays) from 4:30-5:00 PM in the Stata Center G4 Lounge\\n(32G-475A).  Meetings feature short talks, generally given by MIT graduate\\nstudents or visitors from other universities, on various topics in Machine\\nLearning, Statistics, Optimization, and other relevant areas of Math / CS.\\n\\nIf interested, please subscribe to our mailing list to receive talk\\nannouncements:  http://mailman.mit.edu/mailman/listinfo/mitml\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-01-26',\n",
       "  'id': 10},\n",
       " {'summary': 'ML Seminar: Tuesday 02-02-2016 Scalable variational inference that adapts the number of clusters: Mixtures, topics, sequences, and beyond',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-January/000051.html',\n",
       "  'location': '32-D463',\n",
       "  'start': {'dateTime': '2016-02-02T16:15:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-02-02T17:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Dear all,\\n\\nI want to highlight the different space-time coordinates for this ML\\nSeminar relative to the seminars from last semester (Tuesday Feb 2 at\\n4:15pm in 32-D463, Star). Hope to see you there!\\n\\nBest,\\nTamara\\n\\n------------------------------------\\n\\nScalable variational inference that adapts the number of clusters:\\nMixtures, topics, sequences, and beyond\\n\\nSpeaker: Michael Hughes\\nSpeaker Affiliation: Ph.D. candidate in the Department of Computer Science\\nat Brown University\\nHost: Tamara Broderick\\nHost Affiliation: CSAIL\\n\\nDate: Tuesday, February 02, 2016\\nTime:  4:15 PM to 5:15 PM\\n\\nLocation: 32-D463, Star\\n\\nWe develop new algorithms for training Bayesian nonparametric clustering\\nmodels from millions of examples. These models can capture hierarchical or\\nsequential structure and promise to discover new clusters as more data is\\nseen, yielding flexible representations for large-scale or streaming\\napplications. However, existing algorithms fail to live up to this promise.\\nClassic sampling and optimization approaches are vulnerable to local optima\\nand sensitive to initialization, especially the initial number of clusters.\\nTo remedy this, we show that models based on the Dirichlet Process can be\\ntrained via a variational objective function that can sensibly compare\\nmodels with different numbers of clusters. Using this objective function,\\nthe number of active clusters can be optimized for a given dataset. We show\\nhow to add or remove clusters during a single training run via proposal\\nmoves that can escape poor local optima. Our new methods can be applied to\\nlarge datasets by processing small batches with incremental updates and\\nmemoized storage. Our work-in-progress results span applications in\\nclustering natural images, news articles, and patterns of regulatory\\nactivity in the human genome.\\n\\nMike Hughes is a Ph.D. candidate in the Department of Computer Science at\\nBrown University, advised by Erik Sudderth. His research interests include\\nbuilding flexible nonparametric models with hierarchical or sequential\\nstructure, developing effective optimization algorithms for non-convex\\nproblems, and applications in the biological sciences. He completed his\\nundergraduate studies at Olin College of Engineering (2010). You can find\\nhim on the web at www.michaelchughes.com.\\n\\n\\nRelevant URL:\\nFor more information please contact: Teresa Cataldo, <a href=\"mailto:\\ncataldo at csail.mit.edu\">cataldo at csail.mit.edu</a>\\n\\n\\n_______________________________________________\\nSeminars mailing list\\nSeminars at lists.csail.mit.edu\\nhttps://lists.csail.mit.edu/mailman/listinfo/seminars\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-01-26',\n",
       "  'id': 11},\n",
       " {'summary': 'ML Seminar: Tuesday 02-02-2016 Scalable variational inference that adapts the number of clusters: Mixtures, topics, sequences, and beyond',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-February/000054.html',\n",
       "  'location': '32-D463',\n",
       "  'start': {'dateTime': '2016-02-02T16:15:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-02-02T17:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Hi, all,\\n\\nA reminder about today\\'s ML Seminar at 4:15pm.\\n\\nBest,\\nTamara\\n\\nOn Tue, Jan 26, 2016 at 5:53 PM, Tamara Broderick\\n<tbroderick at csail.mit.edu> wrote:\\n>\\n> ------------------------------------\\n>\\n> Scalable variational inference that adapts the number of clusters: Mixtures,\\n> topics, sequences, and beyond\\n>\\n> Speaker: Michael Hughes\\n> Speaker Affiliation: Ph.D. candidate in the Department of Computer Science\\n> at Brown University\\n> Host: Tamara Broderick\\n> Host Affiliation: CSAIL\\n>\\n> Date: Tuesday, February 02, 2016\\n> Time:  4:15 PM to 5:15 PM\\n>\\n> Location: 32-D463, Star\\n>\\n> We develop new algorithms for training Bayesian nonparametric clustering\\n> models from millions of examples. These models can capture hierarchical or\\n> sequential structure and promise to discover new clusters as more data is\\n> seen, yielding flexible representations for large-scale or streaming\\n> applications. However, existing algorithms fail to live up to this promise.\\n> Classic sampling and optimization approaches are vulnerable to local optima\\n> and sensitive to initialization, especially the initial number of clusters.\\n> To remedy this, we show that models based on the Dirichlet Process can be\\n> trained via a variational objective function that can sensibly compare\\n> models with different numbers of clusters. Using this objective function,\\n> the number of active clusters can be optimized for a given dataset. We show\\n> how to add or remove clusters during a single training run via proposal\\n> moves that can escape poor local optima. Our new methods can be applied to\\n> large datasets by processing small batches with incremental updates and\\n> memoized storage. Our work-in-progress results span applications in\\n> clustering natural images, news articles, and patterns of regulatory\\n> activity in the human genome.\\n>\\n> Mike Hughes is a Ph.D. candidate in the Department of Computer Science at\\n> Brown University, advised by Erik Sudderth. His research interests include\\n> building flexible nonparametric models with hierarchical or sequential\\n> structure, developing effective optimization algorithms for non-convex\\n> problems, and applications in the biological sciences. He completed his\\n> undergraduate studies at Olin College of Engineering (2010). You can find\\n> him on the web at www.michaelchughes.com.\\n>\\n>\\n> Relevant URL:\\n> For more information please contact: Teresa Cataldo, <a\\n> href=\"mailto:cataldo at csail.mit.edu\">cataldo at csail.mit.edu</a>\\n>\\n>\\n> _______________________________________________\\n> Seminars mailing list\\n> Seminars at lists.csail.mit.edu\\n> https://lists.csail.mit.edu/mailman/listinfo/seminars\\n>\\n>',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-02-02',\n",
       "  'id': 12},\n",
       " {'summary': 'ML seminar: Wednesday 02-17-2016 Scalable Bayesian Inference for Interacting Time Series',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-February/000056.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2016-02-17T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-02-17T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Dear all,\\n\\nOur second ML Seminar of the semester, this time on Wedn 2/17 at 4pm in D507.\\n\\nBest,\\nTamara\\n\\n\\n---------- Forwarded message ----------\\nFrom:  <calendar at csail.mit.edu>\\nDate: Wed, Feb 10, 2016 at 12:01 AM\\nSubject: TALK: Wednesday 02-17-2016 ML seminar: Scalable Bayesian\\nInference for Interacting Time Series\\nTo: seminars at csail.mit.edu\\n\\n\\nML seminar: Scalable Bayesian Inference for Interacting Time Series\\n\\nSpeaker: Nick Foti\\n\\nHost: Tamara Broderick\\n\\n\\nDate: Wednesday, February 17, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar\\nRoom D507</a>\\n\\nLarge data sets have become increasingly common in many scientific disciplines\\nallowing practitioners to ask more nuanced questions than previously possible.\\nFor instance, neuroscientists have been moving beyond recording small numbers\\nof neurons to whole brain imaging with technologies like magnetoencephalography\\n(MEG). These new techniques enable studying the systems underlying cognitive\\nprocesses. Such information provides new insights into cognitive disorders and\\nthe potential for developing new therapies and assistive devices.  Motivated by\\nsuch neuroscience applications, I will present a Bayesian method to learn the\\nconditional independence structure between high-dimensional time series,\\nrepresented by a graphical model of time series. This approach hinges on a new\\ncomputationally efficient construction of the marginal likelihood for the\\nobserved time series given a graph structure. This perspective allows the\\nproblem to be reduced to traditional Bayesian structure learning. I apply the\\nframework to study the brain interactions occurring during an auditory\\nattention task from a large MEG data set.  I will also briefly discuss some\\nrecent projects related to large-scale and streaming Bayesian computations.\\n\\nRelevant URL: https://nfoti.github.io/\\nFor more information please contact: Stefanie S. Jegelka, Tamara\\nBroderick, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n\\n_______________________________________________\\nSeminars mailing list\\nSeminars at lists.csail.mit.edu\\nhttps://lists.csail.mit.edu/mailman/listinfo/seminars\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-02-11',\n",
       "  'id': 13},\n",
       " {'summary': 'ML seminar: Wednesday 02-17-2016 Scalable Bayesian Inference for Interacting Time Series',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-February/000057.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2016-02-17T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-02-17T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'P.S. Nick Foti is available and interested to meet with anyone who\\nwould like to chat while he\\'s in town. If you\\'d like to meet him\\nbetween 11am and 3:30pm on the day of his talk (Wedn 2/17), email\\nTeresa Cataldo <cataldo at csail.mit.edu> (who is also CCed and who is\\nputting together his schedule).\\n\\nBest,\\nTamara\\n\\n\\nOn Thu, Feb 11, 2016 at 10:54 PM, Tamara Broderick\\n<tbroderick at csail.mit.edu> wrote:\\n> Dear all,\\n>\\n> Our second ML Seminar of the semester, this time on Wedn 2/17 at 4pm in D507.\\n>\\n> Best,\\n> Tamara\\n>\\n>\\n> ---------- Forwarded message ----------\\n> From:  <calendar at csail.mit.edu>\\n> Date: Wed, Feb 10, 2016 at 12:01 AM\\n> Subject: TALK: Wednesday 02-17-2016 ML seminar: Scalable Bayesian\\n> Inference for Interacting Time Series\\n> To: seminars at csail.mit.edu\\n>\\n>\\n> ML seminar: Scalable Bayesian Inference for Interacting Time Series\\n>\\n> Speaker: Nick Foti\\n>\\n> Host: Tamara Broderick\\n>\\n>\\n> Date: Wednesday, February 17, 2016\\n> Time:  4:00 PM to 5:00 PM\\n>\\n> Location: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar\\n> Room D507</a>\\n>\\n> Large data sets have become increasingly common in many scientific disciplines\\n> allowing practitioners to ask more nuanced questions than previously possible.\\n> For instance, neuroscientists have been moving beyond recording small numbers\\n> of neurons to whole brain imaging with technologies like magnetoencephalography\\n> (MEG). These new techniques enable studying the systems underlying cognitive\\n> processes. Such information provides new insights into cognitive disorders and\\n> the potential for developing new therapies and assistive devices.  Motivated by\\n> such neuroscience applications, I will present a Bayesian method to learn the\\n> conditional independence structure between high-dimensional time series,\\n> represented by a graphical model of time series. This approach hinges on a new\\n> computationally efficient construction of the marginal likelihood for the\\n> observed time series given a graph structure. This perspective allows the\\n> problem to be reduced to traditional Bayesian structure learning. I apply the\\n> framework to study the brain interactions occurring during an auditory\\n> attention task from a large MEG data set.  I will also briefly discuss some\\n> recent projects related to large-scale and streaming Bayesian computations.\\n>\\n> Relevant URL: https://nfoti.github.io/\\n> For more information please contact: Stefanie S. Jegelka, Tamara\\n> Broderick, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n>\\n>\\n> _______________________________________________\\n> Seminars mailing list\\n> Seminars at lists.csail.mit.edu\\n> https://lists.csail.mit.edu/mailman/listinfo/seminars',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-02-12',\n",
       "  'id': 13},\n",
       " {'summary': 'ML seminar: Wednesday 02-17-2016 Scalable Bayesian Inference for Interacting Time Series',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-February/000058.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2016-02-17T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-02-17T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Hi,\\n\\nA reminder about the ML seminar today at 4pm in D507.\\n\\nBest,\\nTamara\\n\\n\\nOn Thu, Feb 11, 2016 at 10:54 PM, Tamara Broderick\\n<tbroderick at csail.mit.edu> wrote:\\n> Dear all,\\n>\\n> Our second ML Seminar of the semester, this time on Wedn 2/17 at 4pm in D507.\\n>\\n> Best,\\n> Tamara\\n>\\n>\\n> ---------- Forwarded message ----------\\n> From:  <calendar at csail.mit.edu>\\n> Date: Wed, Feb 10, 2016 at 12:01 AM\\n> Subject: TALK: Wednesday 02-17-2016 ML seminar: Scalable Bayesian\\n> Inference for Interacting Time Series\\n> To: seminars at csail.mit.edu\\n>\\n>\\n> ML seminar: Scalable Bayesian Inference for Interacting Time Series\\n>\\n> Speaker: Nick Foti\\n>\\n> Host: Tamara Broderick\\n>\\n>\\n> Date: Wednesday, February 17, 2016\\n> Time:  4:00 PM to 5:00 PM\\n>\\n> Location: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar\\n> Room D507</a>\\n>\\n> Large data sets have become increasingly common in many scientific disciplines\\n> allowing practitioners to ask more nuanced questions than previously possible.\\n> For instance, neuroscientists have been moving beyond recording small numbers\\n> of neurons to whole brain imaging with technologies like magnetoencephalography\\n> (MEG). These new techniques enable studying the systems underlying cognitive\\n> processes. Such information provides new insights into cognitive disorders and\\n> the potential for developing new therapies and assistive devices.  Motivated by\\n> such neuroscience applications, I will present a Bayesian method to learn the\\n> conditional independence structure between high-dimensional time series,\\n> represented by a graphical model of time series. This approach hinges on a new\\n> computationally efficient construction of the marginal likelihood for the\\n> observed time series given a graph structure. This perspective allows the\\n> problem to be reduced to traditional Bayesian structure learning. I apply the\\n> framework to study the brain interactions occurring during an auditory\\n> attention task from a large MEG data set.  I will also briefly discuss some\\n> recent projects related to large-scale and streaming Bayesian computations.\\n>\\n> Relevant URL: https://nfoti.github.io/\\n> For more information please contact: Stefanie S. Jegelka, Tamara\\n> Broderick, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n>\\n>\\n> _______________________________________________\\n> Seminars mailing list\\n> Seminars at lists.csail.mit.edu\\n> https://lists.csail.mit.edu/mailman/listinfo/seminars',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-02-17',\n",
       "  'id': 13},\n",
       " {'summary': 'Fwd: TALK: Wednesday 03-02-2016 MIT-MSR ML seminar: John Lafferty: Statistical Estimation Under Communication Constraints',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-March/000066.html',\n",
       "  'location': '32-D463',\n",
       "  'start': {'dateTime': '2016-03-02T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-03-02T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'MIT-MSR ML seminar: John Lafferty: Statistical Estimation Under Communication Constraints\\n\\nSpeaker: John Lafferty\\n\\nHost: Stefanie Jegelka, Adam Kalai\\n\\n  \\nDate: Wednesday, March 02, 2016\\nTime:  3:00 PM to 4:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D463\">Seminar Room D463 (Star)</a>\\n\\nStatistical Estimation Under Communication Constraints\\n\\nImagine that I estimate a statistical model from data, and then want to share my model with you. But we are communicating over a resource constrained channel.  By sending lots of bits, I can communicate my model accurately, with little loss in statistical risk. Sending a small number of bits will incur some excess risk.  What can we say about the tradeoff between statistical risk and the communication constraints?  This is a type of rate distortion and constrained minimax problem, for which we provide a sharp analysis in certain nonparametric settings. Joint work with Yuancheng Zhu (University of Chicago).\\n\\nRelevant URL:\\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n\\n\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-03-02',\n",
       "  'id': 14},\n",
       " {'summary': 'TALK: Wednesday 04-06-2016 ML seminar: Truncated Random\\tMeasures',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-April/000078.html',\n",
       "  'location': '32-G882',\n",
       "  'start': {'dateTime': '2016-04-06T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-04-06T17:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Machine Learning seminar: Truncated Random Measures\\n\\nSpeaker: Jonathan Huggins\\nSpeaker Affiliation: MIT\\nHost: Tamara Broderick\\n\\n  \\nDate: Wednesday, April 06, 2016\\nTime:  4:00 PM to 5:15 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-G882\">Seminar Room G882 (Hewlett Room)</a>\\n\\nAbstract:\\nIn applications ranging from information retrieval to social network analysis to healthcare, we would like to make more complex and\\ndetailed inferences from data as we get more of it. Bayesian nonparametric models accomplish this goal by employing an infinite-dimensional parameter. Typically the parameter is in the form\\nof a completely random measure (CRM) or a normalized completely random measure. We detail three classes of sequential CRM representations that can be used in practice for simulation and posterior inference. These representations subsume existing ones that have previously been developed in an ad hoc manner for specific processes. Since a complete infinite-dimensional CRM cannot be used explicitly for computation, sequential representations are often truncated for tractability. We provide truncation error analyses for each type of sequential representation, as well as their normalized versions, thereby generalizing and improving upon existing truncation error bounds in the literature. We analyze the computational complexity of the sequential representations, which in conjunction with our error bounds allows us to study which representations are the most efficient. We discuss some applications of our theoretical results to popular (normalized) CRMs, demonstrating that our results provide a suite of tools that allow for the straightforward representation and analysis of CRMs that have not previously been used in a Bayesian nonparametric context.\\n\\n\\n\\n\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-04-01',\n",
       "  'id': 15},\n",
       " {'summary': 'Fwd: TALK: Wednesday 04-06-2016 ML seminar: Truncated\\tRandom Measures',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-April/000079.html',\n",
       "  'location': '32-G882',\n",
       "  'start': {'dateTime': '2016-04-06T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-04-06T17:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Dear all,\\n\\nNote the location for the ML Seminar on Wedn 4/6 at 4pm: Seminar Room\\n32-G882 (Hewlett Room).\\n\\nBest,\\nTamara\\n\\n\\n---------- Forwarded message ----------\\nFrom:  <calendar at csail.mit.edu>\\nDate: Fri, Apr 1, 2016 at 12:01 AM\\nSubject: TALK: Wednesday 04-06-2016 ML seminar: Truncated Random Measures\\nTo: seminars at csail.mit.edu\\n\\n\\nMachine Learning seminar: Truncated Random Measures\\n\\nSpeaker: Jonathan Huggins\\nSpeaker Affiliation: MIT\\nHost: Tamara Broderick\\n\\n\\nDate: Wednesday, April 06, 2016\\nTime:  4:00 PM to 5:15 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-G882\">Seminar\\nRoom G882 (Hewlett Room)</a>\\n\\nAbstract:\\nIn applications ranging from information retrieval to social network\\nanalysis to healthcare, we would like to make more complex and\\ndetailed inferences from data as we get more of it. Bayesian\\nnonparametric models accomplish this goal by employing an\\ninfinite-dimensional parameter. Typically the parameter is in the form\\nof a completely random measure (CRM) or a normalized completely random\\nmeasure. We detail three classes of sequential CRM representations\\nthat can be used in practice for simulation and posterior inference.\\nThese representations subsume existing ones that have previously been\\ndeveloped in an ad hoc manner for specific processes. Since a complete\\ninfinite-dimensional CRM cannot be used explicitly for computation,\\nsequential representations are often truncated for tractability. We\\nprovide truncation error analyses for each type of sequential\\nrepresentation, as well as their normalized versions, thereby\\ngeneralizing and improving upon existing truncation error bounds in\\nthe literature. We analyze the computational complexity of the\\nsequential representations, which in conjunction with our error bounds\\nallows us to study which representations are the most efficient. We\\ndiscuss some applications of our theoretical results to popular\\n(normalized) CRMs, demonstrating that our results provide a suite of\\ntools that allow for the straightforward representation and analysis\\nof CRMs that have not previously been used in a Bayesian nonparametric\\ncontext.\\n\\nRelevant URL:\\nFor more information please contact: Stefanie S. Jegelka, <a\\nhref=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n\\n_______________________________________________\\nSeminars mailing list\\nSeminars at lists.csail.mit.edu\\nhttps://lists.csail.mit.edu/mailman/listinfo/seminars\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-04-01',\n",
       "  'id': 15},\n",
       " {'summary': 'Fwd: TALK: Wednesday 04-06-2016 ML seminar: Truncated\\tRandom Measures',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-April/000082.html',\n",
       "  'location': '32-G882',\n",
       "  'start': {'dateTime': '2016-04-06T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-04-06T17:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'A reminder about the ML Seminar tomorrow (Wednesday) at 4pm in 32-G882\\n(the Hewlett Room).\\n\\nBest,\\nTamara\\n\\n\\n---------- Forwarded message ----------\\nFrom:  <calendar at csail.mit.edu>\\nDate: Fri, Apr 1, 2016 at 12:01 AM\\nSubject: TALK: Wednesday 04-06-2016 ML seminar: Truncated Random Measures\\nTo: seminars at csail.mit.edu\\n\\n\\nMachine Learning seminar: Truncated Random Measures\\n\\nSpeaker: Jonathan Huggins\\nSpeaker Affiliation: MIT\\nHost: Tamara Broderick\\n\\n\\nDate: Wednesday, April 06, 2016\\nTime:  4:00 PM to 5:15 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-G882\">Seminar\\nRoom G882 (Hewlett Room)</a>\\n\\nAbstract:\\nIn applications ranging from information retrieval to social network\\nanalysis to healthcare, we would like to make more complex and\\ndetailed inferences from data as we get more of it. Bayesian\\nnonparametric models accomplish this goal by employing an\\ninfinite-dimensional parameter. Typically the parameter is in the form\\nof a completely random measure (CRM) or a normalized completely random\\nmeasure. We detail three classes of sequential CRM representations\\nthat can be used in practice for simulation and posterior inference.\\nThese representations subsume existing ones that have previously been\\ndeveloped in an ad hoc manner for specific processes. Since a complete\\ninfinite-dimensional CRM cannot be used explicitly for computation,\\nsequential representations are often truncated for tractability. We\\nprovide truncation error analyses for each type of sequential\\nrepresentation, as well as their normalized versions, thereby\\ngeneralizing and improving upon existing truncation error bounds in\\nthe literature. We analyze the computational complexity of the\\nsequential representations, which in conjunction with our error bounds\\nallows us to study which representations are the most efficient. We\\ndiscuss some applications of our theoretical results to popular\\n(normalized) CRMs, demonstrating that our results provide a suite of\\ntools that allow for the straightforward representation and analysis\\nof CRMs that have not previously been used in a Bayesian nonparametric\\ncontext.\\n\\nRelevant URL:\\nFor more information please contact: Stefanie S. Jegelka, <a\\nhref=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n\\n_______________________________________________\\nSeminars mailing list\\nSeminars at lists.csail.mit.edu\\nhttps://lists.csail.mit.edu/mailman/listinfo/seminars\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-04-05',\n",
       "  'id': 15},\n",
       " {'summary': 'Fwd: TALK: Wednesday 04-20-2016 MIT-MSR ML seminar',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-April/000083.html',\n",
       "  'location': '32-G449',\n",
       "  'start': {'dateTime': '2016-04-20T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-04-20T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'MIT-MSR Machine Learning seminar\\n\\nSpeaker: Matt Taddy\\nSpeaker Affiliation: MSR\\n  \\nDate: Wednesday, April 20, 2016\\nTime:  3:00 PM to 4:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-G449\">Seminar Room G449 (Patil/Kiva)</a>\\n\\nAbstract\\n\\nBig Data is often characterized by large sample sizes, high dimensions, and strange variable distributions. For example, an e-commerce website has 10-100s million observations weekly on a huge number of variables with density spikes at zero and elsewhere and very fat tails. These properties -- big and strange -- beg for nonparametric analysis. We revisit a flavor of distribution-free Bayesian nonparametrics that approximates the data generating process (DGP) with a multinomial sampling model. This model then serves as the basis for analysis of statistics -- functionals of the DGP -- that are useful for decision making regardless of the true DGP. The ideas will be illustrated in the measurement of treatment effect heterogeneity, analysis of heavy tailed distributions, and in fitting decision trees.\\n\\n  \\n\\nThe result is a framework for scalable nonparametric Bayesian decision making on massive data.\\n\\n  \\n\\nBiography\\n\\nMatt Taddy joined Microsoft Research in New England from the University of Chicago Booth School of Business, where he is an Associate Professor of Econometrics and Statistics. Taddyâ€™s research is focused on methodology and applications in statistics, econometrics, and machine learning. Heâ€™s especially interested in the intersections of these fields, and in designing systems for analysis that work automatically and on a massive scale. He has collaborated with small start-ups, with large research agencies including Sandia, Los Alamos, and Lawrence Livermore National Labs, and was a research fellow at eBay from 2014-2016. He also created and teaches the MBA Big Data course at Booth. Taddy earned his PhD in Applied Math and Statistics in 2008 from the University of California, Santa Cruz, as well as a BA in Philosophy and Mathematics and an MSc in Mathematical Statistics from McGill University.\\n\\n\\n\\n\\n\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-04-19',\n",
       "  'id': 16},\n",
       " {'summary': 'MSR Talk: Beyond Backpropagation: Uncertainty Propagation – Neil Lawrence, University of Sheffield | Tuesday, April 26 @ 2 PM',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-April/000085.html',\n",
       "  'location': '1-201',\n",
       "  'start': {'dateTime': '2016-04-26T14:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-04-26T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"##########################################################################################################\\n\\n\\n\\nMSR Talk:  Beyond Backpropagation: Uncertainty Propagation – Neil\\nLawrence, University of Sheffield | Tuesday, April 26 @ 2 PM\\n\\n\\n\\n##########################################################################################################\\n\\n\\n\\nWHO:                 Neil Lawrence\\n\\nAFFILIATION:      University of Sheffield\\n\\nTITLE:                  Beyond Backpropagation: Uncertainty Propagation\\n\\nHOST:                 Nicolo Fusi\\n\\nWHEN:               Tuesday, April 26th 2pm\\n\\nWHERE:              Microsoft Conference Center located at One\\nMemorial Drive, First Floor, Cambridge, MA\\n\\nSCHEDULE:         2:00 PM - 3:00 PM\\n\\n\\n\\nAbstract\\n\\nDeep learning is founded on composable functions that are structured\\nto capture regularities in data and can have their parameters\\noptimized by backpropagation (differentiation via the chain rule).\\nTheir recent success is founded on the increased availability of data\\nand computational power. However, they are not very data efficient. In\\nlow data regimes parameters are not well determined and severe\\noverfitting can occur. The solution is to explicitly handle the\\nindeterminacy by converting it to parameter uncertainty and\\npropagating it through the model. Uncertainty propagation is more\\ninvolved than backpropagation because it involves convolving the\\ncomposite functions with probability distributions and integration is\\nmore challenging than differentiation. We will present one approach to\\nfitting such models using Gaussian processes. The resulting models\\nperform very well in both supervised and unsupervised learning on\\nsmall data sets. The remaining challenge is to scale the algorithms to\\nmuch larger data.\\n\\nBiography\\n\\nNeil Lawrence received his bachelor's degree in Mechanical Engineering\\nfrom the University of Southampton in 1994. Following a period as an\\nfield engineer on oil rigs in the North Sea he returned to academia to\\ncomplete his PhD in 2000 at the Computer Lab in Cambridge University.\\nHe spent a year at Microsoft Research in Cambridge before leaving to\\ntake up a Lectureship at the University of Sheffield, where he was\\nsubsequently appointed Senior Lecturer in 2005. In January 2007 he\\ntook up a post as a Senior Research Fellow at the School of Computer\\nScience in the University of Manchester where he worked in the Machine\\nLearning and Optimisation research group. In August 2010 he returned\\nto Sheffield to take up a collaborative Chair in Neuroscience and\\nComputer Science.\\n\\n\\n\\nNeil's main research interest is machine learning through\\nprobabilistic models. He focuses on both the algorithmic side of these\\nmodels and their application. He has a particular focus on\\napplications in personalized health and computational biology, but\\nhappily dabbles in other areas such as speech, vision and graphics.\\n\\n\\n\\nNeil was Associate Editor in Chief for IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence (from 2011-2013) and is an Action\\nEditor for the Journal of Machine Learning Research. He was the\\nfounding editor of the JMLR Workshop and Conference Proceedings (2006)\\nand is currently series editor. He was an area chair for the NIPS\\nconference in 2005, 2006, 2012 and 2013, Workshops Chair in 2010 and\\nTutorials Chair in 2013. He was General Chair of AISTATS in 2010 and\\nAISTATS Programme Chair in 2012. He was Program Chair of NIPS in 2014\\nand was General Chair for 2015.\\n\\n\\n\\n#######################################################################\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-04-21',\n",
       "  'id': 17},\n",
       " {'summary': 'Wednesday 04-27-2016 Machine Learning seminar: Without-Replacement Sampling for Stochastic Gradient Methods',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-April/000086.html',\n",
       "  'start': {'dateTime': '2016-04-27T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-04-27T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Machine Learning seminar: Without-Replacement Sampling for Stochastic \\nGradient Methods\\n\\nSpeaker: Ohad Shamir\\nDate: Wednesday, April 27, 2016\\nTime: 3:00 PM to 4:00 PM\\nLocation: Seminar Room D463 (Star)\\n\\n\\nAbstract:\\nStochastic gradient methods for machine learning and optimization are \\nusually analyzed assuming data points are sampled with replacement. In \\ncontrast, sampling *without* replacement is far less understood, yet in \\npractice it is very common, often easier to implement, and usually \\nperforms better. In this talk, I'll describe some competitive \\nconvergence guarantees for without-replacement sampling under several \\nscenarios, focusing on the natural regime of few passes over the data. \\nI'll also describe a useful application of these results in the context \\nof distributed optimization with randomly-partitioned data, yielding a \\nnearly-optimal algorithm for regularized least squares, in terms of both \\ncommunication complexity and runtime complexity. The proof techniques \\ncombine ideas from stochastic optimization, adversarial online learning, \\nand transductive learning theory, and can potentially be applied to \\nother stochastic optimization and learning problems.\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-04-22',\n",
       "  'id': 18},\n",
       " {'summary': 'TALK: Monday 05-02-2016 MIT-MSR ML seminar: Santosh Vempala',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-April/000089.html',\n",
       "  'start': {'dateTime': '2016-05-02T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-05-02T16:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'The Zen of Passwords (and the Complexity of Human Computation)\\n\\nSpeaker: Santosh Vempala, Georgia Tech\\n\\nDate: Monday, May 02, 2016\\nTime: 3:00 PM to 4:15 PM\\nLocation: Seminar Room G449 (Patil/Kiva)\\n\\nThe Zen of Passwords (and the Complexity of Human Computation)\\n\\nWhat can a human compute in his/her head with no external aids? As a \\nmotivating application: is there a secure and humanly-usable password \\ngeneration method? How about a humanly-computable one-way function? We \\npropose a rigorous model of human computation and apply it to the \\nproblem of generating and remembering passwords. Our main finding is \\nthat there exist password schemas that are (a) precisely defined (b) \\nhumanly usable, in that they can be learned in minutes and can be used \\nto generate passwords in 10-20 seconds and (c) secure to a well-defined \\nextent, in that individual passwords are hard to guess, and knowing \\nseveral passwords does not enable an adversary to infer others. Our \\nHuman Usability Measure (HUM) allows us to draw sharp boundaries for \\nhuman computability and has other applications, including user-interface \\ndesign and coin-flipping over SMS. It also raises a number of intriguing \\nquestions about human computation.\\n\\nThis is joint work with Manuel Blum, Jeremiah Blocki, Yongkoo Kang, Lisa \\nMasserova, Elan Rosenfeld and Samira Samadi.\\n\\n\\nBiography:\\nSantosh Vempala is a Distinguished Professor of Computer Science in the \\nCollege of Computing at the Georgia Institute of Technology. He joined \\nthe College of Computing in the fall of 2006 as a professor of Computer \\nScience. He spearheaded the Algorithms and Randomness Center and Think \\nTank at Georgia Tech, and served as its first director from 2006 until \\n2011. His research interests include algorithms, randomness, geometry \\nand computing-for-good (C4G). He graduated from CMU in 1997, advised by \\nAvrim Blum, and then taught at at MIT until 2006 except for a year as a \\nMiller fellow at UC Berkeley. Vempala is also a Sloan, Guggenheim, ACM \\nand generally excitable Fellow, especially when a phenomenon that \\nappears complex from one perspective, turns out to be simple from \\nanother. In recent years, he has been trying to understand, with little \\nsuccess, how the brain works and how to model its computational abilities.\\n\\nRelevant URL: http://research.microsoft.com/en-us/events/mmm/',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-04-26',\n",
       "  'id': 19},\n",
       " {'summary': 'Today: Machine Learning seminar: Without-Replacement Sampling for Stochastic Gradient Methods',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-April/000090.html',\n",
       "  'start': {'dateTime': '2016-04-27T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-04-27T16:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Machine Learning seminar: Without-Replacement Sampling for Stochastic Gradient Methods\\n\\nSpeaker: Ohad Shamir\\n  \\nDate: Wednesday, April 27, 2016\\nTime:  3:00 PM to 4:15 PM\\n\\nLocation: Seminar Room D463 (Star)\\n\\n\\nWithout-Replacement Sampling for Stochastic Gradient Methods\\n\\nAbstract:\\nStochastic gradient methods for machine learning and optimization are usually analyzed assuming data points are sampled with replacement. In contrast, sampling *without* replacement is far less understood, yet in practice it is very common, often easier to implement, and usually performs better. In this talk, I'll describe some competitive convergence guarantees for without-replacement sampling under several scenarios, focusing on the natural regime of few passes over the data. I'll also describe a useful application of these results in the context of distributed optimization with randomly-partitioned data, yielding a nearly-optimal algorithm for regularized least squares, in terms of both communication complexity and runtime complexity. The proof techniques combine ideas from stochastic optimization, adversarial online learning, and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems.\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-04-27',\n",
       "  'id': 18},\n",
       " {'summary': 'Tomorrow,\\tMonday 05-02-2016 MIT-MSR ML seminar: Santosh Vempala',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-May/000093.html',\n",
       "  'start': {'dateTime': '2016-05-02T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-05-02T16:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': '> The Zen of Passwords (and the Complexity of Human Computation)\\n>\\n> Speaker: Santosh Vempala, Georgia Tech\\n>\\n> Date: Monday, May 02, 2016\\n> Time: 3:00 PM to 4:15 PM\\n> Location: Seminar Room G449 (Patil/Kiva)\\n>\\n> The Zen of Passwords (and the Complexity of Human Computation)\\n>\\n> What can a human compute in his/her head with no external aids? As a \\n> motivating application: is there a secure and humanly-usable password \\n> generation method? How about a humanly-computable one-way function? We \\n> propose a rigorous model of human computation and apply it to the \\n> problem of generating and remembering passwords. Our main finding is \\n> that there exist password schemas that are (a) precisely defined (b) \\n> humanly usable, in that they can be learned in minutes and can be used \\n> to generate passwords in 10-20 seconds and (c) secure to a \\n> well-defined extent, in that individual passwords are hard to guess, \\n> and knowing several passwords does not enable an adversary to infer \\n> others. Our Human Usability Measure (HUM) allows us to draw sharp \\n> boundaries for human computability and has other applications, \\n> including user-interface design and coin-flipping over SMS. It also \\n> raises a number of intriguing questions about human computation.\\n>\\n> This is joint work with Manuel Blum, Jeremiah Blocki, Yongkoo Kang, \\n> Lisa Masserova, Elan Rosenfeld and Samira Samadi.\\n>\\n>\\n> Biography:\\n> Santosh Vempala is a Distinguished Professor of Computer Science in \\n> the College of Computing at the Georgia Institute of Technology. He \\n> joined the College of Computing in the fall of 2006 as a professor of \\n> Computer Science. He spearheaded the Algorithms and Randomness Center \\n> and Think Tank at Georgia Tech, and served as its first director from \\n> 2006 until 2011. His research interests include algorithms, \\n> randomness, geometry and computing-for-good (C4G). He graduated from \\n> CMU in 1997, advised by Avrim Blum, and then taught at at MIT until \\n> 2006 except for a year as a Miller fellow at UC Berkeley. Vempala is \\n> also a Sloan, Guggenheim, ACM and generally excitable Fellow, \\n> especially when a phenomenon that appears complex from one \\n> perspective, turns out to be simple from another. In recent years, he \\n> has been trying to understand, with little success, how the brain \\n> works and how to model its computational abilities.\\n>\\n> Relevant URL: http://research.microsoft.com/en-us/events/mmm/\\n>',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-05-01',\n",
       "  'id': 19},\n",
       " {'summary': 'TALK: Friday 07-01-2016 Summarizing Big Data Using Submodular Functions',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-June/000100.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2016-07-01T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-07-01T12:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Summarizing Big Data Using Submodular Functions\\n\\nSpeaker: Baharan Mirzasoleiman\\nSpeaker Affiliation: ETH Zurich\\nHost: Stefanie Jegelka\\nHost Affiliation: MIT\\n \\nDate: Friday, July 01, 2016\\nTime:  11:00 AM to 12:15 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar Room D507</a>\\n\\nAbstract: Many large-scale machine learning problems–clustering, non-parametric learning, kernel machines, etc.–require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Submodular functions exhibit a natural diminishing returns property: the marginal benefit of any given element decreases as we select more and more elements. Although maximizing a submodular function is NP-hard in general, a simple greedy algorithm produces solutions competitive with the optimal (intractable) solution. However, this greedy algorithm is impractical for truly large-scale problems. In this talk, we consider scaling submodular optimization techniques. We discuss some of the recent fast methods for maximizaing a (not necessarily monotone) submodular function in a centralized manner. We then consider the case where the data is residing on disk, or arriving over time at a fast pace, and discuss submodular maximization in distributed and streaming fashions. We briefly overview the theoretical results, as well as the effectiveness of these techniques on several real-world applications. \\n\\n\\nBio: Baharan Mirzasoleiman is a Ph.D. student in the Learning and Adaptive Systems Group at ETH Zurich, working with Professor Andreas Krause. She is interested in designing and building large-scale machine learning and data mining algorithms – specially in summarizing large-scale data by selecting a representative subset out of a massive data set using techniques from submodular optimization. She was awarded the Google Anita Borg Memorial Scholarship at 2014. Before that, she completed her B.Sc. in computer engineering at University of Tehran and her M.Sc. in computer engineering from Sharif University of Technology where she worked on influence and information propagation through social networks. \\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-06-20',\n",
       "  'id': 20},\n",
       " {'summary': 'TALK: Friday 07-01-2016 Summarizing Big Data Using Submodular Functions',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-June/000101.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2016-07-01T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-07-01T12:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Summarizing Big Data Using Submodular Functions\\n\\nSpeaker: Baharan Mirzasoleiman\\nSpeaker Affiliation: ETH Zurich\\nHost: Stefanie Jegelka\\nHost Affiliation: MIT\\n \\nDate: Friday, July 01, 2016\\nTime:  11:00 AM to 12:15 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar Room D507</a>\\n\\nAbstract: Many large-scale machine learning problems–clustering, non-parametric learning, kernel machines, etc.–require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Submodular functions exhibit a natural diminishing returns property: the marginal benefit of any given element decreases as we select more and more elements. Although maximizing a submodular function is NP-hard in general, a simple greedy algorithm produces solutions competitive with the optimal (intractable) solution. However, this greedy algorithm is impractical for truly large-scale problems. In this talk, we consider scaling submodular optimization techniques. We discuss some of the recent fast methods for maximizaing a (not necessarily monotone) submodular function in a centralized manner. We then consider the case where the data is residing on disk, or arriving over time at a fast pace, and discuss submodular maximization in distributed and streaming fashions. We briefly overview the theoretical results, as well as the effectiveness of these techniques on several real-world applications. \\n\\n\\nBio: Baharan Mirzasoleiman is a Ph.D. student in the Learning and Adaptive Systems Group at ETH Zurich, working with Professor Andreas Krause. She is interested in designing and building large-scale machine learning and data mining algorithms – specially in summarizing large-scale data by selecting a representative subset out of a massive data set using techniques from submodular optimization. She was awarded the Google Anita Borg Memorial Scholarship at 2014. Before that, she completed her B.Sc. in computer engineering at University of Tehran and her M.Sc. in computer engineering from Sharif University of Technology where she worked on influence and information propagation through social networks. \\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-06-24',\n",
       "  'id': 20},\n",
       " {'summary': 'TALK: Thursday 06-30-2016 Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-June/000102.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2016-06-30T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-06-30T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning\\n\\nSpeaker: Wouter Koolen\\nSpeaker Affiliation: CWI Amsterdam\\nHost: Stefanie Jegelka\\n\\n \\nDate: Thursday, June 30, 2016\\nTime:  3:00 PM to 4:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar Room D507</a>\\n\\nAbstract:\\n\\nWe consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.\\n\\n\\nRelevant URL: http://wouterkoolen.info/\\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-06-24',\n",
       "  'id': 21},\n",
       " {'summary': '2 talk announcements',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-June/000104.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2016-06-30T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-06-30T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'There will be 2 talks, tomorrow (Thu) and Friday. Please come by if you \\nare around, and let me know if you\\'d like to meet any of the speakers!\\n\\nStefanie\\n\\n\\nCombining Adversarial Guarantees and Stochastic Fast Rates in Online \\nLearning\\n\\nSpeaker: Wouter Koolen\\nSpeaker Affiliation: CWI Amsterdam\\nDate: Thursday, June 30, 2016\\nTime:  3:00 PM to 4:00 PM\\n\\nLocation: <a \\nhref=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar \\nRoom D507</a>\\n\\nAbstract:\\n\\nWe consider online learning algorithms that guarantee worst-case regret \\nrates in adversarial environments (so they can be deployed safely and \\nwill perform robustly), yet adapt optimally to favorable stochastic \\nenvironments (so they will perform well in a variety of settings of \\npractical importance). We quantify the friendliness of stochastic \\nenvironments by means of the well-known Bernstein (a.k.a. generalized \\nTsybakov margin) condition. For two recent algorithms (Squint for the \\nHedge setting and MetaGrad for online convex optimization) we show that \\nthe particular form of their data-dependent individual-sequence regret \\nguarantees implies that they adapt automatically to the Bernstein \\nparameters of the stochastic environment. We prove that these algorithms \\nattain fast rates in their respective settings both in expectation and \\nwith high probability.\\n\\n\\n----------------\\n\\nSummarizing Big Data Using Submodular Functions\\n\\nSpeaker: Baharan Mirzasoleiman\\nSpeaker Affiliation: ETH Zurich\\nDate: Friday, July 01, 2016\\nTime:  11:00 AM to 12:00 PM\\n\\nLocation: <a \\nhref=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar \\nRoom D507</a>\\n\\nAbstract: Many large-scale machine learning problems–clustering, \\nnon-parametric learning, kernel machines, etc.–require selecting a small \\nyet representative subset from a large dataset. Such problems can often \\nbe reduced to maximizing a submodular set function subject to various \\nconstraints. Submodular functions exhibit a natural diminishing returns \\nproperty: the marginal benefit of any given element decreases as we \\nselect more and more elements. Although maximizing a submodular function \\nis NP-hard in general, a simple greedy algorithm produces solutions \\ncompetitive with the optimal (intractable) solution. However, this \\ngreedy algorithm is impractical for truly large-scale problems. In this \\ntalk, we consider scaling submodular optimization techniques. We discuss \\nsome of the recent fast methods for maximizing a (not necessarily \\nmonotone) submodular function in a centralized manner. We then consider \\nthe case where the data is residing on disk, or arriving over time at a \\nfast pace, and discuss submodular maximization in distributed and \\nstreaming fashions. We briefly overview the theoretical results, as well \\nas the effectiveness of these techniques on several real-world \\napplications.',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-06-29',\n",
       "  'id': 20},\n",
       " {'summary': 'TALK: Thursday 06-30-2016 Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-June/000105.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2016-06-30T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-06-30T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning\\n\\nSpeaker: Wouter Koolen\\nSpeaker Affiliation: CWI Amsterdam\\nHost: Stefanie Jegelka\\n\\n \\nDate: Thursday, June 30, 2016\\nTime:  3:00 PM to 4:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar Room D507</a>\\n\\nAbstract:\\n\\nWe consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.\\n\\n\\nRelevant URL: http://wouterkoolen.info/\\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-06-30',\n",
       "  'id': 21},\n",
       " {'summary': 'TALK: Friday 07-01-2016 Summarizing Big Data Using Submodular Functions',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-July/000106.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2016-07-01T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-07-01T12:15:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Summarizing Big Data Using Submodular Functions\\n\\nSpeaker: Baharan Mirzasoleiman\\nSpeaker Affiliation: ETH Zurich\\nHost: Stefanie Jegelka\\nHost Affiliation: MIT\\n \\nDate: Friday, July 01, 2016\\nTime:  11:00 AM to 12:15 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar Room D507</a>\\n\\nAbstract: Many large-scale machine learning problems–clustering, non-parametric learning, kernel machines, etc.–require selecting a small yet representative subset from a large dataset. Such problems can often be reduced to maximizing a submodular set function subject to various constraints. Submodular functions exhibit a natural diminishing returns property: the marginal benefit of any given element decreases as we select more and more elements. Although maximizing a submodular function is NP-hard in general, a simple greedy algorithm produces solutions competitive with the optimal (intractable) solution. However, this greedy algorithm is impractical for truly large-scale problems. In this talk, we consider scaling submodular optimization techniques. We discuss some of the recent fast methods for maximizaing a (not necessarily monotone) submodular function in a centralized manner. We then consider the case where the data is residing on disk, or arriving over time at a fast pace, and discuss submodular maximization in distributed and streaming fashions. We briefly overview the theoretical results, as well as the effectiveness of these techniques on several real-world applications. \\n\\n\\nBio: Baharan Mirzasoleiman is a Ph.D. student in the Learning and Adaptive Systems Group at ETH Zurich, working with Professor Andreas Krause. She is interested in designing and building large-scale machine learning and data mining algorithms – specially in summarizing large-scale data by selecting a representative subset out of a massive data set using techniques from submodular optimization. She was awarded the Google Anita Borg Memorial Scholarship at 2014. Before that, she completed her B.Sc. in computer engineering at University of Tehran and her M.Sc. in computer engineering from Sharif University of Technology where she worked on influence and information propagation through social networks. \\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-07-01',\n",
       "  'id': 20},\n",
       " {'summary': 'TALK: ML+Vision talk, 32-D677, 11:00am (6th JULY)',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-July/000107.html',\n",
       "  'location': '32-D677',\n",
       "  'start': {'dateTime': '2016-07-06T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-07-06T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Dear all,\\n\\nYou’re invited to attend the following machine learning + computer vision seminar:\\n\\nTitle: Tensor Representations for Summarizing Fine-grained Actions\\nSpeaker: Dr. Anoop Cherian, Australian National University (ANU).\\n\\nTime: 11:00am, JUL 6th, 2016\\nLocation: 32-D677\\n\\nAbstract:\\nIn this talk, we look at a difficult subset of action recognition problems, namely fine-grained action recognition, which is characterized by actions that have subtle inter-class differences (e.g., cutting versus slicing vegetables) and strong intra-class appearance variations (e.g., slicing cucumbers versus slicing tomatoes). The problem is also often more complicated by occlusions of objects and human body-parts, and presence of hard to detect tools (such as knives, peelers, etc.). Recognizing such actions is important for several applications, including visual surveillance, human-robot interaction, and elderly health-monitoring systems. We describe a representational approach to solving this problem.\\nThe main idea is to use spatio-temporal co-occurrences of visual features, that is used to generate compact tensor representations, which can be used in a simple classifier to discriminate such actions. Two such representations will be introduced, namely (i) using second-order co-occurrences, as captured by the correlations between the predictions of a deep-learned convolutional neural network model, and (ii) using third-order co-occurrences of features extracted from 3D pose-skeleton action sequences. Both these schemes are simple, easy to implement, and our experimental results demonstrate that they achieve state-of-the-art accuracy on several benchmark datasets.\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-07-05',\n",
       "  'id': 22},\n",
       " {'summary': 'two ML/optimization talks next week',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-July/000108.html',\n",
       "  'start': {'dateTime': '2016-07-13T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-07-13T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: Faster Stochastic Methods for Nonconvex Optimization in Machine Learning\\n\\nSpeaker: Shashank Reddi\\nSpeaker Affiliation: Carnegie Mellon University (CMU)\\n  \\nDate: Wednesday, July 13, 2016\\nTime:  3:00 PM to 4:00 PM\\n\\nLocation: Seminar Room G882 (Hewlett Room)\\n\\nAbstract:\\nI will talk about nonconvex empirical loss minimization problems that typically arise in machine learning e.g., in deep learning. (Proximal) Stochastic gradient descent (SGD) is the de-facto algorithm for such problems in the nonconvex setting. However, SGD suffers from slow convergence due to variance of its stochastic gradients. To tackle this issue, we build on ideas from variance reduction literature on convex optimization and develop new nonconvex stochastic methods that enjoy provably faster convergence (to a stationary point), faster than both SGD and its deterministic counterpart, gradient descent. This result marks the first theoretical improvement in this line of research. We also analyze subclasses of nonconvex problems on which our stochastic methods attain linear convergence to the global optimum! Our analysis also shows that our methods enjoy efficient minibatching, exhibiting (theoretical) linear speedups in the nonconvex setting. We conclude with some empirical results, which are promising and encourage a much wider study of our methods.\\n\\nJoint work with Suvrit Sra (MIT), Barnabas Poczos (CMU), Alex Smola (CMU) and Ahmed Hefny (CMU).\\n\\nBio: Sashank Reddi is a Ph.D. student in the Machine Learning department at Carnegie Mellon University,  advised by Alex Smola and Barnabas Poczos. Prior to that, he completed his undergraduate studies at IIT Bombay. He is broadly interested in statistical and computational aspects of Machine Learning, that includes large-scale nonconvex optimization and high dimensional statistics. He recently co-organized the Neural Information Processing Systems (NIPS) workshop on \"Optimization for Machine Learning\".\\n\\n----\\n----\\n\\n\\nML seminar: A variational perspective on accelerated methods in optimization\\n\\nSpeaker: Andre Wibisono\\nSpeaker Affiliation: UW Madison\\n  \\nDate: Thursday, July 14, 2016\\nTime:  3:00 PM to 4:00 PM\\n\\nLocation: Seminar Room G882 (Hewlett Room)\\n\\nAbstract:\\nAccelerated gradient methods play a central role in optimization, achieving optimal rates in many settings.  While many generalizations and extensions of Nesterov\\'s original acceleration method have been proposed, it is not yet clear what is the natural scope of the acceleration concept. In this work, we study accelerated methods from a continuous-time perspective. We show that there is a Lagrangian functional that we call the “Bregman Lagrangian” which generates a large class of accelerated methods in continuous time, including (but not limited to) accelerated gradient descent, its non-Euclidean extension, and accelerated higher-order gradient methods. We show that the continuous-time limit of all of these methods correspond to traveling the same curve in spacetime at different speeds. From this perspective, Nesterov\\'s technique and many of its generalizations can be viewed as a systematic way to go from the continuous-time curves generated by the Bregman Lagrangian to a family of discrete-time accelerated algorithms.\\n\\nBio: Andre Wibisono is a postdoctoral fellow at the ECE department and the Wisconsin Institute for Discovery at UW Madison. He completed his PhD in computer science at UC Berkeley in May 2016, advised by Michael Jordan. Prior to that, he received his M.Eng. from MIT in 2010, advised by Tomaso Poggio.',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-07-07',\n",
       "  'id': 23},\n",
       " {'summary': 'Reminder: ML & OPT talks 13th July and 14th July',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-July/000110.html',\n",
       "  'start': {'dateTime': '2016-07-13T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-07-13T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Two awesome ML seminar talks coming up. Don’t miss!\\n\\nSuvrit\\n–\\n\\nML seminar: Faster Stochastic Methods for Nonconvex Optimization in Machine Learning\\n\\nSpeaker: Sashank Reddi\\nSpeaker Affiliation: Carnegie Mellon University (CMU)\\nDate: Wednesday, July 13, 2016\\nTime:  3:00 PM to 4:00 PM\\n\\nLocation: Seminar Room G882 (Hewlett Room)\\n\\nAbstract:\\nI will talk about nonconvex empirical loss minimization problems that typically arise in machine learning e.g., in deep learning. (Proximal) Stochastic gradient descent (SGD) is the de-facto algorithm for such problems in the nonconvex setting. However, SGD suffers from slow convergence due to variance of its stochastic gradients. To tackle this issue, we build on ideas from variance reduction literature on convex optimization and develop new nonconvex stochastic methods that enjoy provably faster convergence (to a stationary point), faster than both SGD and its deterministic counterpart, gradient descent. This result marks the first theoretical improvement in this line of research. We also analyze subclasses of nonconvex problems on which our stochastic methods attain linear convergence to the global optimum! Our analysis also shows that our methods enjoy efficient minibatching, exhibiting (theoretical) linear speedups in the nonconvex setting. We conclude with some empirical results, which are promising and encourage a much wider study of our methods.\\n\\nJoint work with Suvrit Sra (MIT), Barnabas Poczos (CMU), Alex Smola (CMU) and Ahmed Hefny (CMU).\\n\\nBio: Sashank Reddi is a Ph.D. student in the Machine Learning department at Carnegie Mellon University,  advised by Alex Smola and Barnabas Poczos. Prior to that, he completed his undergraduate studies at IIT Bombay. He is broadly interested in statistical and computational aspects of Machine Learning, that includes large-scale nonconvex optimization and high dimensional statistics. He recently co-organized the Neural Information Processing Systems (NIPS) workshop on \"Optimization for Machine Learning\".\\n\\n\\n\\n\\nML seminar: A variational perspective on accelerated methods in optimization\\n\\nSpeaker: Andre Wibisono\\nSpeaker Affiliation: UW Madison\\nDate: Thursday, July 14, 2016\\nTime:  3:00 PM to 4:00 PM\\n\\nLocation: Seminar Room G882 (Hewlett Room)\\n\\nAbstract:\\nAccelerated gradient methods play a central role in optimization, achieving optimal rates in many settings.  While many generalizations and extensions of Nesterov\\'s original acceleration method have been proposed, it is not yet clear what is the natural scope of the acceleration concept. In this work, we study accelerated methods from a continuous-time perspective. We show that there is a Lagrangian functional that we call the “Bregman Lagrangian” which generates a large class of accelerated methods in continuous time, including (but not limited to) accelerated gradient descent, its non-Euclidean extension, and accelerated higher-order gradient methods. We show that the continuous-time limit of all of these methods correspond to traveling the same curve in spacetime at different speeds. From this perspective, Nesterov\\'s technique and many of its generalizations can be viewed as a systematic way to go from the continuous-time curves generated by the Bregman Lagrangian to a family of discrete-time accelerated algorithms.\\n\\nBio: Andre Wibisono is a postdoctoral fellow at the ECE department and the Wisconsin Institute for Discovery at UW Madison. He completed his PhD in computer science at UC Berkeley in May 2016, advised by Michael Jordan. Prior to that, he received his M.Eng. from MIT in 2010, advised by Tomaso Poggio.\\n\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-07-10',\n",
       "  'id': 23},\n",
       " {'summary': 'Yann LeCun Talk',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-July/000111.html',\n",
       "  'location': '10-250',\n",
       "  'start': {'dateTime': '2016-07-10T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-07-10T18:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Lecture title: \"The Next Frontier in AI: Unsupervised Learning\"\\n\\nYann LeCun\\n         Facebook AI Research & New York University\\n\\nMonday, July 11, 2016:  5:00-6:00 p.m.\\n\\nMIT Room: 10-250\\n\\n----------------------\\n\\nAbstract:\\n\\nThe rapid progress of AI in the last few years are largely the result of\\nadvances in deep learning and neural nets, combined with the availability\\nof large datasets and fast GPUs. We now have systems that can recognize\\nimages with an accuracy that rivals that of humans. This will lead to\\nrevolutions in several domains such as autonomous transportation and\\nmedical image understanding. But all of these systems currently use\\nsupervised learning in which the machine is trained with inputs labeled\\nby humans. The challenge of the next several years is to let machines\\nlearn from raw, unlabeled data, such as video or text. This is known as\\nunsupervised learning. AI systems today do not possess \"common sense\",\\nwhich humans and animals acquire by observing the world, acting in it,\\nand understanding the physical constraints of it. Some of us see\\nunsupervised learning as the key towards machines with common sense.\\nApproaches to unsupervised learning will be reviewed. This presentation\\nassumes some familiarity with the basic concepts of deep learning.\\n\\n\\n----------------------\\n\\nAbout the speaker:\\n\\nYann LeCun received the Electrical Engineer Diploma from Ecole\\nSupérieured\\'Ingénieurs en Electrotechnique et Electronique, and a PhD in\\nComputer Science from Université Pierre et Marie Curie. After a postdoc at\\nthe University of Toronto, he joined AT&T Bell Laboratories in Holmdel, NJ\\nin 1988. He became head of the Image Processing Research Department at\\nAT&T Labs-Research in 1996, and joined NYU as a professor in 2003, after a\\nbrief period as a Fellow of the NEC Research Institute in Princeton.\\n From 2012 to 2014 he directed NYU\\'s initiative in data science and became\\nthe founding director of the NYU Center for Data Science. He was named\\nDirector of AI Research at Facebook in late 2013 and retains a part-time\\nposition on the NYU faculty.\\n\\nSince the mid 1980\\'s he has been working on deep learning methods,\\nparticularly the convolutional network model, which is the basis of many\\nproducts and services deployed by companies such as Facebook, Google,\\nMicrosoft, Baidu, IBM, NEC, AT&T and others for image and video\\nunderstanding, document recognition, human-computer interaction, and\\nspeech recognition.',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-07-10',\n",
       "  'id': 24},\n",
       " {'summary': 'Machine Learning Seminar: Monday, 32-D677, 11am',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-August/000117.html',\n",
       "  'location': '32-D677',\n",
       "  'start': {'dateTime': '2016-08-08T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-08-08T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Dear all,\\n\\nYou are invited to attend the next ML seminar by Alan Malek (UC Berkeley). Here are the details.\\n\\nTitle: Minimax Strategies for Online Linear Regression, square loss prediction,and time series prediction\\nSpeaker: Alan Malek (UC Berkeley)<https://people.eecs.berkeley.edu/~malek/>\\nTime/loc: 32-D677, 11:00–11:45am\\n\\nABSTRACT\\nWe study minimax algorithms for sequential prediction with squared loss. In the game-theoretic formulation, at every round 1 through T, the learner plays an action, the opponent observes this action and plays a response, and the learner incurs the square difference as a loss. The learner's hope is to have low regret, which is the difference between his loss and the best loss in some class. However, instead of searching for an algorithm with upper and lower bounds of matching orders, we focus on the more robust notion of the minimax optimal algorithm. This algorithm guarantees the smallest regret against the worst sequence of responses, and playing it requires computing the optimal action at every possible game history (i.e. solving the game tree). Since minimax algorithms are entirely derived from the structure of the game, they have a natural, game-dependent regularization and no reliance on tuning parameters.\\n\\nWhile computing minimax algorithms typically has exponential complexity, we demonstrate tractable algorithms in three cases, online linear regression, online square loss prediction, and time-series prediction, and discuss what game-dependent regularization emerges.\\n––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\\n\\n\\n\\n\\n\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-08-12',\n",
       "  'id': 25},\n",
       " {'summary': 'Reminder: **TODAY** Machine Learning Seminar: Monday,\\t32-D677, 11am',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-August/000118.html',\n",
       "  'location': '32-D677',\n",
       "  'start': {'dateTime': '2016-08-15T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-08-15T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Reminder: the talk is **today** (15th Aug, 2016).\\n\\nBegin forwarded message:\\n\\nFrom: Suvrit Sra <suvrit at mit.edu<mailto:suvrit at mit.edu>>\\nSubject: Machine Learning Seminar: Monday, 32-D677, 11am\\nDate: August 12, 2016 at 14:20:04 EDT\\nTo: lids-seminars <lids-seminars at exchange.mit.edu<mailto:lids-seminars at exchange.mit.edu>>, <mitml at mit.edu<mailto:mitml at mit.edu>>, <mltea at lists.csail.mit.edu<mailto:mltea at lists.csail.mit.edu>>\\n\\nDear all,\\n\\nYou are invited to attend the next ML seminar by Alan Malek (UC Berkeley). Here are the details.\\n\\nTitle: Minimax Strategies for Online Linear Regression, square loss prediction,and time series prediction\\nSpeaker: Alan Malek (UC Berkeley)<https://people.eecs.berkeley.edu/~malek/>\\nTime/loc: 32-D677, 11:00–11:45am, 15th Aug, 2016.\\n\\nABSTRACT\\nWe study minimax algorithms for sequential prediction with squared loss. In the game-theoretic formulation, at every round 1 through T, the learner plays an action, the opponent observes this action and plays a response, and the learner incurs the square difference as a loss. The learner's hope is to have low regret, which is the difference between his loss and the best loss in some class. However, instead of searching for an algorithm with upper and lower bounds of matching orders, we focus on the more robust notion of the minimax optimal algorithm. This algorithm guarantees the smallest regret against the worst sequence of responses, and playing it requires computing the optimal action at every possible game history (i.e. solving the game tree). Since minimax algorithms are entirely derived from the structure of the game, they have a natural, game-dependent regularization and no reliance on tuning parameters.\\n\\nWhile computing minimax algorithms typically has exponential complexity, we demonstrate tractable algorithms in three cases, online linear regression, online square loss prediction, and time-series prediction, and discuss what game-dependent regularization emerges.\\n––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\\n\\n\\n\\n\\n\\n\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-08-15',\n",
       "  'id': 25},\n",
       " {'summary': 'TALK: Wednesday 09-14-2016 Safe Decision Making Under\\tUncertainty',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-August/000121.html',\n",
       "  'location': '32-D507',\n",
       "  'start': {'dateTime': '2016-09-14T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-09-14T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: Safe Decision Making Under Uncertainty\\n\\nSpeaker: Ashish Kapoor\\nSpeaker Affiliation: MSR\\nHost: Stefanie Jegelka\\n\\n \\nDate: Wednesday, September 14, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-D507\">Seminar Room D507</a>\\n\\nAbstract:\\nMachine Learning is one of the key component that enables systems that operate under uncertainty. For  example,  robotic systems  might employ  sensors  together with  a  machine  learned system  to  identify  obstacles.  However, such data driven system are far from perfect and can result in failure cases that can jeopardize safety. In this talk we will explore a framework  that  aims to preserve safety invariants despite the uncertainties in the environment arising due to incomplete information.  We will first describe a method  to reason about safe plans and control strategies despite perceiving the world through noisy sensors and machine learning systems. At  the  heart  of  our  approach  is  the new Probabilistic Signal Temporal Logic (PrSTL), an expressive language to define stochastic properties, and enforce probabilistic guarantees  on  them. Next, we will consider extensions of these ideas to a sequential decision making framework that considers the trade-off in ri!\\n sk and reward in a near-optimal manner. We will demonstrate our approach by deriving safe plans and controls for quadrotors and  autonomous  vehicles  in  dynamic  environments.\\n \\nBio: Ashish Kapoor is a senior researcher at Microsoft Research, Redmond. Currently, his research focuses on Aerial Informatics and Robotics with an emphasis on building intelligent and autonomous flying agents that are safe and enable applications that can positively influence our society. The research builds upon cutting edge research in machine intelligence, robotics and human-centered computation in order to enable an entire fleet of flying robots that range from micro-UAVs to commercial jetliners. Various applications scenarios include Weather Sensing, Monitoring for Precision Agriculture, Safe Cyber-Physical Systems etc.  Ashish received his PhD from MIT Media Laboratory in 2006. He also holds FAA Commercial Pilot certificate (SEL), FAA Flight Instructor certificate (Airplane Single Engine and Instrument Airplane) and is an avid amateur aircraft builder.\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-08-29',\n",
       "  'id': 26},\n",
       " {'summary': 'TALK: Friday 09-16-2016 ML seminar: Le Song',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000123.html',\n",
       "  'location': '32-G882',\n",
       "  'start': {'dateTime': '2016-09-16T14:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-09-16T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: Discriminative Embedding of Latent Variable Models for Structured Data\\n\\nSpeaker: Le Song\\nSpeaker Affiliation: Georgia Tech\\nHost: Stefanie Jegelka\\n\\n \\nDate: Friday, September 16, 2016\\nTime:  2:00 PM to 3:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-G882\">Seminar Room G882 (Hewlett Room)</a>\\n\\nAbstract: \\nStructured data, such as sequences, trees, graphs and hypergraphs, are prevalent in a number of interdisciplinary areas such as network analysis, knowledge engineering, computational biology, drug design and materials science. The availability of large amount of such structured data has posed great challenges for the machine learning community. How to represent such data to capture their similarities or differences? How to learn predictive models from a large amount of such data, and efficiently? How to learn to generate structured data de novo given certain desired properties? \\n\\nA common approach to tackle these challenges is to first design a similarity measure, called the kernel function, between two data points, based on either statistics of the substructures or probabilistic generative models; and then a machine learning algorithm will optimize a predictive model based on such similarity measure. However, this elegant two-stage approach has difficulty scaling up, and discriminative information is also not exploited during the design of similarity measure. \\n\\nIn this talk, I will present Structure2Vec, an effective and scalable approach for representing structured data based on the idea of embedding latent variable models into a feature space, and learning such feature space using discriminative information. Interestingly, Structure2Vec extracts features by performing a sequence of nested nonlinear operations in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving genome and protein sequences, drug molecules and energy materials, Structure2Vec consistently produces the-state-of-the-art predictive performance. Furthermore, in the materials property prediction problem involving 2.3 million data points, Structure2Vec is able to produces a more accurate model yet being 10,000 times smaller. In the end, I will also discuss potential improvements over current work, possible extensions to network analysis and computer vision, and thoughts on the structured data design problem.\\n\\n \\nBio: \\nLe Song is an assistant professor in the Department of Computational Science and Engineering, College of Computing, Georgia Institute of Technology. He received his Ph.D. in Machine Learning from University of Sydney and NICTA in 2008, and then conducted his post-doctoral research in the Department of Machine Learning, Carnegie Mellon University, between 2008 and 2011. Before he joined Georgia Institute of Technology, he was a research scientist at Google. His principal research direction is machine learning, especially kernel methods and probabilistic graphical models for large scale and complex problems, arising from artificial intelligence, network analysis, computational biology and other interdisciplinary domains. He is the recipient of the AISTATS\\'16 Best Student Paper Award, IPDPS\\'15 Best Paper Award, NSF CAREER Award’14, NIPS’13 Outstanding Paper Award, and ICML’10 Best Paper Award. He has also served as the area chair or senior program committee for many leading machine learning and AI conferences such as ICML, NIPS, AISTATS and AAAI, and the action editor for JMLR.\\n\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-01',\n",
       "  'id': 27},\n",
       " {'summary': 'TALK: Wednesday 09-14-2016 Safe Decision Making Under\\tUncertainty',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000127.html',\n",
       "  'location': '32-G575',\n",
       "  'start': {'dateTime': '2016-09-14T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-09-14T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: Safe Decision Making Under Uncertainty\\n\\nSpeaker: Ashish Kapoor\\nSpeaker Affiliation: MSR\\nHost: Stefanie Jegelka\\n\\n \\nDate: Wednesday, September 14, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/conference_rooms/32-G575\">Conference Room G575</a>\\n\\nAbstract:\\nMachine Learning is one of the key component that enables systems that operate under uncertainty. For  example,  robotic systems  might employ  sensors  together with  a  machine  learned system  to  identify  obstacles.  However, such data driven system are far from perfect and can result in failure cases that can jeopardize safety. In this talk we will explore a framework  that  aims to preserve safety invariants despite the uncertainties in the environment arising due to incomplete information.  We will first describe a method  to reason about safe plans and control strategies despite perceiving the world through noisy sensors and machine learning systems. At  the  heart  of  our  approach  is  the new Probabilistic Signal Temporal Logic (PrSTL), an expressive language to define stochastic properties, and enforce probabilistic guarantees  on  them. Next, we will consider extensions of these ideas to a sequential decision making framework that considers the trade-off in ri!\\n sk and reward in a near-optimal manner. We will demonstrate our approach by deriving safe plans and controls for quadrotors and  autonomous  vehicles  in  dynamic  environments.\\n \\nBio: Ashish Kapoor is a senior researcher at Microsoft Research, Redmond. Currently, his research focuses on Aerial Informatics and Robotics with an emphasis on building intelligent and autonomous flying agents that are safe and enable applications that can positively influence our society. The research builds upon cutting edge research in machine intelligence, robotics and human-centered computation in order to enable an entire fleet of flying robots that range from micro-UAVs to commercial jetliners. Various applications scenarios include Weather Sensing, Monitoring for Precision Agriculture, Safe Cyber-Physical Systems etc.  Ashish received his PhD from MIT Media Laboratory in 2006. He also holds FAA Commercial Pilot certificate (SEL), FAA Flight Instructor certificate (Airplane Single Engine and Instrument Airplane) and is an avid amateur aircraft builder.\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-07',\n",
       "  'id': 26},\n",
       " {'summary': 'TALK: Friday 09-16-2016 ML seminar: Le Song',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000129.html',\n",
       "  'location': '32-G882',\n",
       "  'start': {'dateTime': '2016-09-16T14:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-09-16T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: Discriminative Embedding of Latent Variable Models for Structured Data\\n\\nSpeaker: Le Song\\nSpeaker Affiliation: Georgia Tech\\nHost: Stefanie Jegelka\\n\\n \\nDate: Friday, September 16, 2016\\nTime:  2:00 PM to 3:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-G882\">Seminar Room G882 (Hewlett Room)</a>\\n\\nAbstract: \\nStructured data, such as sequences, trees, graphs and hypergraphs, are prevalent in a number of interdisciplinary areas such as network analysis, knowledge engineering, computational biology, drug design and materials science. The availability of large amount of such structured data has posed great challenges for the machine learning community. How to represent such data to capture their similarities or differences? How to learn predictive models from a large amount of such data, and efficiently? How to learn to generate structured data de novo given certain desired properties? \\n\\nA common approach to tackle these challenges is to first design a similarity measure, called the kernel function, between two data points, based on either statistics of the substructures or probabilistic generative models; and then a machine learning algorithm will optimize a predictive model based on such similarity measure. However, this elegant two-stage approach has difficulty scaling up, and discriminative information is also not exploited during the design of similarity measure. \\n\\nIn this talk, I will present Structure2Vec, an effective and scalable approach for representing structured data based on the idea of embedding latent variable models into a feature space, and learning such feature space using discriminative information. Interestingly, Structure2Vec extracts features by performing a sequence of nested nonlinear operations in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving genome and protein sequences, drug molecules and energy materials, Structure2Vec consistently produces the-state-of-the-art predictive performance. Furthermore, in the materials property prediction problem involving 2.3 million data points, Structure2Vec is able to produces a more accurate model yet being 10,000 times smaller. In the end, I will also discuss potential improvements over current work, possible extensions to network analysis and computer vision, and thoughts on the structured data design problem.\\n\\n \\nBio: \\nLe Song is an assistant professor in the Department of Computational Science and Engineering, College of Computing, Georgia Institute of Technology. He received his Ph.D. in Machine Learning from University of Sydney and NICTA in 2008, and then conducted his post-doctoral research in the Department of Machine Learning, Carnegie Mellon University, between 2008 and 2011. Before he joined Georgia Institute of Technology, he was a research scientist at Google. His principal research direction is machine learning, especially kernel methods and probabilistic graphical models for large scale and complex problems, arising from artificial intelligence, network analysis, computational biology and other interdisciplinary domains. He is the recipient of the AISTATS\\'16 Best Student Paper Award, IPDPS\\'15 Best Paper Award, NSF CAREER Award’14, NIPS’13 Outstanding Paper Award, and ICML’10 Best Paper Award. He has also served as the area chair or senior program committee for many leading machine learning and AI conferences such as ICML, NIPS, AISTATS and AAAI, and the action editor for JMLR.\\n\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-09',\n",
       "  'id': 27},\n",
       " {'summary': 'TALK: Wednesday 10-05-2016 ML seminar: The data-driven (s, S) policy: why you can have confidence in censored demand data',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000130.html',\n",
       "  'location': '32-124',\n",
       "  'start': {'dateTime': '2016-10-05T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-05T13:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: The data-driven (s, S) policy: why you can have confidence in censored demand data\\n\\nSpeaker: Gah-Yi Ban\\nSpeaker Affiliation: London Business School\\nHost: Tamara Broderick\\n\\n \\nDate: Wednesday, October 05, 2016\\nTime:  12:00 PM to 1:00 PM\\n\\nLocation: 32-124\\n\\nAbstract:\\nWe revisit the classical dynamic inventory management problem of Scarf (1959) from a distribution-free, data-driven perspective. We propose a nonparametric estimation procedure for the optimal (s, S) policy that is asymptotically optimal and derive asymptotic confidence intervals around the estimated (s, S) levels. We further consider having at least some of the data censored from the absence of backlogging. We show that the intuitive procedure of correcting for censoring in the demand data directly yields an inconsistent estimate. We then show how to correctly use the censored data to obtain consistent decisions and derive confidence intervals for this policy. Surprisingly, under some conditions, estimated ordering decisions with censored demand data may have smaller variability and mean squared error (MSE) than with fully uncensored data. We thus arrive at the remarkable result that a decision maker with fully uncensored data can add artificial demand data to improve the e!\\n stimation of the (s,S) policy. We provide a prescription for the optimal amount of artificial data to add.\\n\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-12',\n",
       "  'id': 28},\n",
       " {'summary': 'Fwd: [IDSS-Community] LIDS Seminar Series: Suvrit Sra -\\t09/13/16',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000132.html',\n",
       "  'location': '32-141',\n",
       "  'start': {'dateTime': '2016-09-13T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-09-13T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Begin forwarded message:\\n\\nFrom: Roxana Hernandez <roxanah at mit.edu<mailto:roxanah at mit.edu>>\\nSubject: [IDSS-Community] LIDS Seminar Series: Suvrit Sra - 09/13/16\\nDate: September 9, 2016 at 09:53:40 EDT\\nTo: lids-seminars <lids-seminars at mit.edu<mailto:lids-seminars at mit.edu>>, idss-community <idss-community at mit.edu<mailto:idss-community at mit.edu>>\\n\\n\\nLIDS Seminar - Suvrit Sra - 09/13/16\\n\\n\\n\\nFollow us: [http://gallery.mailchimp.com/46ca3681a4ec08b33539f338c/images/f_logo_3.jpg] <http://www.facebook.com/lidsmit>\\n\\n\\n\\n[https://gallery.mailchimp.com/46ca3681a4ec08b33539f338c/images/e0d18ec4-2392-4c06-b16b-06efe6738d48.png]<http://www.lids.mit.edu/>\\n\\n\\n[http://gallery.mailchimp.com/46ca3681a4ec08b33539f338c/images/email_image.jpg]\\n\\nSuvrit Sra\\nMassachusetts Institute of Technology\\n\\nGeometric Optimization\\n\\nDate:\\nTuesday, September 13, 2016\\nTime:\\n4:00pm\\nLocation:\\n32-141\\n\\nReception to follow.\\nABSTRACT & BIO -><https://lids.mit.edu/news-and-events/events/geometric-optimization>\\n\\n\\n\\nThe LIDS Seminar Series is sponsored by Draper Laboratory\\n\\n\\n[https://gallery.mailchimp.com/46ca3681a4ec08b33539f338c/images/18418f61-d8d3-4675-8aef-701789237bc9.jpg]<http://www.draper.com/>\\n\\n\\n\\n\\nABOUT LIDS SEMINARS\\n\\nThe LIDS Seminar series<https://lids.mit.edu/news-and-events/lids-seminar-series> has represented an intellectual signature of the lab over the years. These seminars provide an overview of a topic area and exciting recent progress to a broad audience. The seminar topics span areas of communication, computation, networks, probability and statistics, control, optimization, and signal processing.\\n        [http://gallery.mailchimp.com/46ca3681a4ec08b33539f338c/images/new_stata_photo.jpg]\\nPhotos in this email by Jennifer Donovan\\n\\n\\n\\n————————\\nRoxana Hernandez\\nAssistant to Professor Asu Ozdaglar, Director\\nLaboratory for Information and Decision Systems\\nMassachusetts Institute of Technology\\n77 Massachusetts Avenue #32-D775B\\nCambridge, MA 02139\\nPhone: 617-253-2142\\nFax: 617-253-3578\\nwww.lids.mit.edu<http://www.lids.mit.edu/>\\nwww.facebook.com/lidsmit<http://www.facebook.com/lidsmit>\\n\\n_______________________________________________\\nIdss-community mailing list\\nIdss-community at mit.edu<mailto:Idss-community at mit.edu>\\nhttp://mailman.mit.edu/mailman/listinfo/idss-community\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-12',\n",
       "  'id': 29},\n",
       " {'summary': 'TALK: Wednesday 09-14-2016 Safe Decision Making Under\\tUncertainty',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000134.html',\n",
       "  'location': '32-G575',\n",
       "  'start': {'dateTime': '2016-09-14T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-09-14T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: Safe Decision Making Under Uncertainty\\n\\nSpeaker: Ashish Kapoor\\nSpeaker Affiliation: MSR\\nHost: Stefanie Jegelka\\n\\n \\nDate: Wednesday, September 14, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/conference_rooms/32-G575\">Conference Room G575</a>\\n\\nAbstract:\\nMachine Learning is one of the key component that enables systems that operate under uncertainty. For  example,  robotic systems  might employ  sensors  together with  a  machine  learned system  to  identify  obstacles.  However, such data driven system are far from perfect and can result in failure cases that can jeopardize safety. In this talk we will explore a framework  that  aims to preserve safety invariants despite the uncertainties in the environment arising due to incomplete information.  We will first describe a method  to reason about safe plans and control strategies despite perceiving the world through noisy sensors and machine learning systems. At  the  heart  of  our  approach  is  the new Probabilistic Signal Temporal Logic (PrSTL), an expressive language to define stochastic properties, and enforce probabilistic guarantees  on  them. Next, we will consider extensions of these ideas to a sequential decision making framework that considers the trade-off in ri!\\n sk and reward in a near-optimal manner. We will demonstrate our approach by deriving safe plans and controls for quadrotors and  autonomous  vehicles  in  dynamic  environments.\\n \\nBio: Ashish Kapoor is a senior researcher at Microsoft Research, Redmond. Currently, his research focuses on Aerial Informatics and Robotics with an emphasis on building intelligent and autonomous flying agents that are safe and enable applications that can positively influence our society. The research builds upon cutting edge research in machine intelligence, robotics and human-centered computation in order to enable an entire fleet of flying robots that range from micro-UAVs to commercial jetliners. Various applications scenarios include Weather Sensing, Monitoring for Precision Agriculture, Safe Cyber-Physical Systems etc.  Ashish received his PhD from MIT Media Laboratory in 2006. He also holds FAA Commercial Pilot certificate (SEL), FAA Flight Instructor certificate (Airplane Single Engine and Instrument Airplane) and is an avid amateur aircraft builder.\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-14',\n",
       "  'id': 26},\n",
       " {'summary': 'TALK: Friday 09-16-2016 ML seminar: Le Song',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000137.html',\n",
       "  'location': '32-G882',\n",
       "  'start': {'dateTime': '2016-09-16T14:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-09-16T15:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: Discriminative Embedding of Latent Variable Models for Structured Data\\n\\nSpeaker: Le Song\\nSpeaker Affiliation: Georgia Tech\\nHost: Stefanie Jegelka\\n\\n \\nDate: Friday, September 16, 2016\\nTime:  2:00 PM to 3:00 PM\\n\\nLocation: <a href=\"https://calendar.csail.mit.edu:443/seminar_rooms/32-G882\">Seminar Room G882 (Hewlett Room)</a>\\n\\nAbstract: \\nStructured data, such as sequences, trees, graphs and hypergraphs, are prevalent in a number of interdisciplinary areas such as network analysis, knowledge engineering, computational biology, drug design and materials science. The availability of large amount of such structured data has posed great challenges for the machine learning community. How to represent such data to capture their similarities or differences? How to learn predictive models from a large amount of such data, and efficiently? How to learn to generate structured data de novo given certain desired properties? \\n\\nA common approach to tackle these challenges is to first design a similarity measure, called the kernel function, between two data points, based on either statistics of the substructures or probabilistic generative models; and then a machine learning algorithm will optimize a predictive model based on such similarity measure. However, this elegant two-stage approach has difficulty scaling up, and discriminative information is also not exploited during the design of similarity measure. \\n\\nIn this talk, I will present Structure2Vec, an effective and scalable approach for representing structured data based on the idea of embedding latent variable models into a feature space, and learning such feature space using discriminative information. Interestingly, Structure2Vec extracts features by performing a sequence of nested nonlinear operations in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving genome and protein sequences, drug molecules and energy materials, Structure2Vec consistently produces the-state-of-the-art predictive performance. Furthermore, in the materials property prediction problem involving 2.3 million data points, Structure2Vec is able to produces a more accurate model yet being 10,000 times smaller. In the end, I will also discuss potential improvements over current work, possible extensions to network analysis and computer vision, and thoughts on the structured data design problem.\\n\\n \\nBio: \\nLe Song is an assistant professor in the Department of Computational Science and Engineering, College of Computing, Georgia Institute of Technology. He received his Ph.D. in Machine Learning from University of Sydney and NICTA in 2008, and then conducted his post-doctoral research in the Department of Machine Learning, Carnegie Mellon University, between 2008 and 2011. Before he joined Georgia Institute of Technology, he was a research scientist at Google. His principal research direction is machine learning, especially kernel methods and probabilistic graphical models for large scale and complex problems, arising from artificial intelligence, network analysis, computational biology and other interdisciplinary domains. He is the recipient of the AISTATS\\'16 Best Student Paper Award, IPDPS\\'15 Best Paper Award, NSF CAREER Award’14, NIPS’13 Outstanding Paper Award, and ICML’10 Best Paper Award. He has also served as the area chair or senior program committee for many leading machine learning and AI conferences such as ICML, NIPS, AISTATS and AAAI, and the action editor for JMLR.\\n\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-16',\n",
       "  'id': 27},\n",
       " {'summary': 'Fwd: [Stat-events] STATISTICS SEMINAR - Lorenzo Rosasco - FRIDAY, SEP 16@11AM, E18-304',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000139.html',\n",
       "  'location': 'E18-304',\n",
       "  'start': {'dateTime': '2016-09-16T11:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-09-16T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Another talk of interest today:\\n\\nBegin forwarded message:\\n\\n> From: Dawn Anderson <colquitt at mit.edu>\\n> Subject: [Stat-events] STATISTICS SEMINAR - Lorenzo Rosasco - FRIDAY, SEP 16 at 11AM, E18-304\\n> Date: September 16, 2016 9:06:17 AM EDT\\n> To: stat-events <stat-events at mit.edu>\\n> \\n> STOCHASTICS AND STATISTICS SEMINAR | FRIDAY, SEP. 16 11AM-12PM in Room E18-304 (Room Change)\\n> https://stat.mit.edu/events/lorenzo-rosasco-genova/\\n> \\n> \\n> Title: \\n> Less is More: optimal learning by subsampling and regularization  \\n> \\n> Speaker: \\n> Lorenzo Rosasco  (University of Genova)\\n> http://web.mit.edu/lrosasco/www/\\n> \\n> \\n> Abstract: \\n> In this talk, I will discuss the prediction properties of techniques commonly used to scale up kernel methods and Gaussian processes. In particular, I will focus on data dependent and independent sub-sampling methods, namely Nystrom and random features, and study their generalization properties within a statistical learning theory framework. On the one hand I will show that these methods can achieve optimal learning errors while being computational efficient. On the other hand, I will show that subsampling can be seen as a form of regularization, rather than only a way to speed up computations. Joint work with Raffaello Camoriano, Alessandro Rudi.\\n> \\n> Bio: \\n> Lorenzo Rosasco is assistant professor at the University of Genova, Italy. He is also affiliated with the Massachusetts Institute of Technology (MIT), where is a visiting professor, and with the Istituto Italiano di Tecnologia (IIT), where he is an external collaborator. He coordinates the Laboratory for Computational and Statistical Learning (LCSL), a joint machine learning laboratory between IIT and MIT. He received his PhD from the University of Genova in 2006 and has been visiting student at the Toyota Technological Institute at Chicago and at the Center for Biological and Computational Learning at MIT. Between 2006 and 2009 he was a postdoctoral fellow at MIT. His research focuses on theory, algorithms and application of machine learning.\\n> \\n> \\n> _______________________________________________\\n> Stat-events mailing list\\n> Stat-events at mit.edu\\n> http://mailman.mit.edu/mailman/listinfo/stat-events\\n> _______________________________________________\\n> Stat-events mailing list\\n> Stat-events at mit.edu\\n> http://mailman.mit.edu/mailman/listinfo/stat-events\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-16',\n",
       "  'id': 30},\n",
       " {'summary': \"TALK: Wednesday 10-19-2016 ML seminar: Probabilistic numerics: treating numerical computation as learning, or; it's Bayes all the way down\",\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000142.html',\n",
       "  'location': '32-G575',\n",
       "  'start': {'dateTime': '2016-10-19T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-19T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: Probabilistic numerics: treating numerical computation as learning, or; it\\'s Bayes all the way down\\n\\nSpeaker: Michael Osborne\\nSpeaker Affiliation: Oxford\\nHost: Tamara Broderick\\n\\n \\nDate: Wednesday, October 19, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: 32-G575\\n\\nLocation: 32-G575\\n\\nProbabilistic numerics: treating numerical computation as learning, or; it\\'s Bayes all the way down\\n\\nThis talk will introduce the probabilistic numerics framework. Probabilistic numerics interprets numerical procedures (e.g. optimisation, linear algebra, integration) as demanding Bayesian inference. This interpretation allows: uncertainty management at all levels of an algorithm; for the benefits of structure in numerical tasks to be realised, and; for no more costly computation to be allocated to any constituent numerical algorithm than is necessary to achieve our overall goals. The talk will particularly focus on recent work in probabilistic approaches to numerical integration: Bayesian quadrature, a robust alternative to MCMC methods. Applications of the techniques will be demonstrated to domains including astrometry and sensor networks, illustrating the superior wall-clock performance of probabilistic numeric techniques.\\n\\n\\n\\nBio:\\nMichael A Osborne (DPhil Oxon) is the Dyson Associate Professor in Machine Learning in the Department of Engineering Science, a co-director of the Oxford Martin programme on Technology and Employment, and a Faculty Member of the Oxford-Man Institute for Quantitative Finance, all at the University of Oxford. Within machine learning, he has published on active learning, Gaussian processes, Bayesian optimisation and Bayesian quadrature, and is a co-founder of the emerging field of probabilistic numerics. His algorithms have been applied within fields as diverse as astrostatistics, labour economics, ornithology, and sensor networks.\\n\\nRelevant URL: \\nFor more information please contact: Tamara Broderick, <a href=\"mailto:tbroderick at csail.mit.edu\">tbroderick at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-19',\n",
       "  'id': 31},\n",
       " {'summary': 'Seminar on Thursday: Criminal networks, Carlo Morselli',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000146.html',\n",
       "  'location': '4-237',\n",
       "  'start': {'dateTime': '2016-09-29T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-09-29T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'As part of our class, we are hosting a special seminar / open guest \\nlecture about crime network analysis - it is open to everyone:\\n\\n\\nTitle: Criminal Networks\\n\\nSpeaker: Carlo Morselli, School of Criminology, University of Montreal\\nHost: Stefanie Jegelka and Caroline Uhler\\n\\nDate: Thursday, September 29, 2016\\nTime:  4:00 PM to 5:00 PM\\nLocation: 4-237\\n\\nAbstract:\\nResearch on crime networks has been on a steady rise over the past ten \\nyears and new data sources, methods, and research questions have \\nradically changed how we think of crime. The presentation will provide a \\nbrief synopsis of the main trends in this area of research. This will be \\nfollowed by an assessment of two distinct data sources that demonstrate \\nhow social network analysis may contribute to our understanding of crime \\nnetworks of various proportions. The first data source is the Caviar \\nnetwork, which is composed of 110 participants that were involved in \\nhashish and cocaine importation during the mid-1990s. The network was \\nreconstructed with investigative and court files from a police \\ninvestigation that targeted the importers over a two-year period. This \\ncase study offers a unique outlook on what happens to an illegal drug \\nimportation network that is tampered with and slowly dismantled by the \\npolice over time. The second data source carries the focus of analysis \\nto a population level by compiling all arrests that were made in the \\nprovince of Quebec (Canada) between 2003 and 2009. Overlapping people \\nwho were arrested across the same events over this period resulted in a \\ntwo-mode network which was subsequently converted so as to examine the \\nmain patterns that have been identified across past co-offending \\nresearch. The basic elements underlying each data source and the various \\nquestions, concepts, and methods that may be pursued will guide the bulk \\nof the presentation.',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-27',\n",
       "  'id': 32},\n",
       " {'summary': 'TALK: Wednesday 10-05-2016 ML seminar: The data-driven (s, S) policy: why you can have confidence in censored demand data',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-September/000147.html',\n",
       "  'location': '32-124',\n",
       "  'start': {'dateTime': '2016-10-05T12:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-05T13:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: The data-driven (s, S) policy: why you can have confidence in censored demand data\\n\\nSpeaker: Gah-Yi Ban\\nSpeaker Affiliation: London Business School\\nHost: Tamara Broderick\\n\\n \\nDate: Wednesday, October 05, 2016\\nTime:  12:00 PM to 1:00 PM\\n\\nLocation: 32-124\\n\\nAbstract:\\nWe revisit the classical dynamic inventory management problem of Scarf (1959) from a distribution-free, data-driven perspective. We propose a nonparametric estimation procedure for the optimal (s, S) policy that is asymptotically optimal and derive asymptotic confidence intervals around the estimated (s, S) levels. We further consider having at least some of the data censored from the absence of backlogging. We show that the intuitive procedure of correcting for censoring in the demand data directly yields an inconsistent estimate. We then show how to correctly use the censored data to obtain consistent decisions and derive confidence intervals for this policy. Surprisingly, under some conditions, estimated ordering decisions with censored demand data may have smaller variability and mean squared error (MSE) than with fully uncensored data. We thus arrive at the remarkable result that a decision maker with fully uncensored data can add artificial demand data to improve the e!\\n stimation of the (s,S) policy. We provide a prescription for the optimal amount of artificial data to add.\\n\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-09-28',\n",
       "  'id': 28},\n",
       " {'summary': 'TALK: Wednesday 10-05-2016 ML seminar: The data-driven (s, S) policy: why you can have confidence in censored demand data',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000149.html',\n",
       "  'location': '32-G575',\n",
       "  'start': {'dateTime': '2016-10-05T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-05T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: The data-driven (s, S) policy: why you can have confidence in censored demand data\\n\\nSpeaker: Gah-Yi Ban\\nSpeaker Affiliation: London Business School\\nHost: Tamara Broderick\\n\\n \\nDate: Wednesday, October 05, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: 32-G575\\n\\nAbstract:\\nWe revisit the classical dynamic inventory management problem of Scarf (1959) from a distribution-free, data-driven perspective. We propose a nonparametric estimation procedure for the optimal (s, S) policy that is asymptotically optimal and derive asymptotic confidence intervals around the estimated (s, S) levels. We further consider having at least some of the data censored from the absence of backlogging. We show that the intuitive procedure of correcting for censoring in the demand data directly yields an inconsistent estimate. We then show how to correctly use the censored data to obtain consistent decisions and derive confidence intervals for this policy. Surprisingly, under some conditions, estimated ordering decisions with censored demand data may have smaller variability and mean squared error (MSE) than with fully uncensored data. We thus arrive at the remarkable result that a decision maker with fully uncensored data can add artificial demand data to improve the e!\\n stimation of the (s,S) policy. We provide a prescription for the optimal amount of artificial data to add.\\n\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-03',\n",
       "  'id': 28},\n",
       " {'summary': 'UPDATED SPACE/TIME: Gah-Yi Ban, ML seminar @ 4pm in 32-G575,\\tWedn 10/5',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000150.html',\n",
       "  'location': '32-G575',\n",
       "  'start': {'dateTime': '2016-10-05T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-05T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Note the change in location and time from an earlier announcement of\\nthis seminar.\\n\\nGah-Yi ban is available and interested to meet with anyone who would\\nlike to chat while she is in town. If you\\'d like to meet with her\\nbetween 12 noon and 3:30pm on the day of her talk (Wedn 10/5), please\\nemail Teresa Cataldo <cataldo at csail.mit.edu>.\\n\\n\\n\\n---------- Forwarded message ----------\\nFrom:  <calendar at csail.mit.edu>\\n\\n\\nML seminar: The data-driven (s, S) policy: why you can have confidence\\nin censored demand data\\n\\nSpeaker: Gah-Yi Ban\\nSpeaker Affiliation: London Business School\\nHost: Tamara Broderick\\n\\n\\nDate: Wednesday, October 05, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: 32-G575\\n\\nAbstract:\\nWe revisit the classical dynamic inventory management problem of Scarf\\n(1959) from a distribution-free, data-driven perspective. We propose a\\nnonparametric estimation procedure for the optimal (s, S) policy that\\nis asymptotically optimal and derive asymptotic confidence intervals\\naround the estimated (s, S) levels. We further consider having at\\nleast some of the data censored from the absence of backlogging. We\\nshow that the intuitive procedure of correcting for censoring in the\\ndemand data directly yields an inconsistent estimate. We then show how\\nto correctly use the censored data to obtain consistent decisions and\\nderive confidence intervals for this policy. Surprisingly, under some\\nconditions, estimated ordering decisions with censored demand data may\\nhave smaller variability and mean squared error (MSE) than with fully\\nuncensored data. We thus arrive at the remarkable result that a\\ndecision maker with fully uncensored data can add artificial demand\\ndata to improve the estimation of the (s,S) policy. We provide a\\nprescription for the optimal amount of artificial data to add.\\n\\n\\nRelevant URL:\\nFor more information please contact: Stefanie S. Jegelka, <a\\nhref=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n\\n_______________________________________________\\nSeminars mailing list\\nSeminars at lists.csail.mit.edu\\nhttps://lists.csail.mit.edu/mailman/listinfo/seminars\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-03',\n",
       "  'id': 28},\n",
       " {'summary': 'TALK: Wednesday 10-05-2016 ML seminar: The data-driven (s, S) policy: why you can have confidence in censored demand data',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000151.html',\n",
       "  'location': '32-G575',\n",
       "  'start': {'dateTime': '2016-10-05T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-05T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: The data-driven (s, S) policy: why you can have confidence in censored demand data\\n\\nSpeaker: Gah-Yi Ban\\nSpeaker Affiliation: London Business School\\nHost: Tamara Broderick\\n\\n \\nDate: Wednesday, October 05, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: 32-G575\\n\\nAbstract:\\nWe revisit the classical dynamic inventory management problem of Scarf (1959) from a distribution-free, data-driven perspective. We propose a nonparametric estimation procedure for the optimal (s, S) policy that is asymptotically optimal and derive asymptotic confidence intervals around the estimated (s, S) levels. We further consider having at least some of the data censored from the absence of backlogging. We show that the intuitive procedure of correcting for censoring in the demand data directly yields an inconsistent estimate. We then show how to correctly use the censored data to obtain consistent decisions and derive confidence intervals for this policy. Surprisingly, under some conditions, estimated ordering decisions with censored demand data may have smaller variability and mean squared error (MSE) than with fully uncensored data. We thus arrive at the remarkable result that a decision maker with fully uncensored data can add artificial demand data to improve the e!\\n stimation of the (s,S) policy. We provide a prescription for the optimal amount of artificial data to add.\\n\\n\\nRelevant URL: \\nFor more information please contact: Stefanie S. Jegelka, <a href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-05',\n",
       "  'id': 28},\n",
       " {'summary': 'ML seminar at 4pm in 32-G575: Gah-Yi Ban, The data-driven (s, S) policy: why you can have confidence > in censored demand data',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000152.html',\n",
       "  'location': '32-G575',\n",
       "  'start': {'dateTime': '2016-10-05T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-05T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Reminder: the ML seminar is happening shortly in 32-G575.\\n\\nOn Mon, Oct 3, 2016 at 4:37 PM, Tamara Broderick\\n<tbroderick at csail.mit.edu> wrote:\\n> Note the change in location and time from an earlier announcement of\\n> this seminar.\\n>\\n> Gah-Yi ban is available and interested to meet with anyone who would\\n> like to chat while she is in town. If you\\'d like to meet with her\\n> between 12 noon and 3:30pm on the day of her talk (Wedn 10/5), please\\n> email Teresa Cataldo <cataldo at csail.mit.edu>.\\n>\\n>\\n>\\n> ---------- Forwarded message ----------\\n> From:  <calendar at csail.mit.edu>\\n>\\n>\\n> ML seminar: The data-driven (s, S) policy: why you can have confidence\\n> in censored demand data\\n>\\n> Speaker: Gah-Yi Ban\\n> Speaker Affiliation: London Business School\\n> Host: Tamara Broderick\\n>\\n>\\n> Date: Wednesday, October 05, 2016\\n> Time:  4:00 PM to 5:00 PM\\n>\\n> Location: 32-G575\\n>\\n> Abstract:\\n> We revisit the classical dynamic inventory management problem of Scarf\\n> (1959) from a distribution-free, data-driven perspective. We propose a\\n> nonparametric estimation procedure for the optimal (s, S) policy that\\n> is asymptotically optimal and derive asymptotic confidence intervals\\n> around the estimated (s, S) levels. We further consider having at\\n> least some of the data censored from the absence of backlogging. We\\n> show that the intuitive procedure of correcting for censoring in the\\n> demand data directly yields an inconsistent estimate. We then show how\\n> to correctly use the censored data to obtain consistent decisions and\\n> derive confidence intervals for this policy. Surprisingly, under some\\n> conditions, estimated ordering decisions with censored demand data may\\n> have smaller variability and mean squared error (MSE) than with fully\\n> uncensored data. We thus arrive at the remarkable result that a\\n> decision maker with fully uncensored data can add artificial demand\\n> data to improve the estimation of the (s,S) policy. We provide a\\n> prescription for the optimal amount of artificial data to add.\\n>\\n>\\n> Relevant URL:\\n> For more information please contact: Stefanie S. Jegelka, <a\\n> href=\"mailto:stefje at csail.mit.edu\">stefje at csail.mit.edu</a>\\n>\\n>\\n> _______________________________________________\\n> Seminars mailing list\\n> Seminars at lists.csail.mit.edu\\n> https://lists.csail.mit.edu/mailman/listinfo/seminars',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-05',\n",
       "  'id': 28},\n",
       " {'summary': \"TALK: Wednesday 10-19-2016 ML seminar: Probabilistic numerics: treating numerical computation as learning, or; it's Bayes all the way down\",\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000154.html',\n",
       "  'location': '32-G575',\n",
       "  'start': {'dateTime': '2016-10-19T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-19T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: Probabilistic numerics: treating numerical computation as learning, or; it\\'s Bayes all the way down\\n\\nSpeaker: Michael Osborne\\nSpeaker Affiliation: Oxford\\nHost: Tamara Broderick\\n\\n \\nDate: Wednesday, October 19, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: 32-G575\\n\\nLocation: 32-G575\\n\\nProbabilistic numerics: treating numerical computation as learning, or; it\\'s Bayes all the way down\\n\\nThis talk will introduce the probabilistic numerics framework. Probabilistic numerics interprets numerical procedures (e.g. optimisation, linear algebra, integration) as demanding Bayesian inference. This interpretation allows: uncertainty management at all levels of an algorithm; for the benefits of structure in numerical tasks to be realised, and; for no more costly computation to be allocated to any constituent numerical algorithm than is necessary to achieve our overall goals. The talk will particularly focus on recent work in probabilistic approaches to numerical integration: Bayesian quadrature, a robust alternative to MCMC methods. Applications of the techniques will be demonstrated to domains including astrometry and sensor networks, illustrating the superior wall-clock performance of probabilistic numeric techniques.\\n\\n\\n\\nBio:\\nMichael A Osborne (DPhil Oxon) is the Dyson Associate Professor in Machine Learning in the Department of Engineering Science, a co-director of the Oxford Martin programme on Technology and Employment, and a Faculty Member of the Oxford-Man Institute for Quantitative Finance, all at the University of Oxford. Within machine learning, he has published on active learning, Gaussian processes, Bayesian optimisation and Bayesian quadrature, and is a co-founder of the emerging field of probabilistic numerics. His algorithms have been applied within fields as diverse as astrostatistics, labour economics, ornithology, and sensor networks.\\n\\nRelevant URL: \\nFor more information please contact: Tamara Broderick, <a href=\"mailto:tbroderick at csail.mit.edu\">tbroderick at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-12',\n",
       "  'id': 31},\n",
       " {'summary': \"CANCELLED: Wednesday 10-19-2016 ML seminar: Probabilistic numerics: treating numerical computation as learning, or; it's Bayes all the way down\",\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000155.html',\n",
       "  'location': '32-G575',\n",
       "  'start': {'dateTime': '2016-10-19T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-19T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'The following event has been CANCELLED:\\n\\nML seminar: Probabilistic numerics: treating numerical computation as learning, or; it\\'s Bayes all the way down\\n\\nSpeaker: Michael Osborne\\nSpeaker Affiliation: Oxford\\nHost: Tamara Broderick\\n\\n \\nDate: Wednesday, October 19, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: 32-G575\\n\\nLocation: 32-G575\\n\\nProbabilistic numerics: treating numerical computation as learning, or; it\\'s Bayes all the way down\\n\\nThis talk will introduce the probabilistic numerics framework. Probabilistic numerics interprets numerical procedures (e.g. optimisation, linear algebra, integration) as demanding Bayesian inference. This interpretation allows: uncertainty management at all levels of an algorithm; for the benefits of structure in numerical tasks to be realised, and; for no more costly computation to be allocated to any constituent numerical algorithm than is necessary to achieve our overall goals. The talk will particularly focus on recent work in probabilistic approaches to numerical integration: Bayesian quadrature, a robust alternative to MCMC methods. Applications of the techniques will be demonstrated to domains including astrometry and sensor networks, illustrating the superior wall-clock performance of probabilistic numeric techniques.\\n\\n\\n\\nBio:\\nMichael A Osborne (DPhil Oxon) is the Dyson Associate Professor in Machine Learning in the Department of Engineering Science, a co-director of the Oxford Martin programme on Technology and Employment, and a Faculty Member of the Oxford-Man Institute for Quantitative Finance, all at the University of Oxford. Within machine learning, he has published on active learning, Gaussian processes, Bayesian optimisation and Bayesian quadrature, and is a co-founder of the emerging field of probabilistic numerics. His algorithms have been applied within fields as diverse as astrostatistics, labour economics, ornithology, and sensor networks.\\n\\nRelevant URL: \\nFor more information please contact: Tamara Broderick, <a href=\"mailto:tbroderick at csail.mit.edu\">tbroderick at csail.mit.edu</a>\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-13',\n",
       "  'id': 31},\n",
       " {'summary': 'TALK: Wednesday 10-19-2016 ML seminar: Tina Eliassi-Rad: The Reasonable Effectiveness of Roles in Complex Networks',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000157.html',\n",
       "  'start': {'dateTime': '2016-10-19T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-19T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'ML seminar: The Reasonable Effectiveness of Roles in Complex Networks\\n\\nSpeaker: Tina Eliassi-Rad\\nSpeaker Affiliation: Northeastern University\\nHost: Tamara Broderick\\n\\n  \\nDate: Wednesday, October 19, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: G575\\n\\nAbstract:\\n\\nGiven a network, how can we automatically discover roles (or functions) of nodes? Roles compactly represent structural behaviors of nodes and generalize across various networks. Examples of roles include \"clique-members,\" \"periphery-nodes,\" \"bridges,\" etc. Are there good features that we can extract for nodes that indicate role-membership? How are roles different from communities and from equivalences (from sociology)? What are the applications in which these discovered roles can be effectively used? In this talk, we address these questions, provide unsupervised and supervised algorithms for role discovery, and discuss why roles are so effective in many applications from transfer learning to re-identification to anomaly detection to mining time-evolving networks and multi-relational graphs.\\n\\n\\nBio:\\nTina Eliassi-Rad is an Associate Professor of Computer Science at Northeastern University in Boston, MA. She is also on the faculty of Northeastern\\'s Network Science Institute. Prior to joining Northeastern, Tina was an Associate Professor of Computer Science at Rutgers University; and before that she was a Member of Technical Staff and Principal Investigator at Lawrence Livermore National Laboratory. Tina earned her Ph.D. in Computer Sciences (with a minor in Mathematical Statistics) at the University of Wisconsin-Madison. Her research is rooted in data mining and machine learning; and spans theory, algorithms, and applications of massive data from networked representations of physical and social phenomena. Tina\\'s work has been applied to personalized search on the World-Wide Web, statistical indices of large-scale scientific simulation data, fraud detection, mobile ad targeting, and cyber situational awareness. Her algorithms have been incorporated into systems used by the government and industry (e.g., IBM System G Graph Analytics) as well as open-source software (e.g., Stanford Network Analysis Project). In 2010, she received an Outstanding Mentor Award from the Office of Science at the US Department of Energy. For more details, visit http://eliassi.org.',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-14',\n",
       "  'id': 33},\n",
       " {'summary': 'One-on-one Meetings with Tina Eliassi-Rad (ML Seminar\\tSpeaker 10/19)',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000158.html',\n",
       "  'start': {'dateTime': '2016-10-19T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-19T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'If you\\'d like to meet with Tina Eliassi-Rad on Wednesday, please email Teresa\\nCataldo cataldo at csail.mit.edu .\\nProfessor Eliassi-Rad is available to meet on Wedn 10/19 during 2pm--3:45pm\\nand also after her talk until 6pm.\\n\\nBest,\\nTamara\\n\\n\\nOn Fri, Oct 14, 2016 at 1:51 PM, Stefanie Jegelka <stefje at csail.mit.edu>\\nwrote:\\n\\n> ML seminar: The Reasonable Effectiveness of Roles in Complex Networks\\n>\\n> Speaker: Tina Eliassi-Rad\\n> Speaker Affiliation: Northeastern University\\n> Host: Tamara Broderick\\n>\\n>\\n> Date: Wednesday, October 19, 2016\\n> Time:  4:00 PM to 5:00 PM\\n>\\n> Location: G575\\n>\\n> Abstract:\\n>\\n> Given a network, how can we automatically discover roles (or functions) of\\n> nodes? Roles compactly represent structural behaviors of nodes and\\n> generalize across various networks. Examples of roles include\\n> \"clique-members,\" \"periphery-nodes,\" \"bridges,\" etc. Are there good\\n> features that we can extract for nodes that indicate role-membership? How\\n> are roles different from communities and from equivalences (from\\n> sociology)? What are the applications in which these discovered roles can\\n> be effectively used? In this talk, we address these questions, provide\\n> unsupervised and supervised algorithms for role discovery, and discuss why\\n> roles are so effective in many applications from transfer learning to\\n> re-identification to anomaly detection to mining time-evolving networks and\\n> multi-relational graphs.\\n>\\n>\\n> Bio:\\n> Tina Eliassi-Rad is an Associate Professor of Computer Science at\\n> Northeastern University in Boston, MA. She is also on the faculty of\\n> Northeastern\\'s Network Science Institute. Prior to joining Northeastern,\\n> Tina was an Associate Professor of Computer Science at Rutgers University;\\n> and before that she was a Member of Technical Staff and Principal\\n> Investigator at Lawrence Livermore National Laboratory. Tina earned her\\n> Ph.D. in Computer Sciences (with a minor in Mathematical Statistics) at the\\n> University of Wisconsin-Madison. Her research is rooted in data mining and\\n> machine learning; and spans theory, algorithms, and applications of massive\\n> data from networked representations of physical and social phenomena.\\n> Tina\\'s work has been applied to personalized search on the World-Wide Web,\\n> statistical indices of large-scale scientific simulation data, fraud\\n> detection, mobile ad targeting, and cyber situational awareness. Her\\n> algorithms have been incorporated into systems used by the!\\n>   government and industry (e.g., IBM System G Graph Analytics) as well as\\n> open-source software (e.g., Stanford Network Analysis Project). In 2010,\\n> she received an Outstanding Mentor Award from the Office of Science at the\\n> US Department of Energy. For more details, visit http://eliassi.org.\\n>\\n>\\n>\\n>\\n>\\n>\\n> _______________________________________________\\n> Mitml mailing list\\n> Mitml at mit.edu\\n> http://mailman.mit.edu/mailman/listinfo/mitml\\n>\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-14',\n",
       "  'id': 33},\n",
       " {'summary': 'One-on-one Meetings with Tina Eliassi-Rad (ML Seminar\\tSpeaker 10/19)',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000162.html',\n",
       "  'start': {'dateTime': '2016-10-19T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-19T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Reminder to sign up for one-on-one meetings with our speaker this week\\nby emailing Teresa Cataldo <cataldo at csail.mit.edu>\\n\\nOn Fri, Oct 14, 2016 at 3:48 PM, Tamara Broderick\\n<tbroderick at csail.mit.edu> wrote:\\n> If you\\'d like to meet with Tina Eliassi-Rad on Wednesday, please email\\n> Teresa Cataldo cataldo at csail.mit.edu .\\n> Professor Eliassi-Rad is available to meet on Wedn 10/19 during 2pm--3:45pm\\n> and also after her talk until 6pm.\\n>\\n> Best,\\n> Tamara\\n>\\n>\\n> On Fri, Oct 14, 2016 at 1:51 PM, Stefanie Jegelka <stefje at csail.mit.edu>\\n> wrote:\\n>>\\n>> ML seminar: The Reasonable Effectiveness of Roles in Complex Networks\\n>>\\n>> Speaker: Tina Eliassi-Rad\\n>> Speaker Affiliation: Northeastern University\\n>> Host: Tamara Broderick\\n>>\\n>>\\n>> Date: Wednesday, October 19, 2016\\n>> Time:  4:00 PM to 5:00 PM\\n>>\\n>> Location: G575\\n>>\\n>> Abstract:\\n>>\\n>> Given a network, how can we automatically discover roles (or functions) of\\n>> nodes? Roles compactly represent structural behaviors of nodes and\\n>> generalize across various networks. Examples of roles include\\n>> \"clique-members,\" \"periphery-nodes,\" \"bridges,\" etc. Are there good features\\n>> that we can extract for nodes that indicate role-membership? How are roles\\n>> different from communities and from equivalences (from sociology)? What are\\n>> the applications in which these discovered roles can be effectively used? In\\n>> this talk, we address these questions, provide unsupervised and supervised\\n>> algorithms for role discovery, and discuss why roles are so effective in\\n>> many applications from transfer learning to re-identification to anomaly\\n>> detection to mining time-evolving networks and multi-relational graphs.\\n>>\\n>>\\n>> Bio:\\n>> Tina Eliassi-Rad is an Associate Professor of Computer Science at\\n>> Northeastern University in Boston, MA. She is also on the faculty of\\n>> Northeastern\\'s Network Science Institute. Prior to joining Northeastern,\\n>> Tina was an Associate Professor of Computer Science at Rutgers University;\\n>> and before that she was a Member of Technical Staff and Principal\\n>> Investigator at Lawrence Livermore National Laboratory. Tina earned her\\n>> Ph.D. in Computer Sciences (with a minor in Mathematical Statistics) at the\\n>> University of Wisconsin-Madison. Her research is rooted in data mining and\\n>> machine learning; and spans theory, algorithms, and applications of massive\\n>> data from networked representations of physical and social phenomena. Tina\\'s\\n>> work has been applied to personalized search on the World-Wide Web,\\n>> statistical indices of large-scale scientific simulation data, fraud\\n>> detection, mobile ad targeting, and cyber situational awareness. Her\\n>> algorithms have been incorporated into systems used by the!\\n>>   government and industry (e.g., IBM System G Graph Analytics) as well as\\n>> open-source software (e.g., Stanford Network Analysis Project). In 2010, she\\n>> received an Outstanding Mentor Award from the Office of Science at the US\\n>> Department of Energy. For more details, visit http://eliassi.org.\\n>>\\n>>\\n>>\\n>>\\n>>\\n>>\\n>> _______________________________________________\\n>> Mitml mailing list\\n>> Mitml at mit.edu\\n>> http://mailman.mit.edu/mailman/listinfo/mitml\\n>\\n>',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-17',\n",
       "  'id': 33},\n",
       " {'summary': 'Fwd: [Theory-seminars] TALK: Tuesday 10-18-2016 Jonathan Ullman: Algorithmic Stability for Adaptive Data Analysis',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000163.html',\n",
       "  'location': '8-201',\n",
       "  'start': {'dateTime': '2016-10-18T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-18T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Begin forwarded message:\\n\\nFrom: Vinod Vaikuntanathan <vinod.nathan at gmail.com<mailto:vinod.nathan at gmail.com>>\\nSubject: Fwd: [Theory-seminars] TALK: Tuesday 10-18-2016 Jonathan Ullman: Algorithmic Stability for Adaptive Data Analysis\\nDate: October 18, 2016 at 12:44:41 EDT\\n\\n\\n\\n\\nThe following talk today at the TOC colloquium might be of interest to ML folks.  I was wondering: could you forward the announcement to the right ML group mailing list?\\n\\nthank you,\\n\\nVinod\\n\\n\\n\\n---------- Forwarded message ----------\\nFrom: <calendar at csail.mit.edu<mailto:calendar at csail.mit.edu>>\\nDate: Tue, Oct 18, 2016 at 12:01 AM\\nSubject: [Theory-seminars] TALK: Tuesday 10-18-2016 Jonathan Ullman: Algorithmic Stability for Adaptive Data Analysis\\nTo: seminars at csail.mit.edu<mailto:seminars at csail.mit.edu>, theory-seminars at csail.mit.edu<mailto:theory-seminars at csail.mit.edu>\\n\\n\\nJonathan Ullman: Algorithmic Stability for Adaptive Data Analysis\\n\\nSeminar Series: Theory of Computation (TOC) Seminar Series 2016\\n\\nSpeaker: Jonathan Ullman\\n\\nHost: Ankur Moitra\\n\\nHost Affiliation: MIT, CSAIL\\n\\nDate: Tuesday, October 18, 2016\\n\\nTime: 4:00 PM to 5:00 PM\\n\\nRefreshments Time: 3:45 PM\\n\\nLocation: Patil/Kiva G449\\n\\n\\nAbstract:\\nAdaptivity is an important feature of data analysis - the choice of questions to ask about a dataset often depends on previous interactions with the same dataset. However, statistical validity is typically studied in a nonadaptive model, where all questions are specified before the dataset is drawn. Recent work by Dwork et al. (STOC, 2015) and Hardt and Ullman (FOCS, 2014) initiated a general formal study of this problem, and gave the first upper and lower bounds on the achievable generalization error for adaptive data analysis.\\n\\nSpecifically, suppose there is an unknown distribution P and a set of n independent samples x is drawn from P. We seek an algorithm that, given x as input, accurately answers a sequence of adaptively chosen \"queries\" about the unknown distribution P. How many samples n must we draw from the distribution, as a function of the type of queries, the number of queries, and the desired level of accuracy?\\n\\nWe give new upper bounds on the number of samples n that are needed to answer statistical queries. The bounds improve and simplify the work of Dwork et al. (STOC, 2015), and have been applied in subsequent work by those authors (Science, 2015; NIPS, 2015). As in Dwork et al., our algorithms are based on a connection with algorithmic stability in the form of differential privacy. We extend their work by giving a quantitatively optimal, more general, and simpler proof of their main theorem that the stability notion guaranteed by differential privacy implies low generalization error.\\n\\nJoint work with Raef Bassily, Kobbi Nissim, Adam Smith, Thomas Steinke, and Uri Stemmer.\\n\\n\\n\\nRelevant URL:\\n\\nFor more information please contact: Deborah Goodwin, 617.324.7303<tel:617.324.7303>, dlehto at csail.mit.edu<mailto:dlehto at csail.mit.edu>\\n\\n_______________________________________________\\nTheory-seminars mailing list\\nTheory-seminars at lists.csail.mit.edu<mailto:Theory-seminars at lists.csail.mit.edu>\\nhttps://lists.csail.mit.edu/mailman/listinfo/theory-seminars\\n\\n\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-18',\n",
       "  'id': 34},\n",
       " {'summary': 'Reminder: ML seminar today!: Tina Eliassi-Rad: The Reasonable Effectiveness of Roles in Complex Networks',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000164.html',\n",
       "  'start': {'dateTime': '2016-10-19T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-19T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': '-------------------------------\\n> ML seminar: The Reasonable Effectiveness of Roles in Complex Networks\\n>\\n> Speaker: Tina Eliassi-Rad\\n> Speaker Affiliation: Northeastern University\\n> Host: Tamara Broderick\\n>\\n>\\n> Date: Wednesday, October 19, 2016\\n> Time:  4:00 PM to 5:00 PM\\n>\\n> Location: G575\\n>\\n> Abstract:\\n>\\n> Given a network, how can we automatically discover roles (or functions) of nodes? Roles compactly represent structural behaviors of nodes and generalize across various networks. Examples of roles include \"clique-members,\" \"periphery-nodes,\" \"bridges,\" etc. Are there good features that we can extract for nodes that indicate role-membership? How are roles different from communities and from equivalences (from sociology)? What are the applications in which these discovered roles can be effectively used? In this talk, we address these questions, provide unsupervised and supervised algorithms for role discovery, and discuss why roles are so effective in many applications from transfer learning to re-identification to anomaly detection to mining time-evolving networks and multi-relational graphs.\\n>\\n>\\n> Bio:\\n> Tina Eliassi-Rad is an Associate Professor of Computer Science at Northeastern University in Boston, MA. She is also on the faculty of Northeastern\\'s Network Science Institute. Prior to joining Northeastern, Tina was an Associate Professor of Computer Science at Rutgers University; and before that she was a Member of Technical Staff and Principal Investigator at Lawrence Livermore National Laboratory. Tina earned her Ph.D. in Computer Sciences (with a minor in Mathematical Statistics) at the University of Wisconsin-Madison. Her research is rooted in data mining and machine learning; and spans theory, algorithms, and applications of massive data from networked representations of physical and social phenomena. Tina\\'s work has been applied to personalized search on the World-Wide Web, statistical indices of large-scale scientific simulation data, fraud detection, mobile ad targeting, and cyber situational awareness. Her algorithms have been incorporated into systems used by the!\\n>   government and industry (e.g., IBM System G Graph Analytics) as well as open-source software (e.g., Stanford Network Analysis Project). In 2010, she received an Outstanding Mentor Award from the Office of Science at the US Department of Energy. For more details, visit http://eliassi.org.\\n>\\n>\\n>\\n>\\n>\\n>\\n> _______________________________________________\\n> Mitml mailing list\\n> Mitml at mit.edu\\n> http://mailman.mit.edu/mailman/listinfo/mitml',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-19',\n",
       "  'id': 33},\n",
       " {'summary': 'Fwd: LIDS Seminar: Jeffrey Bilmes - 10/25/16',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000165.html',\n",
       "  'location': '32-141',\n",
       "  'start': {'dateTime': '2016-10-25T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-25T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Begin forwarded message:\\n\\nFrom: Roxana Hernandez <roxanah at mit.edu<mailto:roxanah at mit.edu>>\\nSubject: [IDSS-Community] LIDS Seminar: Jeffrey Bilmes - 10/25/16\\nDate: October 19, 2016 at 15:16:03 EDT\\nTo: lids-seminars <lids-seminars at mit.edu<mailto:lids-seminars at mit.edu>>, idss-community <idss-community at mit.edu<mailto:idss-community at mit.edu>>\\n\\nLIDS Seminar - Jeffrey Bilmes - 10/25/16\\n\\n\\n\\nFollow us: [http://gallery.mailchimp.com/46ca3681a4ec08b33539f338c/images/f_logo_3.jpg] <http://www.facebook.com/lidsmit>\\n\\n\\n\\n[https://gallery.mailchimp.com/46ca3681a4ec08b33539f338c/images/e0d18ec4-2392-4c06-b16b-06efe6738d48.png]<http://www.lids.mit.edu/>\\n\\n\\n[http://gallery.mailchimp.com/46ca3681a4ec08b33539f338c/images/email_image.jpg]\\n\\nJeffrey Bilmes\\nUniversity of Washington, Seattle\\n\\nDeep Submodular Functions: Learning and Applications in Data Science\\n\\nDate:\\nTuesday, October 25, 2016\\nTime:\\n4:00 pm\\nLocation:\\n32-141\\n\\nReception to follow.\\nABSTRACT & BIO -><https://lids.mit.edu/news-and-events/events/deep-submodular-functions-learning-and-applications-data-science>\\n\\n\\n\\nThe LIDS Seminar Series is sponsored by Draper Laboratory\\n\\n\\n[https://gallery.mailchimp.com/46ca3681a4ec08b33539f338c/images/18418f61-d8d3-4675-8aef-701789237bc9.jpg]<http://www.draper.com/>\\n\\n\\n\\n\\nABOUT LIDS SEMINARS\\n\\nThe LIDS Seminar series<https://lids.mit.edu/news-and-events/lids-seminar-series> has represented an intellectual signature of the lab over the years. These seminars provide an overview of a topic area and exciting recent progress to a broad audience. The seminar topics span areas of communication, computation, networks, probability and statistics, control, optimization, and signal processing.\\n        [http://gallery.mailchimp.com/46ca3681a4ec08b33539f338c/images/new_stata_photo.jpg]\\nPhotos in this email by Jennifer Donovan\\n\\n\\n\\n\\n————————\\nRoxana Hernandez\\nAssistant to Professor Asu Ozdaglar, Director\\nLaboratory for Information and Decision Systems\\nMassachusetts Institute of Technology\\n77 Massachusetts Avenue #32-D775B\\nCambridge, MA 02139\\nPhone: 617-253-2142\\nFax: 617-253-3578\\nwww.lids.mit.edu<http://www.lids.mit.edu/>\\nwww.facebook.com/lidsmit<http://www.facebook.com/lidsmit>\\n\\n_______________________________________________\\nIdss-community mailing list\\nIdss-community at mit.edu<mailto:Idss-community at mit.edu>\\nhttp://mailman.mit.edu/mailman/listinfo/idss-community\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-19',\n",
       "  'id': 29},\n",
       " {'summary': 'TALK: Wednesday 10-26-2016 ML seminar: Elad Hazan',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000166.html',\n",
       "  'location': '32-D463',\n",
       "  'start': {'dateTime': '2016-10-26T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-26T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"ML colloquium: Elad Hazan: A Non-generative Framework and Convex Relaxations for Unsupervised Learning\\n\\nSpeaker: Elad Hazan\\nSpeaker Affiliation: Princeton\\nHost: Stefanie Jegelka\\n\\n  \\nDate: Wednesday, October 26, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: 32-D463\\n\\nWe'll describe a novel theoretical framework for unsupervised learning which is not based on generative assumptions. It is comparative, and allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization. These includes dictionary learning and learning of algebraic manifolds.\\n\\nJoint work with Tengyu Ma\\n\\n\\nbio:\\n\\nElad Hazan is a professor of computer science at Princeton university. He joined in 2015 from the Technion, where he had been an associate professor of operations research. His research focuses on the design and analysis of algorithms for basic problems in machine learning and optimization. Amongst his contributions are the co-development of the AdaGrad algorithm for training learning machines, and the first sublinear-time algorithms for convex optimization. He is the recipient of (twice) the IBM Goldberg best paper award in 2012 for contributions to sublinear time algorithms for machine learning, and in 2008 for decision making under uncertainty, a European Research Council grant, a Marie Curie fellowship and a Google Research Award (twice). He serves on the steering committee of the Association for Computational Learning and has been program chair for COLT 2015.\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-19',\n",
       "  'id': 35},\n",
       " {'summary': 'talk by Elad Hazan on Wednesday',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000169.html',\n",
       "  'location': '32-D463',\n",
       "  'start': {'dateTime': '2016-10-26T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-26T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Elad Hazan is giving a talk this Wednesday - talk and abstract are below. Please let me know if you'd like to meet him.\\n\\nBest,\\nStefanie\\n\\n\\n\\nElad Hazan: A Non-generative Framework and Convex Relaxations for Unsupervised Learning\\n\\nTime:  4:00 PM to 5:00 PM \\n\\nLocation: 32-D463 \\n\\nWe'll describe a novel theoretical framework for unsupervised learning which is not based on generative assumptions. It is comparative, and allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization. These includes dictionary learning and learning of algebraic manifolds. \\n\\nJoint work with Tengyu Ma \\n\\n\\nbio: \\n\\nElad Hazan is a professor of computer science at Princeton university. He joined in 2015 from the Technion, where he had been an associate professor of operations research. His research focuses on the design and analysis of algorithms for basic problems in machine learning and optimization. Amongst his contributions are the co-development of the AdaGrad algorithm for training learning machines, and the first sublinear-time algorithms for convex optimization. He is the recipient of (twice) the IBM Goldberg best paper award in 2012 for contributions to sublinear time algorithms for machine learning, and in 2008 for decision making under uncertainty, a European Research Council grant, a Marie Curie fellowship and a Google Research Award (twice). He serves on the steering committee of the Association for Computational Learning and has been program chair for COLT 2015.\\n\\n\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-24',\n",
       "  'id': 35},\n",
       " {'summary': 'seminar today: Jeff Bilmes: Deep Submodular Functions: Learning and Applications in Data Science',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000170.html',\n",
       "  'location': '32-141',\n",
       "  'start': {'dateTime': '2016-10-25T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-25T18:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"This LIDS seminar today may be of interest:\\n\\n\\nDeep Submodular Functions: Learning and Applications in Data Science\\n\\nSpeaker: Jeffrey Bilmes, University of Washington\\nTime: Today, 4-5pm\\nLocation: 32-141\\n\\nAbstract:\\nIn this talk, we'll first review how submodular functions are useful in \\ndata science for various data manipulation problems (e.g., summarization \\nand partitioning), and how certain submodular functions (e.g., sums of \\nconcave composed with modular functions (SCMs)) are particularly useful \\ndue to their practical viability and scalability. We then introduce a \\nnew class of submodular functions called deep submodular functions \\n(DSFs). We motivate DSFs by addressing a limitation of SCMs. We then \\nsituate DSFs within the broader context of classes of submodular \\nfunctions in relationship both to various matroid ranks and SCMs.  \\nNotably, we find that DSFs constitute a strictly broader class than SCMs \\n(thus justifying their mathematical study), although they retain all of \\nthe attractive properties of SCMs. Interestingly, some DSFs can be seen \\nas special cases of deep neural networks (DNNs), hence the name, but \\nDSFs still do not comprise all submodular functions.  Finally, we show \\nhow to learn DSFs in a max-margin framework, and offer some preliminary \\nbut encouraging empirical learning results on both synthetic and \\nreal-world data instances.\\n\\nJoint work with Brian Dolhansky.\\n\\n\\nBio:\\nJeffrey A. Bilmes is a professor in the Department of Electrical \\nEngineering at the University of Washington, Seattle and an adjunct \\nprofessor in the Department of Computer Science and Engineering and the \\nDepartment of Linguistics. He received his Ph.D. in Computer Science \\nfrom the University of California, Berkeley. He is a 2001 NSF Career \\naward winner, a 2002 CRA Digital Government Fellow, a 2008 NAE Gilbreth \\nLectureship award recipient, and a 2012/2013 ISCA Distinguished \\nLecturer. Prof. Bilmes has been working on submodularity in machine \\nlearning for more than thirteen years. He received a best paper award at \\nICML 2013, a best paper award at NIPS 2013, and a best paper award at \\nACM-BCB 2016 for work in this area. Prof. Bilmes is also a recipient of \\na 25-year best paper award from the International Conference on \\nSupercomputing for his 1997 paper on high-performance matrix \\ncomputations.  Prof. Bilmes authored the graphical models toolkit \\n(GMTK), a dynamic graphical-model based optimized software system that \\nis widely used in speech and language processing, bioinformatics, and \\nhuman-activity recognition.\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-25',\n",
       "  'id': 36},\n",
       " {'summary': 'Talk by Elad Hazan today!!',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-October/000172.html',\n",
       "  'location': '32-D463',\n",
       "  'start': {'dateTime': '2016-10-26T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-10-26T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Just a reminder about Elad’s upcoming talk today!\\n\\nElad Hazan: A Non-generative Framework and Convex Relaxations for Unsupervised Learning\\n\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: 32-D463\\n\\nWe'll describe a novel theoretical framework for unsupervised learning which is not based on generative assumptions. It is comparative, and allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization. These includes dictionary learning and learning of algebraic manifolds.\\n\\nJoint work with Tengyu Ma\\n\\n\\nbio:\\n\\nElad Hazan is a professor of computer science at Princeton university. He joined in 2015 from the Technion, where he had been an associate professor of operations research. His research focuses on the design and analysis of algorithms for basic problems in machine learning and optimization. Amongst his contributions are the co-development of the AdaGrad algorithm for training learning machines, and the first sublinear-time algorithms for convex optimization. He is the recipient of (twice) the IBM Goldberg best paper award in 2012 for contributions to sublinear time algorithms for machine learning, and in 2008 for decision making under uncertainty, a European Research Council grant, a Marie Curie fellowship and a Google Research Award (twice). He serves on the steering committee of the Association for Computational Learning and has been program chair for COLT 2015.\\n\\n\\n\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-10-26',\n",
       "  'id': 35},\n",
       " {'summary': 'TALK: Wednesday 11-16-2016 Honglak Lee: Deep architectures for visual reasoning, multimodal learning, and decision-making',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-November/000177.html',\n",
       "  'location': '32-D463',\n",
       "  'start': {'dateTime': '2016-11-16T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-11-16T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"Honglak Lee will give an ML seminar next week. Please let me know if you \\nwould like to meet him.\\nStefanie\\n\\n\\n\\nML Seminar: Honglak Lee: Deep architectures for visual reasoning, multimodal learning, and decision-making\\n\\nSpeaker: Honglak Lee\\nSpeaker Affiliation: University of Michgan\\n\\n  \\nDate: Wednesday, November 16, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: 32-D463\\n\\nAbstract:\\nOver the recent years, deep learning has emerged as a powerful method for learning feature representations from complex input data, and it has been greatly successful in computer vision, speech recognition, and language modeling. While many deep learning algorithms focus on a discriminative task and extract only task-relevant features that are invariant to other factors, complex sensory data is often generated from intricate interaction between underlying factors of variations (for example, pose, morphology and viewpoints for 3d object images). In this work, we tackle the problem of learning deep representations that disentangle underlying factors of variation and allow for complex visual reasoning and inference. We present several successful instances of deep architectures and their learning methods in\\nsupervised and weakly-supervised settings. Further, I will talk about visual analogy making with disentangled representations, as well as a\\nconnection between disentangling and unsupervised learning. In the second part of the talk, I will describe my work on learning deep representations from multiple heterogeneous input modalities.\\nSpecifically, I will talk about multimodal learning via conditional prediction that explicitly encourages cross-modal associations. This framework provides a theoretical guarantee about learning a joint distribution and explains recent progress in deep architectures that\\ninterface vision and language, such as caption generation and conditional image synthesis. I will also describe other related work on learning joint embedding from images and text for fine-grained recognition and zero-shot learning. Finally, I will describe my work on  combining deep learning and reinforcement learning. Specifically, I will talk about how learning a predictive generative model  from video data can be useful for reinforcement learning via better exploration. I also will talk about a new memory-based architecture that helps  sequential decision making in a first-person view and active perception setting.\\n\\n\\nBio:\\nHonglak Lee is an Associate Professor of Computer Science and Engineering at the University of Michigan, Ann Arbor. He received his Ph.D. from Computer Science Department at Stanford University in 2010, advised by Prof. Andrew Ng. His research focuses on deep learning and representation learning, which spans over unsupervised and semi-supervised learning, supervised learning, transfer learning, structured prediction, optimization, and reinforcement learning. His methods have been successfully applied to computer vision and other\\nperception problems. He received best paper awards at ICML 2009 and CEAS 2005. He has served as a guest editor of IEEE TPAMI Special Issue\\non Learning Deep Architectures, an editorial board member of Neural Networks, and he is currently an associate editor of IEEE TPAMI. He\\nalso has served as area chairs of ICML, NIPS, ICLR, ICCV, CVPR, ECCV, AAAI, and IJCAI. He received the Google Faculty Research Award (2011),\\nNSF CAREER Award (2015), and was selected as one of AI's 10 to Watch by IEEE Intelligent Systems (2013) and a research fellow by Alfred P. Sloan Foundation (2016).\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-11-09',\n",
       "  'id': 37},\n",
       " {'summary': 'TALK: Wednesday 11-16-2016 Honglak Lee: Deep architectures for visual reasoning, multimodal learning, and decision-making',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-November/000182.html',\n",
       "  'location': '32-D463',\n",
       "  'start': {'dateTime': '2016-11-16T16:00:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-11-16T17:00:00', 'timeZone': 'America/New_York'},\n",
       "  'message': \"ML Colloquium: Honglak Lee: Deep architectures for visual reasoning, multimodal learning, and decision-making\\n\\nSpeaker: Honglak Lee\\nSpeaker Affiliation: University of Michgan\\n\\n  \\nDate: Wednesday, November 16, 2016\\nTime:  4:00 PM to 5:00 PM\\n\\nLocation: 32-D463\\n\\nAbstract:\\nOver the recent years, deep learning has emerged as a powerful method for learning feature representations from complex input data, and it has been greatly successful in computer vision, speech recognition, and language modeling. While many deep learning algorithms focus on a discriminative task and extract only task-relevant features that are invariant to other factors, complex sensory data is often generated from intricate interaction between underlying factors of variations (for example, pose, morphology and viewpoints for 3d object images). In this work, we tackle the problem of learning deep representations that disentangle underlying factors of variation and allow for complex visual reasoning and inference. We present several successful instances of deep architectures and their learning methods in\\nsupervised and weakly-supervised settings. Further, I will talk about visual analogy making with disentangled representations, as well as a\\nconnection between disentangling and unsupervised learning. In the second part of the talk, I will describe my work on learning deep representations from multiple heterogeneous input modalities.\\nSpecifically, I will talk about multimodal learning via conditional prediction that explicitly encourages cross-modal associations. This framework provides a theoretical guarantee about learning a joint distribution and explains recent progress in deep architectures that\\ninterface vision and language, such as caption generation and conditional image synthesis. I will also describe other related work on learning joint embedding from images and text for fine-grained recognition and zero-shot learning. Finally, I will describe my work on  combining deep learning and reinforcement learning. Specifically, I will talk about how learning a predictive generative model  from video data can be useful for reinforcement learning via better exploration. I also will talk about a new memory-based architecture that helps  sequential decision making in a first-person view and active perception setting.\\n\\n\\nBio:\\nHonglak Lee is an Associate Professor of Computer Science and Engineering at the University of Michigan, Ann Arbor. He received his Ph.D. from Computer Science Department at Stanford University in 2010, advised by Prof. Andrew Ng. His research focuses on deep learning and representation learning, which spans over unsupervised and semi-supervised learning, supervised learning, transfer learning, structured prediction, optimization, and reinforcement learning. His methods have been successfully applied to computer vision and other\\nperception problems. He received best paper awards at ICML 2009 and CEAS 2005. He has served as a guest editor of IEEE TPAMI Special Issue\\non Learning Deep Architectures, an editorial board member of Neural Networks, and he is currently an associate editor of IEEE TPAMI. He\\nalso has served as area chairs of ICML, NIPS, ICLR, ICCV, CVPR, ECCV, AAAI, and IJCAI. He received the Google Faculty Research Award (2011),\\nNSF CAREER Award (2015), and was selected as one of AI's 10 to Watch by IEEE Intelligent Systems (2013) and a research fellow by Alfred P. Sloan Foundation (2016).\",\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-11-16',\n",
       "  'id': 37},\n",
       " {'summary': 'ML Seminar: Song Han, Stanford University - Tuesday, December 13, 2016 at 10:30 AM in the Haus Room, 36-428',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-December/000189.html',\n",
       "  'location': '36-428',\n",
       "  'start': {'dateTime': '2016-12-13T10:30:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-12-13T11:30:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Song Han\\nStanford University\\n\\nDate/Time: Tues, Dec. 13, 10:30am-11:30am\\nLocation: 36-428 (Haus Room)\\n\\nTitle: \"From Compression to Acceleration: Efficient Methods for Deep Learning\"\\n\\nCo-hosted by Professors Tommi S. Jaakkola and Vivienne Sze\\n\\nAbstract:\\nDeep neural networks have evolved to be the state-of-the-art technique for machine-learning tasks ranging from computer vision to speech recognition to natural language processing. However, running such neural network is both computationally intensive and memory intensive, making it power hungry to deploy on embedded systems with limited power budget. To address this limitation, this talk presents an algorithm and hardware co-design methodology for improving the efficiency of deep learning.\\n\\nStarting with changing the algorithm, this talk introduces \"Deep Compression\" that can compress the deep neural network models by 10x-49x without loss of prediction accuracy for a broad range of CNN, RNN, and LSTMs. Followed by proposing a new hardware architecture, this talk introduces EIE, the \"Efficient Inference Engine\" that can do decompression and inference simultaneously which significantly saves memory bandwidth. Taking advantage of the compressed model, and being able to deal with the irregular computation pattern efficiently, EIE achieves 13x speedup and 3000x better energy efficient over GPU.\\nFinally, this talk closes the loop by revisiting model compression and provides practical guidance on hardware efficiency oriented model compression techniques.\\n\\nBio:\\nSong Han is a fifth-year Ph.D. student with Prof. Bill Dally at Stanford University. His research focuses on energy-efficient deep learning computing, at the intersection between machine learning and computer architecture. He proposed Deep Compression that can compress state-of-the-art CNNs by 10x-49x while fully preserving prediction accuracy. He designed EIE: Efficient Inference Engine, a hardware accelerator that can make inference directly on the compressed sparse model, which gives significant speedup and energy saving. His work has been covered by TheNextPlatform, TechEmergence, Embedded Vision and O\\'Reilly. His work received the Best Paper Award in ICLR\\'16 and Best Poster Award in Stanford Cloud Workshop\\'16.\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-12-06',\n",
       "  'id': 38},\n",
       " {'summary': 'ML Seminar: Song Han, Stanford University - TODAY, December 13, 2016 at 10:30 AM in the Haus Room, 36-428',\n",
       "  'description': 'http://mailman.mit.edu/mailman/private/mitml/2016-December/000193.html',\n",
       "  'location': '36-428',\n",
       "  'start': {'dateTime': '2016-12-13T10:30:00', 'timeZone': 'America/New_York'},\n",
       "  'end': {'dateTime': '2016-12-13T11:30:00', 'timeZone': 'America/New_York'},\n",
       "  'message': 'Song Han\\nStanford University\\n\\nDate/Time: TODAY, Dec. 13, 10:30am-11:30am\\nLocation: 36-428 (Haus Room)\\n\\nTitle: \"From Compression to Acceleration: Efficient Methods for Deep Learning\"\\n\\nCo-hosted by Professors Tommi S. Jaakkola and Vivienne Sze\\n\\nAbstract:\\nDeep neural networks have evolved to be the state-of-the-art technique for machine-learning tasks ranging from computer vision to speech recognition to natural language processing. However, running such neural network is both computationally intensive and memory intensive, making it power hungry to deploy on embedded systems with limited power budget. To address this limitation, this talk presents an algorithm and hardware co-design methodology for improving the efficiency of deep learning.\\n\\nStarting with changing the algorithm, this talk introduces \"Deep Compression\" that can compress the deep neural network models by 10x-49x without loss of prediction accuracy for a broad range of CNN, RNN, and LSTMs. Followed by proposing a new hardware architecture, this talk introduces EIE, the \"Efficient Inference Engine\" that can do decompression and inference simultaneously which significantly saves memory bandwidth. Taking advantage of the compressed model, and being able to deal with the irregular computation pattern efficiently, EIE achieves 13x speedup and 3000x better energy efficient over GPU.\\nFinally, this talk closes the loop by revisiting model compression and provides practical guidance on hardware efficiency oriented model compression techniques.\\n\\nBio:\\nSong Han is a fifth-year Ph.D. student with Prof. Bill Dally at Stanford University. His research focuses on energy-efficient deep learning computing, at the intersection between machine learning and computer architecture. He proposed Deep Compression that can compress state-of-the-art CNNs by 10x-49x while fully preserving prediction accuracy. He designed EIE: Efficient Inference Engine, a hardware accelerator that can make inference directly on the compressed sparse model, which gives significant speedup and energy saving. His work has been covered by TheNextPlatform, TechEmergence, Embedded Vision and O\\'Reilly. His work received the Best Paper Award in ICLR\\'16 and Best Poster Award in Stanford Cloud Workshop\\'16.\\n\\n',\n",
       "  'is_talk': True,\n",
       "  'is_correction': False,\n",
       "  'posted_date': '2016-12-13',\n",
       "  'id': 38}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_listing_id(metadata): \n",
    "    \n",
    "    for m in all_metadata[-30:]: \n",
    "        similarity = similar(m['message'], metadata['message'])\n",
    "        if metadata['is_correction']: \n",
    "            if similarity > 0.3: \n",
    "                return m['id']\n",
    "        else: \n",
    "            if similarity > 0.5: \n",
    "                return m['id']\n",
    "            \n",
    "    return -1\n",
    "    \n",
    "\n",
    "all_metadata = []\n",
    "counter = 0\n",
    "for index in indices[-200:][::-1]: \n",
    "    \n",
    "    l = Listing(list_id, index) \n",
    "    \n",
    "    if l.is_talk: \n",
    "        \n",
    "        metadata = l.get_parsed_metadata_dense()\n",
    "        \n",
    "        listing_id = get_listing_id(metadata)\n",
    "        metadata['id'] = listing_id\n",
    "        if listing_id == -1: \n",
    "            counter += 1\n",
    "            metadata['id'] = counter\n",
    "            \n",
    "        all_metadata.append(metadata)\n",
    "        \n",
    "        \n",
    "        # Add to all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mitml'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.url.split('/')[-3] + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>end</th>\n",
       "      <th>id</th>\n",
       "      <th>is_correction</th>\n",
       "      <th>is_talk</th>\n",
       "      <th>location</th>\n",
       "      <th>message</th>\n",
       "      <th>posted_date</th>\n",
       "      <th>start</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-09-17T15:00:00', 'timeZone'...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D507</td>\n",
       "      <td>Just a reminder about this MIT-ML Seminar talk...</td>\n",
       "      <td>2015-09-16</td>\n",
       "      <td>{'dateTime': '2015-09-17T14:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Thursday 09-17-2015 Extreme Classificati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-10-22T17:00:00', 'timeZone'...</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Begin forwarded message:\\n\\n&gt; From: calendar a...</td>\n",
       "      <td>2015-10-21</td>\n",
       "      <td>{'dateTime': '2015-10-22T16:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: TALK: Thursday 10-22-2015 Brian Kulis: Sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-11-12T17:00:00', 'timeZone'...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D507</td>\n",
       "      <td>COEVOLVE: A Joint Point Process Model for Info...</td>\n",
       "      <td>2015-11-05</td>\n",
       "      <td>{'dateTime': '2015-11-12T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Thursday 11-12-2015 COEVOLVE: A Joint Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-11-10T14:00:00', 'timeZone'...</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G449</td>\n",
       "      <td>Title: Speech Production Features for Deep Neu...</td>\n",
       "      <td>2015-11-09</td>\n",
       "      <td>{'dateTime': '2015-11-10T13:00:00', 'timeZone'...</td>\n",
       "      <td>Tuesday Nov. 10 - Seminar: Leonardo Badino - S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-11-17T12:00:00', 'timeZone'...</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dear all,\\nProf. Risi Kondor from University o...</td>\n",
       "      <td>2015-11-10</td>\n",
       "      <td>{'dateTime': '2015-11-17T11:00:00', 'timeZone'...</td>\n",
       "      <td>Risi Kondor: MIT-ML Seminar (Tuesday, November...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-11-12T16:00:00', 'timeZone'...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D507</td>\n",
       "      <td>ML seminar: COEVOLVE: A Joint Point Process Mo...</td>\n",
       "      <td>2015-11-12</td>\n",
       "      <td>{'dateTime': '2015-11-12T15:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Thursday 11-12-2015 COEVOLVE: A Joint Po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-11-12T16:00:00', 'timeZone'...</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D507</td>\n",
       "      <td>Today! this talk will be of interest to those ...</td>\n",
       "      <td>2015-11-12</td>\n",
       "      <td>{'dateTime': '2015-11-12T15:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: TALK: Thursday 11-12-2015 COEVOLVE: A Joi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-11-19T17:00:00', 'timeZone'...</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D463</td>\n",
       "      <td>Thursday at the MIT-MSR ML seminar, Rebecca St...</td>\n",
       "      <td>2015-11-15</td>\n",
       "      <td>{'dateTime': '2015-11-19T16:00:00', 'timeZone'...</td>\n",
       "      <td>Rebecca Steorts Thursday 4pm at MIT-MSR ML sem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-11-17T12:00:00', 'timeZone'...</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reminder - this talk will be happening tomorro...</td>\n",
       "      <td>2015-11-16</td>\n",
       "      <td>{'dateTime': '2015-11-17T11:00:00', 'timeZone'...</td>\n",
       "      <td>Risi Kondor: MIT-ML Seminar (Tuesday, November...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-11-17T12:00:00', 'timeZone'...</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>For those who are interested, Risi will be pre...</td>\n",
       "      <td>2015-11-17</td>\n",
       "      <td>{'dateTime': '2015-11-17T11:00:00', 'timeZone'...</td>\n",
       "      <td>Risi Kondor: MIT-ML Seminar (Tuesday, November...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-11-19T17:00:00', 'timeZone'...</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>9-201</td>\n",
       "      <td>Begin forwarded message:\\n\\n&gt; From: calendar a...</td>\n",
       "      <td>2015-11-19</td>\n",
       "      <td>{'dateTime': '2015-11-19T16:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: TALK: Thursday 11-19-2015 Methods for Qua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-12-02T17:00:00', 'timeZone'...</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Title:  Sequential Information Gathering: From...</td>\n",
       "      <td>2015-12-01</td>\n",
       "      <td>{'dateTime': '2015-12-02T16:00:00', 'timeZone'...</td>\n",
       "      <td>Talk: Hamed Hassani: Sequential Information Ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2015-12-11T12:00:00', 'timeZone'...</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-141</td>\n",
       "      <td>---------- Forwarded message ----------\\nFrom:...</td>\n",
       "      <td>2015-12-07</td>\n",
       "      <td>{'dateTime': '2015-12-11T11:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: [Lids-seminars] Friday, Dec 11, 11am, 32-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hi all,\\n\\nWe have been running the Machine Le...</td>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Machine Learning Tea seminar : Looking for new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-02-02T17:15:00', 'timeZone'...</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D463</td>\n",
       "      <td>Dear all,\\n\\nI want to highlight the different...</td>\n",
       "      <td>2016-01-26</td>\n",
       "      <td>{'dateTime': '2016-02-02T16:15:00', 'timeZone'...</td>\n",
       "      <td>ML Seminar: Tuesday 02-02-2016 Scalable variat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-02-02T17:15:00', 'timeZone'...</td>\n",
       "      <td>12</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D463</td>\n",
       "      <td>Hi, all,\\n\\nA reminder about today's ML Semina...</td>\n",
       "      <td>2016-02-02</td>\n",
       "      <td>{'dateTime': '2016-02-02T16:15:00', 'timeZone'...</td>\n",
       "      <td>ML Seminar: Tuesday 02-02-2016 Scalable variat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-02-17T17:00:00', 'timeZone'...</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D507</td>\n",
       "      <td>Dear all,\\n\\nOur second ML Seminar of the seme...</td>\n",
       "      <td>2016-02-11</td>\n",
       "      <td>{'dateTime': '2016-02-17T16:00:00', 'timeZone'...</td>\n",
       "      <td>ML seminar: Wednesday 02-17-2016 Scalable Baye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-02-17T17:00:00', 'timeZone'...</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D507</td>\n",
       "      <td>P.S. Nick Foti is available and interested to ...</td>\n",
       "      <td>2016-02-12</td>\n",
       "      <td>{'dateTime': '2016-02-17T16:00:00', 'timeZone'...</td>\n",
       "      <td>ML seminar: Wednesday 02-17-2016 Scalable Baye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-02-17T17:00:00', 'timeZone'...</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D507</td>\n",
       "      <td>Hi,\\n\\nA reminder about the ML seminar today a...</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>{'dateTime': '2016-02-17T16:00:00', 'timeZone'...</td>\n",
       "      <td>ML seminar: Wednesday 02-17-2016 Scalable Baye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-03-02T16:00:00', 'timeZone'...</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D463</td>\n",
       "      <td>MIT-MSR ML seminar: John Lafferty: Statistical...</td>\n",
       "      <td>2016-03-02</td>\n",
       "      <td>{'dateTime': '2016-03-02T15:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: TALK: Wednesday 03-02-2016 MIT-MSR ML sem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-04-06T17:15:00', 'timeZone'...</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G882</td>\n",
       "      <td>Machine Learning seminar: Truncated Random Mea...</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>{'dateTime': '2016-04-06T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 04-06-2016 ML seminar: Truncat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-04-06T17:15:00', 'timeZone'...</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G882</td>\n",
       "      <td>Dear all,\\n\\nNote the location for the ML Semi...</td>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>{'dateTime': '2016-04-06T16:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: TALK: Wednesday 04-06-2016 ML seminar: Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-04-06T17:15:00', 'timeZone'...</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G882</td>\n",
       "      <td>A reminder about the ML Seminar tomorrow (Wedn...</td>\n",
       "      <td>2016-04-05</td>\n",
       "      <td>{'dateTime': '2016-04-06T16:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: TALK: Wednesday 04-06-2016 ML seminar: Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-04-20T16:00:00', 'timeZone'...</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G449</td>\n",
       "      <td>MIT-MSR Machine Learning seminar\\n\\nSpeaker: M...</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>{'dateTime': '2016-04-20T15:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: TALK: Wednesday 04-20-2016 MIT-MSR ML sem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-04-26T15:00:00', 'timeZone'...</td>\n",
       "      <td>17</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1-201</td>\n",
       "      <td>##############################################...</td>\n",
       "      <td>2016-04-21</td>\n",
       "      <td>{'dateTime': '2016-04-26T14:00:00', 'timeZone'...</td>\n",
       "      <td>MSR Talk: Beyond Backpropagation: Uncertainty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-04-27T16:00:00', 'timeZone'...</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Machine Learning seminar: Without-Replacement ...</td>\n",
       "      <td>2016-04-22</td>\n",
       "      <td>{'dateTime': '2016-04-27T15:00:00', 'timeZone'...</td>\n",
       "      <td>Wednesday 04-27-2016 Machine Learning seminar:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-05-02T16:15:00', 'timeZone'...</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Zen of Passwords (and the Complexity of Hu...</td>\n",
       "      <td>2016-04-26</td>\n",
       "      <td>{'dateTime': '2016-05-02T15:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Monday 05-02-2016 MIT-MSR ML seminar: Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-04-27T16:15:00', 'timeZone'...</td>\n",
       "      <td>18</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Machine Learning seminar: Without-Replacement ...</td>\n",
       "      <td>2016-04-27</td>\n",
       "      <td>{'dateTime': '2016-04-27T15:00:00', 'timeZone'...</td>\n",
       "      <td>Today: Machine Learning seminar: Without-Repla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-05-02T16:15:00', 'timeZone'...</td>\n",
       "      <td>19</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&gt; The Zen of Passwords (and the Complexity of ...</td>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>{'dateTime': '2016-05-02T15:00:00', 'timeZone'...</td>\n",
       "      <td>Tomorrow,\\tMonday 05-02-2016 MIT-MSR ML semina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-07-01T12:15:00', 'timeZone'...</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D507</td>\n",
       "      <td>Summarizing Big Data Using Submodular Function...</td>\n",
       "      <td>2016-06-20</td>\n",
       "      <td>{'dateTime': '2016-07-01T11:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Friday 07-01-2016 Summarizing Big Data U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-09-14T17:00:00', 'timeZone'...</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G575</td>\n",
       "      <td>ML seminar: Safe Decision Making Under Uncerta...</td>\n",
       "      <td>2016-09-07</td>\n",
       "      <td>{'dateTime': '2016-09-14T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 09-14-2016 Safe Decision Makin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-09-16T15:00:00', 'timeZone'...</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G882</td>\n",
       "      <td>ML seminar: Discriminative Embedding of Latent...</td>\n",
       "      <td>2016-09-09</td>\n",
       "      <td>{'dateTime': '2016-09-16T14:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Friday 09-16-2016 ML seminar: Le Song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-05T13:00:00', 'timeZone'...</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-124</td>\n",
       "      <td>ML seminar: The data-driven (s, S) policy: why...</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>{'dateTime': '2016-10-05T12:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 10-05-2016 ML seminar: The dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-09-13T17:00:00', 'timeZone'...</td>\n",
       "      <td>29</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-141</td>\n",
       "      <td>Begin forwarded message:\\n\\nFrom: Roxana Herna...</td>\n",
       "      <td>2016-09-12</td>\n",
       "      <td>{'dateTime': '2016-09-13T16:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: [IDSS-Community] LIDS Seminar Series: Suv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-09-14T17:00:00', 'timeZone'...</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G575</td>\n",
       "      <td>ML seminar: Safe Decision Making Under Uncerta...</td>\n",
       "      <td>2016-09-14</td>\n",
       "      <td>{'dateTime': '2016-09-14T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 09-14-2016 Safe Decision Makin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-09-16T15:00:00', 'timeZone'...</td>\n",
       "      <td>27</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G882</td>\n",
       "      <td>ML seminar: Discriminative Embedding of Latent...</td>\n",
       "      <td>2016-09-16</td>\n",
       "      <td>{'dateTime': '2016-09-16T14:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Friday 09-16-2016 ML seminar: Le Song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-09-16T12:00:00', 'timeZone'...</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>E18-304</td>\n",
       "      <td>Another talk of interest today:\\n\\nBegin forwa...</td>\n",
       "      <td>2016-09-16</td>\n",
       "      <td>{'dateTime': '2016-09-16T11:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: [Stat-events] STATISTICS SEMINAR - Lorenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-19T17:00:00', 'timeZone'...</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G575</td>\n",
       "      <td>ML seminar: Probabilistic numerics: treating n...</td>\n",
       "      <td>2016-09-19</td>\n",
       "      <td>{'dateTime': '2016-10-19T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 10-19-2016 ML seminar: Probabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-09-29T17:00:00', 'timeZone'...</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>4-237</td>\n",
       "      <td>As part of our class, we are hosting a special...</td>\n",
       "      <td>2016-09-27</td>\n",
       "      <td>{'dateTime': '2016-09-29T16:00:00', 'timeZone'...</td>\n",
       "      <td>Seminar on Thursday: Criminal networks, Carlo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-05T13:00:00', 'timeZone'...</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-124</td>\n",
       "      <td>ML seminar: The data-driven (s, S) policy: why...</td>\n",
       "      <td>2016-09-28</td>\n",
       "      <td>{'dateTime': '2016-10-05T12:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 10-05-2016 ML seminar: The dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-05T17:00:00', 'timeZone'...</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G575</td>\n",
       "      <td>ML seminar: The data-driven (s, S) policy: why...</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>{'dateTime': '2016-10-05T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 10-05-2016 ML seminar: The dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-05T17:00:00', 'timeZone'...</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G575</td>\n",
       "      <td>Note the change in location and time from an e...</td>\n",
       "      <td>2016-10-03</td>\n",
       "      <td>{'dateTime': '2016-10-05T16:00:00', 'timeZone'...</td>\n",
       "      <td>UPDATED SPACE/TIME: Gah-Yi Ban, ML seminar @ 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-05T17:00:00', 'timeZone'...</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G575</td>\n",
       "      <td>ML seminar: The data-driven (s, S) policy: why...</td>\n",
       "      <td>2016-10-05</td>\n",
       "      <td>{'dateTime': '2016-10-05T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 10-05-2016 ML seminar: The dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-05T17:00:00', 'timeZone'...</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G575</td>\n",
       "      <td>Reminder: the ML seminar is happening shortly ...</td>\n",
       "      <td>2016-10-05</td>\n",
       "      <td>{'dateTime': '2016-10-05T16:00:00', 'timeZone'...</td>\n",
       "      <td>ML seminar at 4pm in 32-G575: Gah-Yi Ban, The ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-19T17:00:00', 'timeZone'...</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G575</td>\n",
       "      <td>ML seminar: Probabilistic numerics: treating n...</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>{'dateTime': '2016-10-19T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 10-19-2016 ML seminar: Probabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-19T17:00:00', 'timeZone'...</td>\n",
       "      <td>31</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-G575</td>\n",
       "      <td>The following event has been CANCELLED:\\n\\nML ...</td>\n",
       "      <td>2016-10-13</td>\n",
       "      <td>{'dateTime': '2016-10-19T16:00:00', 'timeZone'...</td>\n",
       "      <td>CANCELLED: Wednesday 10-19-2016 ML seminar: Pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-19T17:00:00', 'timeZone'...</td>\n",
       "      <td>33</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ML seminar: The Reasonable Effectiveness of Ro...</td>\n",
       "      <td>2016-10-14</td>\n",
       "      <td>{'dateTime': '2016-10-19T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 10-19-2016 ML seminar: Tina El...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-19T17:00:00', 'timeZone'...</td>\n",
       "      <td>33</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>If you'd like to meet with Tina Eliassi-Rad on...</td>\n",
       "      <td>2016-10-14</td>\n",
       "      <td>{'dateTime': '2016-10-19T16:00:00', 'timeZone'...</td>\n",
       "      <td>One-on-one Meetings with Tina Eliassi-Rad (ML ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-19T17:00:00', 'timeZone'...</td>\n",
       "      <td>33</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reminder to sign up for one-on-one meetings wi...</td>\n",
       "      <td>2016-10-17</td>\n",
       "      <td>{'dateTime': '2016-10-19T16:00:00', 'timeZone'...</td>\n",
       "      <td>One-on-one Meetings with Tina Eliassi-Rad (ML ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-18T17:00:00', 'timeZone'...</td>\n",
       "      <td>34</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>8-201</td>\n",
       "      <td>Begin forwarded message:\\n\\nFrom: Vinod Vaikun...</td>\n",
       "      <td>2016-10-18</td>\n",
       "      <td>{'dateTime': '2016-10-18T16:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: [Theory-seminars] TALK: Tuesday 10-18-201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-19T17:00:00', 'timeZone'...</td>\n",
       "      <td>33</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-------------------------------\\n&gt; ML seminar:...</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>{'dateTime': '2016-10-19T16:00:00', 'timeZone'...</td>\n",
       "      <td>Reminder: ML seminar today!: Tina Eliassi-Rad:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-25T17:00:00', 'timeZone'...</td>\n",
       "      <td>29</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-141</td>\n",
       "      <td>Begin forwarded message:\\n\\nFrom: Roxana Herna...</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>{'dateTime': '2016-10-25T16:00:00', 'timeZone'...</td>\n",
       "      <td>Fwd: LIDS Seminar: Jeffrey Bilmes - 10/25/16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-26T17:00:00', 'timeZone'...</td>\n",
       "      <td>35</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D463</td>\n",
       "      <td>ML colloquium: Elad Hazan: A Non-generative Fr...</td>\n",
       "      <td>2016-10-19</td>\n",
       "      <td>{'dateTime': '2016-10-26T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 10-26-2016 ML seminar: Elad Hazan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-26T17:00:00', 'timeZone'...</td>\n",
       "      <td>35</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D463</td>\n",
       "      <td>Elad Hazan is giving a talk this Wednesday - t...</td>\n",
       "      <td>2016-10-24</td>\n",
       "      <td>{'dateTime': '2016-10-26T16:00:00', 'timeZone'...</td>\n",
       "      <td>talk by Elad Hazan on Wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-25T18:00:00', 'timeZone'...</td>\n",
       "      <td>36</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-141</td>\n",
       "      <td>This LIDS seminar today may be of interest:\\n\\...</td>\n",
       "      <td>2016-10-25</td>\n",
       "      <td>{'dateTime': '2016-10-25T17:00:00', 'timeZone'...</td>\n",
       "      <td>seminar today: Jeff Bilmes: Deep Submodular Fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-10-26T17:00:00', 'timeZone'...</td>\n",
       "      <td>35</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D463</td>\n",
       "      <td>Just a reminder about Elad’s upcoming talk tod...</td>\n",
       "      <td>2016-10-26</td>\n",
       "      <td>{'dateTime': '2016-10-26T16:00:00', 'timeZone'...</td>\n",
       "      <td>Talk by Elad Hazan today!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-11-16T17:00:00', 'timeZone'...</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D463</td>\n",
       "      <td>Honglak Lee will give an ML seminar next week....</td>\n",
       "      <td>2016-11-09</td>\n",
       "      <td>{'dateTime': '2016-11-16T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 11-16-2016 Honglak Lee: Deep a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-11-16T17:00:00', 'timeZone'...</td>\n",
       "      <td>37</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>32-D463</td>\n",
       "      <td>ML Colloquium: Honglak Lee: Deep architectures...</td>\n",
       "      <td>2016-11-16</td>\n",
       "      <td>{'dateTime': '2016-11-16T16:00:00', 'timeZone'...</td>\n",
       "      <td>TALK: Wednesday 11-16-2016 Honglak Lee: Deep a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-12-13T11:30:00', 'timeZone'...</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>36-428</td>\n",
       "      <td>Song Han\\nStanford University\\n\\nDate/Time: Tu...</td>\n",
       "      <td>2016-12-06</td>\n",
       "      <td>{'dateTime': '2016-12-13T10:30:00', 'timeZone'...</td>\n",
       "      <td>ML Seminar: Song Han, Stanford University - Tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>http://mailman.mit.edu/mailman/private/mitml/2...</td>\n",
       "      <td>{'dateTime': '2016-12-13T11:30:00', 'timeZone'...</td>\n",
       "      <td>38</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>36-428</td>\n",
       "      <td>Song Han\\nStanford University\\n\\nDate/Time: TO...</td>\n",
       "      <td>2016-12-13</td>\n",
       "      <td>{'dateTime': '2016-12-13T10:30:00', 'timeZone'...</td>\n",
       "      <td>ML Seminar: Song Han, Stanford University - TO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          description  \\\n",
       "0   http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "1   http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "2   http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "3   http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "4   http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "5   http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "6   http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "7   http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "8   http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "9   http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "10  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "11  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "12  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "13  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "14  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "15  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "16  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "17  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "18  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "19  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "20  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "21  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "22  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "23  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "24  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "25  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "26  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "27  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "28  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "29  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "..                                                ...   \n",
       "43  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "44  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "45  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "46  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "47  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "48  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "49  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "50  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "51  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "52  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "53  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "54  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "55  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "56  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "57  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "58  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "59  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "60  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "61  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "62  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "63  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "64  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "65  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "66  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "67  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "68  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "69  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "70  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "71  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "72  http://mailman.mit.edu/mailman/private/mitml/2...   \n",
       "\n",
       "                                                  end  id  is_correction  \\\n",
       "0   {'dateTime': '2015-09-17T15:00:00', 'timeZone'...   1          False   \n",
       "1   {'dateTime': '2015-10-22T17:00:00', 'timeZone'...   2          False   \n",
       "2   {'dateTime': '2015-11-12T17:00:00', 'timeZone'...   3          False   \n",
       "3   {'dateTime': '2015-11-10T14:00:00', 'timeZone'...   4          False   \n",
       "4   {'dateTime': '2015-11-17T12:00:00', 'timeZone'...   5          False   \n",
       "5   {'dateTime': '2015-11-12T16:00:00', 'timeZone'...   3          False   \n",
       "6   {'dateTime': '2015-11-12T16:00:00', 'timeZone'...   3          False   \n",
       "7   {'dateTime': '2015-11-19T17:00:00', 'timeZone'...   6          False   \n",
       "8   {'dateTime': '2015-11-17T12:00:00', 'timeZone'...   5          False   \n",
       "9   {'dateTime': '2015-11-17T12:00:00', 'timeZone'...   5          False   \n",
       "10  {'dateTime': '2015-11-19T17:00:00', 'timeZone'...   7          False   \n",
       "11  {'dateTime': '2015-12-02T17:00:00', 'timeZone'...   8          False   \n",
       "12  {'dateTime': '2015-12-11T12:00:00', 'timeZone'...   9          False   \n",
       "13                                                NaN  10          False   \n",
       "14  {'dateTime': '2016-02-02T17:15:00', 'timeZone'...  11          False   \n",
       "15  {'dateTime': '2016-02-02T17:15:00', 'timeZone'...  12          False   \n",
       "16  {'dateTime': '2016-02-17T17:00:00', 'timeZone'...  13          False   \n",
       "17  {'dateTime': '2016-02-17T17:00:00', 'timeZone'...  13          False   \n",
       "18  {'dateTime': '2016-02-17T17:00:00', 'timeZone'...  13          False   \n",
       "19  {'dateTime': '2016-03-02T16:00:00', 'timeZone'...  14          False   \n",
       "20  {'dateTime': '2016-04-06T17:15:00', 'timeZone'...  15          False   \n",
       "21  {'dateTime': '2016-04-06T17:15:00', 'timeZone'...  15          False   \n",
       "22  {'dateTime': '2016-04-06T17:15:00', 'timeZone'...  15          False   \n",
       "23  {'dateTime': '2016-04-20T16:00:00', 'timeZone'...  16          False   \n",
       "24  {'dateTime': '2016-04-26T15:00:00', 'timeZone'...  17          False   \n",
       "25  {'dateTime': '2016-04-27T16:00:00', 'timeZone'...  18          False   \n",
       "26  {'dateTime': '2016-05-02T16:15:00', 'timeZone'...  19          False   \n",
       "27  {'dateTime': '2016-04-27T16:15:00', 'timeZone'...  18          False   \n",
       "28  {'dateTime': '2016-05-02T16:15:00', 'timeZone'...  19          False   \n",
       "29  {'dateTime': '2016-07-01T12:15:00', 'timeZone'...  20          False   \n",
       "..                                                ...  ..            ...   \n",
       "43  {'dateTime': '2016-09-14T17:00:00', 'timeZone'...  26          False   \n",
       "44  {'dateTime': '2016-09-16T15:00:00', 'timeZone'...  27          False   \n",
       "45  {'dateTime': '2016-10-05T13:00:00', 'timeZone'...  28          False   \n",
       "46  {'dateTime': '2016-09-13T17:00:00', 'timeZone'...  29          False   \n",
       "47  {'dateTime': '2016-09-14T17:00:00', 'timeZone'...  26          False   \n",
       "48  {'dateTime': '2016-09-16T15:00:00', 'timeZone'...  27          False   \n",
       "49  {'dateTime': '2016-09-16T12:00:00', 'timeZone'...  30          False   \n",
       "50  {'dateTime': '2016-10-19T17:00:00', 'timeZone'...  31          False   \n",
       "51  {'dateTime': '2016-09-29T17:00:00', 'timeZone'...  32          False   \n",
       "52  {'dateTime': '2016-10-05T13:00:00', 'timeZone'...  28          False   \n",
       "53  {'dateTime': '2016-10-05T17:00:00', 'timeZone'...  28          False   \n",
       "54  {'dateTime': '2016-10-05T17:00:00', 'timeZone'...  28          False   \n",
       "55  {'dateTime': '2016-10-05T17:00:00', 'timeZone'...  28          False   \n",
       "56  {'dateTime': '2016-10-05T17:00:00', 'timeZone'...  28          False   \n",
       "57  {'dateTime': '2016-10-19T17:00:00', 'timeZone'...  31          False   \n",
       "58  {'dateTime': '2016-10-19T17:00:00', 'timeZone'...  31          False   \n",
       "59  {'dateTime': '2016-10-19T17:00:00', 'timeZone'...  33          False   \n",
       "60  {'dateTime': '2016-10-19T17:00:00', 'timeZone'...  33          False   \n",
       "61  {'dateTime': '2016-10-19T17:00:00', 'timeZone'...  33          False   \n",
       "62  {'dateTime': '2016-10-18T17:00:00', 'timeZone'...  34          False   \n",
       "63  {'dateTime': '2016-10-19T17:00:00', 'timeZone'...  33          False   \n",
       "64  {'dateTime': '2016-10-25T17:00:00', 'timeZone'...  29          False   \n",
       "65  {'dateTime': '2016-10-26T17:00:00', 'timeZone'...  35          False   \n",
       "66  {'dateTime': '2016-10-26T17:00:00', 'timeZone'...  35          False   \n",
       "67  {'dateTime': '2016-10-25T18:00:00', 'timeZone'...  36          False   \n",
       "68  {'dateTime': '2016-10-26T17:00:00', 'timeZone'...  35          False   \n",
       "69  {'dateTime': '2016-11-16T17:00:00', 'timeZone'...  37          False   \n",
       "70  {'dateTime': '2016-11-16T17:00:00', 'timeZone'...  37          False   \n",
       "71  {'dateTime': '2016-12-13T11:30:00', 'timeZone'...  38          False   \n",
       "72  {'dateTime': '2016-12-13T11:30:00', 'timeZone'...  38          False   \n",
       "\n",
       "    is_talk location                                            message  \\\n",
       "0      True  32-D507  Just a reminder about this MIT-ML Seminar talk...   \n",
       "1      True      NaN  Begin forwarded message:\\n\\n> From: calendar a...   \n",
       "2      True  32-D507  COEVOLVE: A Joint Point Process Model for Info...   \n",
       "3      True  32-G449  Title: Speech Production Features for Deep Neu...   \n",
       "4      True      NaN  Dear all,\\nProf. Risi Kondor from University o...   \n",
       "5      True  32-D507  ML seminar: COEVOLVE: A Joint Point Process Mo...   \n",
       "6      True  32-D507  Today! this talk will be of interest to those ...   \n",
       "7      True  32-D463  Thursday at the MIT-MSR ML seminar, Rebecca St...   \n",
       "8      True      NaN  Reminder - this talk will be happening tomorro...   \n",
       "9      True      NaN  For those who are interested, Risi will be pre...   \n",
       "10     True    9-201  Begin forwarded message:\\n\\n> From: calendar a...   \n",
       "11     True      NaN  Title:  Sequential Information Gathering: From...   \n",
       "12     True   32-141  ---------- Forwarded message ----------\\nFrom:...   \n",
       "13     True      NaN  Hi all,\\n\\nWe have been running the Machine Le...   \n",
       "14     True  32-D463  Dear all,\\n\\nI want to highlight the different...   \n",
       "15     True  32-D463  Hi, all,\\n\\nA reminder about today's ML Semina...   \n",
       "16     True  32-D507  Dear all,\\n\\nOur second ML Seminar of the seme...   \n",
       "17     True  32-D507  P.S. Nick Foti is available and interested to ...   \n",
       "18     True  32-D507  Hi,\\n\\nA reminder about the ML seminar today a...   \n",
       "19     True  32-D463  MIT-MSR ML seminar: John Lafferty: Statistical...   \n",
       "20     True  32-G882  Machine Learning seminar: Truncated Random Mea...   \n",
       "21     True  32-G882  Dear all,\\n\\nNote the location for the ML Semi...   \n",
       "22     True  32-G882  A reminder about the ML Seminar tomorrow (Wedn...   \n",
       "23     True  32-G449  MIT-MSR Machine Learning seminar\\n\\nSpeaker: M...   \n",
       "24     True    1-201  ##############################################...   \n",
       "25     True      NaN  Machine Learning seminar: Without-Replacement ...   \n",
       "26     True      NaN  The Zen of Passwords (and the Complexity of Hu...   \n",
       "27     True      NaN  Machine Learning seminar: Without-Replacement ...   \n",
       "28     True      NaN  > The Zen of Passwords (and the Complexity of ...   \n",
       "29     True  32-D507  Summarizing Big Data Using Submodular Function...   \n",
       "..      ...      ...                                                ...   \n",
       "43     True  32-G575  ML seminar: Safe Decision Making Under Uncerta...   \n",
       "44     True  32-G882  ML seminar: Discriminative Embedding of Latent...   \n",
       "45     True   32-124  ML seminar: The data-driven (s, S) policy: why...   \n",
       "46     True   32-141  Begin forwarded message:\\n\\nFrom: Roxana Herna...   \n",
       "47     True  32-G575  ML seminar: Safe Decision Making Under Uncerta...   \n",
       "48     True  32-G882  ML seminar: Discriminative Embedding of Latent...   \n",
       "49     True  E18-304  Another talk of interest today:\\n\\nBegin forwa...   \n",
       "50     True  32-G575  ML seminar: Probabilistic numerics: treating n...   \n",
       "51     True    4-237  As part of our class, we are hosting a special...   \n",
       "52     True   32-124  ML seminar: The data-driven (s, S) policy: why...   \n",
       "53     True  32-G575  ML seminar: The data-driven (s, S) policy: why...   \n",
       "54     True  32-G575  Note the change in location and time from an e...   \n",
       "55     True  32-G575  ML seminar: The data-driven (s, S) policy: why...   \n",
       "56     True  32-G575  Reminder: the ML seminar is happening shortly ...   \n",
       "57     True  32-G575  ML seminar: Probabilistic numerics: treating n...   \n",
       "58     True  32-G575  The following event has been CANCELLED:\\n\\nML ...   \n",
       "59     True      NaN  ML seminar: The Reasonable Effectiveness of Ro...   \n",
       "60     True      NaN  If you'd like to meet with Tina Eliassi-Rad on...   \n",
       "61     True      NaN  Reminder to sign up for one-on-one meetings wi...   \n",
       "62     True    8-201  Begin forwarded message:\\n\\nFrom: Vinod Vaikun...   \n",
       "63     True      NaN  -------------------------------\\n> ML seminar:...   \n",
       "64     True   32-141  Begin forwarded message:\\n\\nFrom: Roxana Herna...   \n",
       "65     True  32-D463  ML colloquium: Elad Hazan: A Non-generative Fr...   \n",
       "66     True  32-D463  Elad Hazan is giving a talk this Wednesday - t...   \n",
       "67     True   32-141  This LIDS seminar today may be of interest:\\n\\...   \n",
       "68     True  32-D463  Just a reminder about Elad’s upcoming talk tod...   \n",
       "69     True  32-D463  Honglak Lee will give an ML seminar next week....   \n",
       "70     True  32-D463  ML Colloquium: Honglak Lee: Deep architectures...   \n",
       "71     True   36-428  Song Han\\nStanford University\\n\\nDate/Time: Tu...   \n",
       "72     True   36-428  Song Han\\nStanford University\\n\\nDate/Time: TO...   \n",
       "\n",
       "   posted_date                                              start  \\\n",
       "0   2015-09-16  {'dateTime': '2015-09-17T14:00:00', 'timeZone'...   \n",
       "1   2015-10-21  {'dateTime': '2015-10-22T16:00:00', 'timeZone'...   \n",
       "2   2015-11-05  {'dateTime': '2015-11-12T16:00:00', 'timeZone'...   \n",
       "3   2015-11-09  {'dateTime': '2015-11-10T13:00:00', 'timeZone'...   \n",
       "4   2015-11-10  {'dateTime': '2015-11-17T11:00:00', 'timeZone'...   \n",
       "5   2015-11-12  {'dateTime': '2015-11-12T15:00:00', 'timeZone'...   \n",
       "6   2015-11-12  {'dateTime': '2015-11-12T15:00:00', 'timeZone'...   \n",
       "7   2015-11-15  {'dateTime': '2015-11-19T16:00:00', 'timeZone'...   \n",
       "8   2015-11-16  {'dateTime': '2015-11-17T11:00:00', 'timeZone'...   \n",
       "9   2015-11-17  {'dateTime': '2015-11-17T11:00:00', 'timeZone'...   \n",
       "10  2015-11-19  {'dateTime': '2015-11-19T16:00:00', 'timeZone'...   \n",
       "11  2015-12-01  {'dateTime': '2015-12-02T16:00:00', 'timeZone'...   \n",
       "12  2015-12-07  {'dateTime': '2015-12-11T11:00:00', 'timeZone'...   \n",
       "13  2016-01-26                                                NaN   \n",
       "14  2016-01-26  {'dateTime': '2016-02-02T16:15:00', 'timeZone'...   \n",
       "15  2016-02-02  {'dateTime': '2016-02-02T16:15:00', 'timeZone'...   \n",
       "16  2016-02-11  {'dateTime': '2016-02-17T16:00:00', 'timeZone'...   \n",
       "17  2016-02-12  {'dateTime': '2016-02-17T16:00:00', 'timeZone'...   \n",
       "18  2016-02-17  {'dateTime': '2016-02-17T16:00:00', 'timeZone'...   \n",
       "19  2016-03-02  {'dateTime': '2016-03-02T15:00:00', 'timeZone'...   \n",
       "20  2016-04-01  {'dateTime': '2016-04-06T16:00:00', 'timeZone'...   \n",
       "21  2016-04-01  {'dateTime': '2016-04-06T16:00:00', 'timeZone'...   \n",
       "22  2016-04-05  {'dateTime': '2016-04-06T16:00:00', 'timeZone'...   \n",
       "23  2016-04-19  {'dateTime': '2016-04-20T15:00:00', 'timeZone'...   \n",
       "24  2016-04-21  {'dateTime': '2016-04-26T14:00:00', 'timeZone'...   \n",
       "25  2016-04-22  {'dateTime': '2016-04-27T15:00:00', 'timeZone'...   \n",
       "26  2016-04-26  {'dateTime': '2016-05-02T15:00:00', 'timeZone'...   \n",
       "27  2016-04-27  {'dateTime': '2016-04-27T15:00:00', 'timeZone'...   \n",
       "28  2016-05-01  {'dateTime': '2016-05-02T15:00:00', 'timeZone'...   \n",
       "29  2016-06-20  {'dateTime': '2016-07-01T11:00:00', 'timeZone'...   \n",
       "..         ...                                                ...   \n",
       "43  2016-09-07  {'dateTime': '2016-09-14T16:00:00', 'timeZone'...   \n",
       "44  2016-09-09  {'dateTime': '2016-09-16T14:00:00', 'timeZone'...   \n",
       "45  2016-09-12  {'dateTime': '2016-10-05T12:00:00', 'timeZone'...   \n",
       "46  2016-09-12  {'dateTime': '2016-09-13T16:00:00', 'timeZone'...   \n",
       "47  2016-09-14  {'dateTime': '2016-09-14T16:00:00', 'timeZone'...   \n",
       "48  2016-09-16  {'dateTime': '2016-09-16T14:00:00', 'timeZone'...   \n",
       "49  2016-09-16  {'dateTime': '2016-09-16T11:00:00', 'timeZone'...   \n",
       "50  2016-09-19  {'dateTime': '2016-10-19T16:00:00', 'timeZone'...   \n",
       "51  2016-09-27  {'dateTime': '2016-09-29T16:00:00', 'timeZone'...   \n",
       "52  2016-09-28  {'dateTime': '2016-10-05T12:00:00', 'timeZone'...   \n",
       "53  2016-10-03  {'dateTime': '2016-10-05T16:00:00', 'timeZone'...   \n",
       "54  2016-10-03  {'dateTime': '2016-10-05T16:00:00', 'timeZone'...   \n",
       "55  2016-10-05  {'dateTime': '2016-10-05T16:00:00', 'timeZone'...   \n",
       "56  2016-10-05  {'dateTime': '2016-10-05T16:00:00', 'timeZone'...   \n",
       "57  2016-10-12  {'dateTime': '2016-10-19T16:00:00', 'timeZone'...   \n",
       "58  2016-10-13  {'dateTime': '2016-10-19T16:00:00', 'timeZone'...   \n",
       "59  2016-10-14  {'dateTime': '2016-10-19T16:00:00', 'timeZone'...   \n",
       "60  2016-10-14  {'dateTime': '2016-10-19T16:00:00', 'timeZone'...   \n",
       "61  2016-10-17  {'dateTime': '2016-10-19T16:00:00', 'timeZone'...   \n",
       "62  2016-10-18  {'dateTime': '2016-10-18T16:00:00', 'timeZone'...   \n",
       "63  2016-10-19  {'dateTime': '2016-10-19T16:00:00', 'timeZone'...   \n",
       "64  2016-10-19  {'dateTime': '2016-10-25T16:00:00', 'timeZone'...   \n",
       "65  2016-10-19  {'dateTime': '2016-10-26T16:00:00', 'timeZone'...   \n",
       "66  2016-10-24  {'dateTime': '2016-10-26T16:00:00', 'timeZone'...   \n",
       "67  2016-10-25  {'dateTime': '2016-10-25T17:00:00', 'timeZone'...   \n",
       "68  2016-10-26  {'dateTime': '2016-10-26T16:00:00', 'timeZone'...   \n",
       "69  2016-11-09  {'dateTime': '2016-11-16T16:00:00', 'timeZone'...   \n",
       "70  2016-11-16  {'dateTime': '2016-11-16T16:00:00', 'timeZone'...   \n",
       "71  2016-12-06  {'dateTime': '2016-12-13T10:30:00', 'timeZone'...   \n",
       "72  2016-12-13  {'dateTime': '2016-12-13T10:30:00', 'timeZone'...   \n",
       "\n",
       "                                              summary  \n",
       "0   TALK: Thursday 09-17-2015 Extreme Classificati...  \n",
       "1   Fwd: TALK: Thursday 10-22-2015 Brian Kulis: Sm...  \n",
       "2   TALK: Thursday 11-12-2015 COEVOLVE: A Joint Po...  \n",
       "3   Tuesday Nov. 10 - Seminar: Leonardo Badino - S...  \n",
       "4   Risi Kondor: MIT-ML Seminar (Tuesday, November...  \n",
       "5   TALK: Thursday 11-12-2015 COEVOLVE: A Joint Po...  \n",
       "6   Fwd: TALK: Thursday 11-12-2015 COEVOLVE: A Joi...  \n",
       "7   Rebecca Steorts Thursday 4pm at MIT-MSR ML sem...  \n",
       "8   Risi Kondor: MIT-ML Seminar (Tuesday, November...  \n",
       "9   Risi Kondor: MIT-ML Seminar (Tuesday, November...  \n",
       "10  Fwd: TALK: Thursday 11-19-2015 Methods for Qua...  \n",
       "11  Talk: Hamed Hassani: Sequential Information Ga...  \n",
       "12  Fwd: [Lids-seminars] Friday, Dec 11, 11am, 32-...  \n",
       "13  Machine Learning Tea seminar : Looking for new...  \n",
       "14  ML Seminar: Tuesday 02-02-2016 Scalable variat...  \n",
       "15  ML Seminar: Tuesday 02-02-2016 Scalable variat...  \n",
       "16  ML seminar: Wednesday 02-17-2016 Scalable Baye...  \n",
       "17  ML seminar: Wednesday 02-17-2016 Scalable Baye...  \n",
       "18  ML seminar: Wednesday 02-17-2016 Scalable Baye...  \n",
       "19  Fwd: TALK: Wednesday 03-02-2016 MIT-MSR ML sem...  \n",
       "20  TALK: Wednesday 04-06-2016 ML seminar: Truncat...  \n",
       "21  Fwd: TALK: Wednesday 04-06-2016 ML seminar: Tr...  \n",
       "22  Fwd: TALK: Wednesday 04-06-2016 ML seminar: Tr...  \n",
       "23  Fwd: TALK: Wednesday 04-20-2016 MIT-MSR ML sem...  \n",
       "24  MSR Talk: Beyond Backpropagation: Uncertainty ...  \n",
       "25  Wednesday 04-27-2016 Machine Learning seminar:...  \n",
       "26  TALK: Monday 05-02-2016 MIT-MSR ML seminar: Sa...  \n",
       "27  Today: Machine Learning seminar: Without-Repla...  \n",
       "28  Tomorrow,\\tMonday 05-02-2016 MIT-MSR ML semina...  \n",
       "29  TALK: Friday 07-01-2016 Summarizing Big Data U...  \n",
       "..                                                ...  \n",
       "43  TALK: Wednesday 09-14-2016 Safe Decision Makin...  \n",
       "44        TALK: Friday 09-16-2016 ML seminar: Le Song  \n",
       "45  TALK: Wednesday 10-05-2016 ML seminar: The dat...  \n",
       "46  Fwd: [IDSS-Community] LIDS Seminar Series: Suv...  \n",
       "47  TALK: Wednesday 09-14-2016 Safe Decision Makin...  \n",
       "48        TALK: Friday 09-16-2016 ML seminar: Le Song  \n",
       "49  Fwd: [Stat-events] STATISTICS SEMINAR - Lorenz...  \n",
       "50  TALK: Wednesday 10-19-2016 ML seminar: Probabi...  \n",
       "51  Seminar on Thursday: Criminal networks, Carlo ...  \n",
       "52  TALK: Wednesday 10-05-2016 ML seminar: The dat...  \n",
       "53  TALK: Wednesday 10-05-2016 ML seminar: The dat...  \n",
       "54  UPDATED SPACE/TIME: Gah-Yi Ban, ML seminar @ 4...  \n",
       "55  TALK: Wednesday 10-05-2016 ML seminar: The dat...  \n",
       "56  ML seminar at 4pm in 32-G575: Gah-Yi Ban, The ...  \n",
       "57  TALK: Wednesday 10-19-2016 ML seminar: Probabi...  \n",
       "58  CANCELLED: Wednesday 10-19-2016 ML seminar: Pr...  \n",
       "59  TALK: Wednesday 10-19-2016 ML seminar: Tina El...  \n",
       "60  One-on-one Meetings with Tina Eliassi-Rad (ML ...  \n",
       "61  One-on-one Meetings with Tina Eliassi-Rad (ML ...  \n",
       "62  Fwd: [Theory-seminars] TALK: Tuesday 10-18-201...  \n",
       "63  Reminder: ML seminar today!: Tina Eliassi-Rad:...  \n",
       "64       Fwd: LIDS Seminar: Jeffrey Bilmes - 10/25/16  \n",
       "65  TALK: Wednesday 10-26-2016 ML seminar: Elad Hazan  \n",
       "66                    talk by Elad Hazan on Wednesday  \n",
       "67  seminar today: Jeff Bilmes: Deep Submodular Fu...  \n",
       "68                         Talk by Elad Hazan today!!  \n",
       "69  TALK: Wednesday 11-16-2016 Honglak Lee: Deep a...  \n",
       "70  TALK: Wednesday 11-16-2016 Honglak Lee: Deep a...  \n",
       "71  ML Seminar: Song Han, Stanford University - Tu...  \n",
       "72  ML Seminar: Song Han, Stanford University - TO...  \n",
       "\n",
       "[73 rows x 10 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
